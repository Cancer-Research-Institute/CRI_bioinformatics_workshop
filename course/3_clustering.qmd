---
title: "Clustering concepts and correlation"
---

# Distances

## Manhattan Distance

Manhattan distance, or L1 distance, measures the distance between two points by summing the absolute differences of their Cartesian coordinates. It is equivalent to the total number of moves required to go from one point to another if only axis-aligned moves (up, down, left, right) are allowed, mimicking a city grid.

### Considerations

 - Less influenced by outliers compared to Euclidean distance.
 - Useful in high-dimensional spaces.
 
## Euclidean Distance

Euclidean distance, also known as the L2 distance, is the most common metric used to measure the straight-line distance between two points in Euclidean space. It is the default distance measure in many analytical applications.

### Considerations

 - Measures the shortest path between points.
 - Sensitive to outliers.
 - Used in default settings for many algorithms like K-means clustering.

## `dist` in R

```R
# Load the iris dataset
data(iris)
iris_numeric <- iris[, 1:4]  # Exclude the species column

# Euclidean distance
euclidean_distances <- dist(iris_numeric, method = "euclidean")

# Manhattan distance
manhattan_distances <- dist(iris_numeric, method = "manhattan")

```



# Clustering

*Clustering* is a technique used to group similar objects or data points together based on their characteristics or features. For example, clustering can be applied to gene expression data to identify groups of genes that behave similarly under different experimental conditions. These clusters might ultimately inform cell types or subtypes. There are several different clustering techniques, including hierarchical or K-means clustering. It is important to understand the limitations or assumptions of a clustering algorithm when applying it to your data.

## K-means clustering

K-means clustering is a popular unsupervised machine learning algorithm used for partitioning a dataset into a predetermined number of clusters. The goal of k-means clustering is to group data points into clusters such that data points within the same cluster are more similar to each other than to those in other clusters. The algorithm works iteratively to assign each data point to the nearest cluster centroid (center point of a cluster) and then update the centroid based on the mean of all data points assigned to that cluster. This process continues until the centroids no longer change significantly, or a specified number of iterations is reached.

K-means has some limitations, such as sensitivity to the initial random selection of centroids and the need to specify the number of clusters beforehand. Additionally, k-means may not perform well on datasets with non-spherical or irregularly shaped clusters.

Running
```R
# Load data
data(iris)
iris_data <- iris[, -5]

# Run k-means
set.seed(123)
kmeans_result <- kmeans(iris_data, centers = 3, nstart = 20)

# Results
print(kmeans_result)
table(Cluster = kmeans_result$cluster, Species = iris$Species)

# Visualization
library(ggplot2)
ggplot(iris, aes(Sepal.Length, Sepal.Width, color = factor(kmeans_result$cluster))) +
  geom_point(alpha = 0.5) +
  labs(color = 'Cluster') +
  ggtitle("K-means Clustering of the Iris Dataset")

```
![Kmeans on iris data](kmeans.png)


## Heirarchical Clustering

```R
# Install and load necessary packages
if (!requireNamespace("stringdist", quietly = TRUE)) {
  install.packages("stringdist")
}
library(stringdist)

# Define TCR sequences
tcr_sequences <- c("CASSLGTQYEQYF", "CASSLGTEAFF", "CASSQETQYEQYF", "CASSLRTDTQYF")
names(tcr_sequences) <- tcr_sequences  # Use sequences as labels

# Calculate pairwise string distances using the Levenshtein method
dist_matrix <- stringdistmatrix(tcr_sequences, tcr_sequences, method = "lv")

# Perform hierarchical clustering using the complete linkage method
hc <- hclust(as.dist(dist_matrix), method = "complete")

# Plot the dendrogram
plot(hc, main = "Hierarchical Clustering of TCR Sequences", sub = "", xlab = "", ylab = "Distance",
     labels = names(tcr_sequences), hang = -1)  # Ensure labels hang below the plot
```
![Sequence Distance Clustering](tcr_string_dist.png)

```R
# Install and load pheatmap if not already installed
if (!requireNamespace("pheatmap", quietly = TRUE)) {
    install.packages("pheatmap")
}
library(pheatmap)

# Set seed for reproducibility
set.seed(42)

# Define parameters
num_genes <- 100
num_samples <- 50  # 25 T cells + 25 Cancer cells

# Simulate base gene expression
gene_expression <- matrix(rnorm(num_genes * num_samples, mean = 10, sd = 2), 
                          nrow = num_genes, ncol = num_samples)

# Introduce differences in expression between the two groups
gene_expression[81:100, 1:25] <- gene_expression[81:100, 1:25] + 2  # T cells
gene_expression[81:100, 26:50] <- gene_expression[81:100, 26:50] - 2  # Cancer cells

# Label rows and columns
rownames(gene_expression) <- paste("Gene", 1:num_genes, sep = "")
colnames(gene_expression) <- c(paste("T_Cell", 1:25, sep = ""), paste("Cancer_Cell", 1:25, sep = ""))

# Transpose the gene expression matrix
transposed_gene_expression <- t(gene_expression)

# Creating a heatmap with clustering and annotation
pheatmap(transposed_gene_expression, 
         show_rownames = TRUE, 
         show_colnames = FALSE, 
         clustering_distance_rows = "euclidean",
         cluster_rows  = TRUE,
         cluster_cols  = FALSE,
         main = "Heatmap of Gene Expression with Clustering")

```
![Expression Distance Clustering](gene_expression_dist.png)
![Expression Distance Clustering](gene_expression_heatmap.png)

# Dimension Reduction

Dimension reduction is a crucial step in analyzing high-dimensional data, such as gene expression data, where the number of features (genes) is large compared to the number of samples (experimental conditions). Dimension reduction techniques aim to simplify the data while preserving its essential structure, simplifying hundreds or thousands of features into a two-dimensional space. For clustering cell types or samples, this approach enables the identification of which genes are the most different across groups or clusters.

## Principal Component Analysis (PCA)

PCA is a widely used dimension reduction technique that transforms high-dimensional data into a lower-dimensional representation by identifying the principal components that capture the maximum variance in the data. These principal components are orthogonal to each other and can be used to visualize the data in lower dimensions.

```R
# Load necessary packages
library(ggplot2)

# Load data
data(iris)
iris_data <- iris[, 1:4]

# PCA
pca_results <- prcomp(iris_data, center = TRUE, scale. = TRUE)
print(summary(pca_results))

# Visualization
biplot(pca_results, scale = 0)

# Scatter plot of the first two PCs
pc_df <- data.frame(PC1 = pca_results$x[,1], PC2 = pca_results$x[,2], Species = iris$Species)
ggplot(pc_df, aes(x = PC1, y = PC2, color = Species)) +
    geom_point() +
    labs(title = "PCA of Iris Dataset",
         x = "Principal Component 1",
         y = "Principal Component 2") +
    theme_minimal()
```

# Correlation

Correlation measures the strength and direction of the relationship between two variables. In bioinformatics, correlation analysis is often used to explore relationships between gene expression levels across different samples or experimental conditions.

## Spearman vs. Pearson Correlation

- **Pearson Correlation**: Measures the linear relationship between two variables. It assumes that the variables are normally distributed and have a linear relationship.
- **Spearman Correlation**: Measures the monotonic relationship between two variables. It does not assume linearity and is more robust to outliers and non-normal distributions.

## Example Code

```R
# Load required libraries
library(ggpubr)

# Example dataframe
df <- data.frame(
  Gene1 = c(1, 2, 3, 4, 5),
  Gene2 = c(5, 4, 3, 2, 1),
  Gene3 = c(2, 3, 4, 5, 6)
)

# Perform PCA
pca_result <- prcomp(df)

# Plot PCA
plot(pca_result$x[,1], pca_result$x[,2], 
     xlab = "PC1", ylab = "PC2", 
     main = "PCA Plot")

# Perform clustering
# Example clustering algorithm: k-means
kmeans_result <- kmeans(df, centers = 2)

# Plot clustering
plot(df, col = kmeans_result$cluster, 
     main = "Clustering Plot")

# Add correlation statistics to a plot
# Example plot
ggscatter(df, x = "Gene1", y = "Gene2", 
          add = "reg.line", 
          cor.coef = TRUE, 
          cor.method = "spearman", 
          cor.coeff.args = list(method = "spearman"))
```


