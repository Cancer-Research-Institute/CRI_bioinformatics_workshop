[
  {
    "objectID": "authors.html",
    "href": "authors.html",
    "title": "Faculty",
    "section": "",
    "text": "Malachi Griffith, Ph.D. is an Associate Professor of Medicine (Division of Oncology), Associate Professor of Genetics and Assistant Director of the McDonnell Genome Institute at Washington University. Dr. Griffith has more than 15 years of experience in the fields of genomics, bioinformatics, data mining, and cancer research. He has published over 130 studies, received numerous research awards and honors and held several large grants including an NIH NHGRI K99/R00 Career Development Award, an NCI ITCR U01 award, an NCI Biomedical Knowledgebase U24 award, and a V Scholar Award. He has mentored more than 50 bioinformatics trainees and taught more than 500 as an instructor for Cold Spring Harbor Laboratories and the Canadian Bioinformatics Workshops. Dr. Griffith’s research is focused on improving our understanding of cancer biology and the development of personalized medicine strategies for cancer using genomics and informatics technologies. Dr. Griffith is involved in more than ten investigator initiated clinical trials utilizing his lab’s tools for design of personalized neoantigen targeting immunotherapies. Dr. Griffith’s lab has made substantial contributions to open source and open access resources for cancer research. The development of bioinformatics methods and software for immunogenomics has become a major focus of his lab.\n\n\n\n\n\nObi Griffith, Ph.D. is an Associate Professor of Medicine and Genetics at Washington University School of Medicine in St. Louis, where he also serves as Assistant Director at the McDonnell Genome Institute. His research focuses on developing personalized medicine strategies for cancer, utilizing genomic technologies to study gene regulatory changes, particularly in breast cancer. Dr. Griffith employs bioinformatics and statistical methods to analyze high-throughput sequencing data, aiming to identify biomarkers for diagnostics, prognosis, and drug response prediction. He co-developed the Clinical Interpretations of Variants in Cancer (CIViC), an open-source database that links cancer mutations to targeted therapies. Dr. Griffith completed his PhD in Medical Genetics at the University of British Columbia in 2008 and has held postdoctoral fellowships at the BC Cancer Agency and Lawrence Berkeley National Laboratory. Before joining Washington University, he was an Assistant Professor at the University of British Columbia and a bioinformatics consultant at various institutions.\n\n\n\n\n\nNataly Naser Al Deen, Ph.D. is currently a Senior Computational Biologist at Memorial Sloan Kettering Cancer Center, leading the efforts on Spatial Transcriptomics Techniques (mainly Visium HD) in pre-malignant lesions, including in ovarian cancer (serous tubal intraepithelial carcinoma (STIC)) and gastric cancers (signet ring cell (SRC) carcinoma). She is a wet-lab cancer researcher by training (10 years) and recently shifted to computational oncology during her second postdoctoral fellowship in cancer immunotherapy and melanoma research at Dr. Antoni Ribas’ Lab at UCLA (2022-2024), focusing on spatial biology techniques to study tumor-immune interactions from patients with melanoma who underwent immune checkpoint blockade therapies. Prior, she completed a 2-year postdoctoral fellowship in cancer genomics at the Ding Lab at Washington University School of Medicine (WashU), where she led the single cell and single nuclei RNA/ATAC-sequencing technologies, Visium spatial transcriptomics and 3D Lightsheet Microscopy, focusing on breast, pancreatic, renal cell carcinoma, and metastatic colorectal cancers (wet-lab), and was part of the HTAN, CPTAC, and SenNet consortia. She obtained her Ph.D. in Cell and Molecular Biology from the American University of Beirut and received three fully-funded scholarships from the United States Department of State including the Fulbright Scholarship to Georgetown University where she obtained her M.Sc. in Tumor Biology. Through her parallel passion for science policy, advocacy, and activism for the rights of minority and indigenous populations, she founded an NGO in 2015 called “Pink Steps”, to serve as an exercise, health and mental-wellness support group for breast cancer survivors in Lebanon and was named on Forbes 30 Under 30 List for her humanitarian work. In 2018 she represented Lebanon at the 68th Lindau Nobel laureate Meeting in Physiology/Medicine and collaborated with Nobel Laureates on several panels including for research and science policy. She then served as the only Global Academic Fellow to the Precision Medicine Council at the World Economic Forum in 2019 and as an expert member at the Public Policy Projects UK’s Global Genomics Roundtables, Phase 2 in 2022-2024.\n\n\n\n\n\nNicholas Ceglia, Ph.D. is a principal computational biologist at Memorial Sloan Kettering Cancer Center in New York. He received his PhD in computer science from the University of California, Irvine. Currently, he leads the cellular phenotyping team that is part of the Shah Lab and the Computational Immuno-oncology initiative. His research focuses on the development of computational methods to understand co-evolution of cancer and the adaptive immune system.\n\n\n\n\n\nKatie Campbell, Ph.D. is an Adjunct Assistant Professor at the University of California, Los Angeles (UCLA). Her research is at the intersection of cancer biology, immunology, and bioinformatics, integrating these methodologies to study how melanoma responds to immunotherapy. Leveraging high-dimensional spatial profiling and bulk sequencing modalities, her work aims to discover resistance mechanisms that stratify patients and inform the next generation of precision oncology strategies. Katie earned her undergraduate degree in biochemistry at the Penn State University and her PhD in molecular cell biology at Washington University in St. Louis. Her training, research, and career have been supported by NIH NCI T32 training programs at Washington University in St. Louis and UCLA, the Cancer Research Institute Irvington Postdoctoral Fellowship, the Gil Nickel Melanoma Research Fellowship through the V Foundation and UCLA Jonsson Comprehensive Cancer Center, the Parker Institute for Cancer Immunotherapy and V Foundation Bridge Fellowship, and the Melanoma Research Alliance-Ressler Family Fund Young Investigator Award. In addition to her research, Dr. Campbell is actively involved in mentorship and professional development, having served five years and elected Chair of the Associate Member Council of the American Association for Cancer Research (AACR), and research and patient advocacy, through the AACR and the Melanoma Research Foundation."
  },
  {
    "objectID": "authors.html#instructors",
    "href": "authors.html#instructors",
    "title": "Faculty",
    "section": "",
    "text": "Malachi Griffith, Ph.D. is an Associate Professor of Medicine (Division of Oncology), Associate Professor of Genetics and Assistant Director of the McDonnell Genome Institute at Washington University. Dr. Griffith has more than 15 years of experience in the fields of genomics, bioinformatics, data mining, and cancer research. He has published over 130 studies, received numerous research awards and honors and held several large grants including an NIH NHGRI K99/R00 Career Development Award, an NCI ITCR U01 award, an NCI Biomedical Knowledgebase U24 award, and a V Scholar Award. He has mentored more than 50 bioinformatics trainees and taught more than 500 as an instructor for Cold Spring Harbor Laboratories and the Canadian Bioinformatics Workshops. Dr. Griffith’s research is focused on improving our understanding of cancer biology and the development of personalized medicine strategies for cancer using genomics and informatics technologies. Dr. Griffith is involved in more than ten investigator initiated clinical trials utilizing his lab’s tools for design of personalized neoantigen targeting immunotherapies. Dr. Griffith’s lab has made substantial contributions to open source and open access resources for cancer research. The development of bioinformatics methods and software for immunogenomics has become a major focus of his lab.\n\n\n\n\n\nObi Griffith, Ph.D. is an Associate Professor of Medicine and Genetics at Washington University School of Medicine in St. Louis, where he also serves as Assistant Director at the McDonnell Genome Institute. His research focuses on developing personalized medicine strategies for cancer, utilizing genomic technologies to study gene regulatory changes, particularly in breast cancer. Dr. Griffith employs bioinformatics and statistical methods to analyze high-throughput sequencing data, aiming to identify biomarkers for diagnostics, prognosis, and drug response prediction. He co-developed the Clinical Interpretations of Variants in Cancer (CIViC), an open-source database that links cancer mutations to targeted therapies. Dr. Griffith completed his PhD in Medical Genetics at the University of British Columbia in 2008 and has held postdoctoral fellowships at the BC Cancer Agency and Lawrence Berkeley National Laboratory. Before joining Washington University, he was an Assistant Professor at the University of British Columbia and a bioinformatics consultant at various institutions.\n\n\n\n\n\nNataly Naser Al Deen, Ph.D. is currently a Senior Computational Biologist at Memorial Sloan Kettering Cancer Center, leading the efforts on Spatial Transcriptomics Techniques (mainly Visium HD) in pre-malignant lesions, including in ovarian cancer (serous tubal intraepithelial carcinoma (STIC)) and gastric cancers (signet ring cell (SRC) carcinoma). She is a wet-lab cancer researcher by training (10 years) and recently shifted to computational oncology during her second postdoctoral fellowship in cancer immunotherapy and melanoma research at Dr. Antoni Ribas’ Lab at UCLA (2022-2024), focusing on spatial biology techniques to study tumor-immune interactions from patients with melanoma who underwent immune checkpoint blockade therapies. Prior, she completed a 2-year postdoctoral fellowship in cancer genomics at the Ding Lab at Washington University School of Medicine (WashU), where she led the single cell and single nuclei RNA/ATAC-sequencing technologies, Visium spatial transcriptomics and 3D Lightsheet Microscopy, focusing on breast, pancreatic, renal cell carcinoma, and metastatic colorectal cancers (wet-lab), and was part of the HTAN, CPTAC, and SenNet consortia. She obtained her Ph.D. in Cell and Molecular Biology from the American University of Beirut and received three fully-funded scholarships from the United States Department of State including the Fulbright Scholarship to Georgetown University where she obtained her M.Sc. in Tumor Biology. Through her parallel passion for science policy, advocacy, and activism for the rights of minority and indigenous populations, she founded an NGO in 2015 called “Pink Steps”, to serve as an exercise, health and mental-wellness support group for breast cancer survivors in Lebanon and was named on Forbes 30 Under 30 List for her humanitarian work. In 2018 she represented Lebanon at the 68th Lindau Nobel laureate Meeting in Physiology/Medicine and collaborated with Nobel Laureates on several panels including for research and science policy. She then served as the only Global Academic Fellow to the Precision Medicine Council at the World Economic Forum in 2019 and as an expert member at the Public Policy Projects UK’s Global Genomics Roundtables, Phase 2 in 2022-2024.\n\n\n\n\n\nNicholas Ceglia, Ph.D. is a principal computational biologist at Memorial Sloan Kettering Cancer Center in New York. He received his PhD in computer science from the University of California, Irvine. Currently, he leads the cellular phenotyping team that is part of the Shah Lab and the Computational Immuno-oncology initiative. His research focuses on the development of computational methods to understand co-evolution of cancer and the adaptive immune system.\n\n\n\n\n\nKatie Campbell, Ph.D. is an Adjunct Assistant Professor at the University of California, Los Angeles (UCLA). Her research is at the intersection of cancer biology, immunology, and bioinformatics, integrating these methodologies to study how melanoma responds to immunotherapy. Leveraging high-dimensional spatial profiling and bulk sequencing modalities, her work aims to discover resistance mechanisms that stratify patients and inform the next generation of precision oncology strategies. Katie earned her undergraduate degree in biochemistry at the Penn State University and her PhD in molecular cell biology at Washington University in St. Louis. Her training, research, and career have been supported by NIH NCI T32 training programs at Washington University in St. Louis and UCLA, the Cancer Research Institute Irvington Postdoctoral Fellowship, the Gil Nickel Melanoma Research Fellowship through the V Foundation and UCLA Jonsson Comprehensive Cancer Center, the Parker Institute for Cancer Immunotherapy and V Foundation Bridge Fellowship, and the Melanoma Research Alliance-Ressler Family Fund Young Investigator Award. In addition to her research, Dr. Campbell is actively involved in mentorship and professional development, having served five years and elected Chair of the Associate Member Council of the American Association for Cancer Research (AACR), and research and patient advocacy, through the AACR and the Melanoma Research Foundation."
  },
  {
    "objectID": "authors.html#teaching-assistants",
    "href": "authors.html#teaching-assistants",
    "title": "Faculty",
    "section": "Teaching Assistants",
    "text": "Teaching Assistants\n\n\n\n\n\nChristie Chang is a PhD student in the Immunology program at Stanford in the Satpathy Lab. She is currently optimizing immune cell-cell interaction labeling technology, focusing on dendritic cell and T cell interactions. Utilizing her background in computer science, genetics, and immunology, she hopes to better understand how dendritic cells can influence T cell activation and fate through the use of single-cell, high-dimensional assays and new technologies.\n\n\n\n\n\nKelsy Cotto, Ph.D. completed her PhD in the Molecular Cell Biology program at Washington University in St. Louis. Her thesis focused on the integration of genomic and transcriptomic data to identify cancer-specific splice altering variants. Prior to that, she graduated from Mercer University with honors in Biochemistry and Molecular Biology. She is passionate about using computational approaches to further understand biology and develop better therapeutic approaches for patients. She has experience working with genomic and transcriptomic data using data from Illumina, Oxford-Nanopore Technologies, 10X Genomics, particularly to identify tumor specific antigens and the TCRs that recognize them.\n\n\n\n\n\nEvelyn Schmidt is a first-year PhD trainee and beginning research in Dr. Jennifer Foltz’s lab, where she uses single-cell sequencing approaches to improve natural killer cell therapy for cancer. Before starting my PhD, Evelyn worked as a bioinformatician with Dr. Obi and Malachi Griffith, contributing to the lab’s neoantigen calling pipeline and the development of pVACtools. She loves reading fantasy books, listening to all different types of music, and a good cup of coffee.\n\n\n\n\n\nKartik Singhal is a PhD student in the Molecular Genetics and Genomics program at Washington University in St Louis, USA. He is pursuing his thesis research in Malachi Griffith and Obi Griffith’s lab where he is interested in using multi-omic techniques to better understand cancer’s interaction with the immune system. He primarily analyzes single-cell omics data and works in R and Python for his analyses.\n\n\n\n\n\nZachary Skidmore is a quantitative scientist and bioinformatician, holding 10+ years of experience in the fields of bioinformatics and computational biology. His undergraduate work was completed at the Ohio State University and graduate level work completed at the University of Illinois. After graduation he started work at the McDonnell Genome Institute at Washington University in Saint Louis under the mentorship of Drs.Obi and Malachi Griffith. His research focus has traditionally been in the realm of cancer biology where he has used and developed tools and techniques to aid in the analysis and interpretation of sequencing data. Recently he has been focused on translational science using liquid biopsies.\n\n\n\n\n\nMatthew Zatzman, Ph.D. is a Senior Computational Biologist at Memorial Sloan Kettering Cancer Center, working under the supervision of Dr. Sohrab Shah and Dr. Andrew McPherson in the Department of Computational Oncology. Matthew completed his PhD in Dr. Adam Shlien’s lab at the University of Toronto, where he studied the role of hypertranscription in driving aggressive cancers. Currently, Matthew applies single-cell computational methods to map out the complex interplay between tumor and non-tumor cell phenotypes in the tumor microenvironment."
  },
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Resources",
    "section": "",
    "text": "This workshop will utilize the R programmling language through the RStudio user interface. We will perform these activities on the RStudio Server, available through the Posit Cloud platform. While the Introduction to R programming (Days 1-2) is is designed for all skill levels, having a basic understanding of R programming may enhance your learning experience. If you are new to these areas, consider reviewing one or a few of the following introductory materials prior to the workshop. Over the course of the workshop, teaching assistants (TAs) will be working with small groups of attendees during the hands-on exercises. Feel free to direct your questions to the them, the instructors, or the Slack channel.\n\nPosit Cloud account: You should have received access to Posit Cloud through a link to your email address for this workshop. Logging into the workspace, using this email address, will provide access to all course materials and allow for data analysis through RStudio Server on your internet browser.\nLaptop computer: Please bring your own laptop with internet capability. Since the workshop will take place through Posit Cloud, the interface will be accessed through an internet browser. Google Chrome is recommended.\n\n\n\n\nR: In order to utilize the RStudio Desktop application from your laptop (and not through the browser), you will first need to install the latest version of R.\nRStudio Desktop can be downloaded here. Note that this functions in the same manner as the RStudio Server by Posit Cloud, but utilizes the storage and compute of the laptop machine itself. Laptops vary in memory and storage space, which can dictate the speed and capability of data analysis, and this may result in slower processes with large datasets."
  },
  {
    "objectID": "resources.html#on-your-own-outside-of-the-course",
    "href": "resources.html#on-your-own-outside-of-the-course",
    "title": "Resources",
    "section": "",
    "text": "R: In order to utilize the RStudio Desktop application from your laptop (and not through the browser), you will first need to install the latest version of R.\nRStudio Desktop can be downloaded here. Note that this functions in the same manner as the RStudio Server by Posit Cloud, but utilizes the storage and compute of the laptop machine itself. Laptops vary in memory and storage space, which can dictate the speed and capability of data analysis, and this may result in slower processes with large datasets."
  },
  {
    "objectID": "course/python_tutorial.html",
    "href": "course/python_tutorial.html",
    "title": "Intro to Python",
    "section": "",
    "text": "You can import packages using “import” (analogous to ‘library’ in R).\n\nimport os\n\nYou can also assign “nicknames” to packages when you’re importing them.\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\nIf you only need certain functions in a package, you can also only import the module within the package that you need.\n\nimport matplotlib.pyplot as plt\nimport skimage.io as io",
    "crumbs": [
      "Spatial Analysis",
      "Intro to Python"
    ]
  },
  {
    "objectID": "course/python_tutorial.html#importing-packages",
    "href": "course/python_tutorial.html#importing-packages",
    "title": "Intro to Python",
    "section": "",
    "text": "You can import packages using “import” (analogous to ‘library’ in R).\n\nimport os\n\nYou can also assign “nicknames” to packages when you’re importing them.\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\nIf you only need certain functions in a package, you can also only import the module within the package that you need.\n\nimport matplotlib.pyplot as plt\nimport skimage.io as io",
    "crumbs": [
      "Spatial Analysis",
      "Intro to Python"
    ]
  },
  {
    "objectID": "course/python_tutorial.html#basic-data-structures",
    "href": "course/python_tutorial.html#basic-data-structures",
    "title": "Intro to Python",
    "section": "Basic data structures",
    "text": "Basic data structures\nA python list is like an array:\n\nxs = [3, 1, 2]   # Create a list\nprint(xs)\n\nOne difference between R and Python is that Python uses 0-indexing, meaning that the first element of the list is accessed using 0.\n\nprint(xs[0])\nprint(xs[-1])     # Negative indices count from the end of the list; prints \"2\"\n\nIn addition to accessing list elements one at a time, Python provides concise syntax to access sublists; this is known as slicing.\n\nnums = list(range(5))    # range is a built-in function that creates a list of integers\nprint(nums)         # Prints \"[0, 1, 2, 3, 4]\"\nprint(nums[2:4])    # Get a slice from index 2 to 4 (exclusive); prints \"[2, 3]\"\nprint(nums[2:])     # Get a slice from index 2 to the end; prints \"[2, 3, 4]\"\nprint(nums[:2])     # Get a slice from the start to index 2 (exclusive); prints \"[0, 1]\"\nprint(nums[:])      # Get a slice of the whole list; prints [\"0, 1, 2, 3, 4]\"\nprint(nums[:-1])    # Slice indices can be negative; prints [\"0, 1, 2, 3]\"\nnums[2:4] = [8, 9] # Assign a new sublist to a slice\nprint(nums)         # Prints \"[0, 1, 8, 9, 4]\"\n\nYou can loop over the elements of a list like this:\n\nanimals = ['cat', 'dog', 'monkey']\nfor animal in animals:\n    print(animal)\n\nIf you want access to the index of each element within the body of a loop, use the built-in enumerate function.\n\nanimals = ['cat', 'dog', 'monkey']\nfor idx, animal in enumerate(animals):\n    print('#{}: {}'.format(idx + 1, animal)) #in 'print' syntax, the variables after 'format' are printed where the '{}' are\n\nWhen programming, frequently we want to perform operations on every element of a list. As a simple example, consider the following code that computes square numbers:\n\nnums = [0, 1, 2, 3, 4]\nsquares = []\nfor x in nums:\n    squares.append(x ** 2)\nprint(squares)\n\nYou can make this code a lot simpler using a list comprehension:\n\nnums = [0, 1, 2, 3, 4]\nsquares = [x ** 2 for x in nums]\nprint(squares)\n\nList comprehensions can also contain conditions (“%” is the mod function, which performs division between two numbers and returns the remainder, ex. 6 % 3 = 0, 7 % 3 = 1):\n\nnums = [0, 1, 2, 3, 4]\neven_squares = [x ** 2 for x in nums if x % 2 == 0]\nprint(even_squares)\n\nAnother useful data structure is a dictionary. A dictionary stores (key, value) pairs. You can use it like this:\n\nd = {'cat': 'cute', 'dog': 'furry'}  # Create a new dictionary with some data in 'key':'value' pairs\nprint(d['cat'])       # Get an entry from a dictionary; prints \"cute\"\nprint('cat' in d)     # Check if a dictionary has a given key; prints \"True\"\n\n\nd['fish'] = 'wet'    # Create an entry in a dictionary\nprint(d['fish'])      # Access an entry in a dictionary, prints \"wet\"\n\nYou can iterate over the entries in a dictionary:\n\nd = {'person': 2, 'cat': 4, 'spider': 8}\nfor animal, legs in d.items(): # Here, we are deconstructing each key-value pair into variables called 'animal' and 'legs'\n    print('A {} has {} legs'.format(animal, legs))",
    "crumbs": [
      "Spatial Analysis",
      "Intro to Python"
    ]
  },
  {
    "objectID": "course/python_tutorial.html#defining-functions",
    "href": "course/python_tutorial.html#defining-functions",
    "title": "Intro to Python",
    "section": "Defining functions",
    "text": "Defining functions\nPython functions are defined using the def keyword. For example:\n\ndef sign(x):\n    if x &gt; 0:\n        return 'positive'\n    elif x &lt; 0:\n        return 'negative'\n    else:\n        return 'zero'\n\n\nfor x in [-1, 0, 1]:\n    print(sign(x))",
    "crumbs": [
      "Spatial Analysis",
      "Intro to Python"
    ]
  },
  {
    "objectID": "course/python_tutorial.html#numpy",
    "href": "course/python_tutorial.html#numpy",
    "title": "Intro to Python",
    "section": "Numpy",
    "text": "Numpy\nNumpy is the core library for scientific computing in Python. It provides a high-performance multidimensional array object, and tools for working with these arrays.\n\nimport numpy as np\n\nWe can initialize numpy arrays from nested Python lists, and access elements using square brackets:\n\na = np.array([[1,2,3],[4,5,6]])\nprint(a)\nprint(a.shape)\nprint(a[0, 0], a[0, 1], a[1, 0])\n\nBoolean array indexing lets you pick out arbitrary elements of an array. Frequently this type of indexing is used to select the elements of an array that satisfy some condition. Here is an example:\n\na = np.array([[1,2], [3, 4], [5, 6]])\n\nbool_idx = (a &gt; 2)  # Find the elements of a that are bigger than 2;\n                    # this returns a numpy array of Booleans of the same\n                    # shape as a, where each slot of bool_idx tells\n                    # whether that element of a is &gt; 2.\n\nprint(bool_idx)\n\n\n# We use boolean array indexing to construct a rank 1 array\n# consisting of the elements of a corresponding to the True values\n# of bool_idx\nprint(a[bool_idx])\n\n# We can do all of the above in a single concise statement:\nprint(a[a &gt; 2])\n\nNumpy is a powerful library that underlies a lot of machine learning packages in Python. To learn more, check out the documentation: https://numpy.org/doc/stable/user/quickstart.html.\nScipy is a collection of mathematical algorithms built on top of Numpy. For more tutorials and information, see https://scipy.org/.",
    "crumbs": [
      "Spatial Analysis",
      "Intro to Python"
    ]
  },
  {
    "objectID": "course/python_tutorial.html#pandas",
    "href": "course/python_tutorial.html#pandas",
    "title": "Intro to Python",
    "section": "Pandas",
    "text": "Pandas\nPandas is a powerful library for working with tabular data (similar to data frames in R).\n\nimport pandas as pd\n\nHere is a simple example:\n\ndata = {\n  \"calories\": [420, 380, 390],\n  \"duration\": [50, 40, 45]\n}\n\n# Load data into a DataFrame object:\ndf = pd.DataFrame(data)\ndf\n\nTo locate a specific row:\n\nprint(df.loc[0])\n\nAdd column to data frame:\n\ndf['group'] = ['group1','group1','group2']\ndf\n\nSubset a data frame:\n\n# Only keep rows in group 1\ndf.loc[df['group'] == 'group1']\n\nCount the number of rows in each group:\n\ndf.groupby('group').size()\n\nYou can also perform operations on groups of rows. For example, here, we are finding the mean calories in each group.\n\ndf.groupby('group')['calories'].mean()\n\nPandas dataframes are indexed (the numbers on the left hand side). Even when you subset a table, the indices do not change. For example, we can see that when we subset for group2 only, the index for that row stays the same.\n\ndf.loc[df['group'] == 'group2']\n\nIf we want to reset the index, we can use the reset_index function:\n\ndf.loc[df['group'] == 'group2'].reset_index()\n\nWe can merge 2 data frames using hte merge function:\n\nnew_data = {\n  \"group\": ['group1', 'group2'],\n  \"new_col\": [100, 200]\n}\n\n# Load data into a DataFrame object:\nnew_df = pd.DataFrame(new_data)\nnew_df\n\n\ndf.merge(new_df, on='group') # merge the two tables using the 'group' column\n\nTo read data directly from a file and load it as a pandas DataFrame:\n\npd.read_csv(\"example_data/cell_table.csv\")\n\nPandas has a lot more functionality. For more tutorials and information, see https://pandas.pydata.org/docs/index.html.",
    "crumbs": [
      "Spatial Analysis",
      "Intro to Python"
    ]
  },
  {
    "objectID": "course/python_tutorial.html#matplotlib-and-seaborn",
    "href": "course/python_tutorial.html#matplotlib-and-seaborn",
    "title": "Intro to Python",
    "section": "Matplotlib and Seaborn",
    "text": "Matplotlib and Seaborn\nMatplotlib is a plotting library. In this section, we will give a brief introduction to the matplotlib.pyplot module.\n\nimport matplotlib.pyplot as plt\n\nHere is a simple example:\n\n# Compute the x and y coordinates for points on a sine curve\nx = np.arange(0, 3 * np.pi, 0.1)\ny = np.sin(x)\n\n# Plot the points using matplotlib\nplt.plot(x, y)\nplt.show()\n\nYou can plot different things in the same figure using the subplot function. Here is an example:\n\n# Compute the x and y coordinates for points on sine and cosine curves\nx = np.arange(0, 3 * np.pi, 0.1)\ny_sin = np.sin(x)\ny_cos = np.cos(x)\n\n# Set up a subplot grid that has height 2 and width 1,\n# and set the first such subplot as active.\nplt.subplot(2, 1, 1)\n\n# Make the first plot\nplt.plot(x, y_sin)\nplt.title('Sine')\n\n# Set the second subplot as active, and make the second plot.\nplt.subplot(2, 1, 2)\nplt.plot(x, y_cos)\nplt.title('Cosine')\n\n# Show the figure.\nplt.show()\n\nSeaborn is another popular plotting package that can generate “prettier” plots (maybe similar to ‘ggplot2’ in R).\n\nimport seaborn as sns\n\n\n# Load an example dataset\ntips = sns.load_dataset(\"tips\")\n\n# Create a visualization\nsns.relplot(\n    data=tips,\n    x=\"total_bill\", y=\"tip\", col=\"time\",\n    hue=\"smoker\", style=\"smoker\", size=\"size\",\n)\nplt.show()\n\nFor more documentation on these two packages, see https://matplotlib.org/3.5.3/api/_as_gen/matplotlib.pyplot.html and https://seaborn.pydata.org/tutorial/introduction.html.",
    "crumbs": [
      "Spatial Analysis",
      "Intro to Python"
    ]
  },
  {
    "objectID": "course/python_tutorial.html#images",
    "href": "course/python_tutorial.html#images",
    "title": "Intro to Python",
    "section": "Images",
    "text": "Images\nThere are a few different packages that you can use for opening images. scikit-image (shorted as skimage) is a popular one.\n\nimport skimage.io as io\n\nHere, we are loading in an image.\n\nexample_image = io.imread(\"example_data/fov1/image_data/CD45.tiff\")\n\nWe can see that the image is just an array of numbers.\n\nexample_image\n\nWe can inspect the shape of the array (the shape of the image).\n\nexample_image.shape\n\nWe can also display the image.\n\nfig = plt.figure(figsize=(8,8))\nplt.imshow(example_image, origin=\"lower\", cmap='gray', vmax=np.quantile(example_image,0.99))\nplt.axis('off')\nplt.tight_layout()\n\nFor more documentation on scikit-image, see https://scikit-image.org/.",
    "crumbs": [
      "Spatial Analysis",
      "Intro to Python"
    ]
  },
  {
    "objectID": "course/python_tutorial.html#additional-exercises",
    "href": "course/python_tutorial.html#additional-exercises",
    "title": "Intro to Python",
    "section": "Additional exercises",
    "text": "Additional exercises\n\nRead in the cell tabe at “example_data/cell_table.csv”. How many unique types of cells are there? How many unique FOVs?\nFilter the cell table for cells in FOV2. How many CD4 T cells are there?\nMake a dictionary mapping each cell ID to its cell type.\nMake a bargraph showing the number of cells of each cell type in FOV1.\nRead in some example images in the “example_data” folder and play with the “vmax” parameter to see how the image changes.",
    "crumbs": [
      "Spatial Analysis",
      "Intro to Python"
    ]
  },
  {
    "objectID": "course/2024/3_clustering.html",
    "href": "course/2024/3_clustering.html",
    "title": "Clustering concepts and correlation",
    "section": "",
    "text": "Understanding distance metrics is critical for clustering and dimensionality reduction because these techniques heavily rely on measuring the similarity or dissimilarity between data points. Distance metrics define how “close” or “far” two points are in the feature space, and they play a crucial role in determining the grouping of similar points in clustering algorithms and the preservation of local and global structures in dimensionality reduction techniques.\n\n\nManhattan distance, or L1 distance, measures the distance between two points by summing the absolute differences of their Cartesian coordinates. It is equivalent to the total number of moves required to go from one point to another if only axis-aligned moves (up, down, left, right) are allowed, mimicking a city grid.\n\n\n\nLess influenced by outliers compared to Euclidean distance.\nUseful in high-dimensional spaces.\n\n\n\n\n\nEuclidean distance, also known as the L2 distance, is the most common metric used to measure the straight-line distance between two points in Euclidean space. It is the default distance measure in many analytical applications.\n\n\n\nMeasures the shortest path between points.\nSensitive to outliers.\nUsed in default settings for many algorithms like K-means clustering.\n\n\n\n\n\n# Load the iris dataset\ndata(iris)\niris_numeric &lt;- iris[, 1:4]  # Exclude the species column\n\n# Euclidean distance\neuclidean_distances &lt;- dist(iris_numeric, method = \"euclidean\")\n\n# Manhattan distance\nmanhattan_distances &lt;- dist(iris_numeric, method = \"manhattan\")"
  },
  {
    "objectID": "course/2024/3_clustering.html#manhattan-distance",
    "href": "course/2024/3_clustering.html#manhattan-distance",
    "title": "Clustering concepts and correlation",
    "section": "",
    "text": "Manhattan distance, or L1 distance, measures the distance between two points by summing the absolute differences of their Cartesian coordinates. It is equivalent to the total number of moves required to go from one point to another if only axis-aligned moves (up, down, left, right) are allowed, mimicking a city grid.\n\n\n\nLess influenced by outliers compared to Euclidean distance.\nUseful in high-dimensional spaces."
  },
  {
    "objectID": "course/2024/3_clustering.html#euclidean-distance",
    "href": "course/2024/3_clustering.html#euclidean-distance",
    "title": "Clustering concepts and correlation",
    "section": "",
    "text": "Euclidean distance, also known as the L2 distance, is the most common metric used to measure the straight-line distance between two points in Euclidean space. It is the default distance measure in many analytical applications.\n\n\n\nMeasures the shortest path between points.\nSensitive to outliers.\nUsed in default settings for many algorithms like K-means clustering."
  },
  {
    "objectID": "course/2024/3_clustering.html#dist-in-r",
    "href": "course/2024/3_clustering.html#dist-in-r",
    "title": "Clustering concepts and correlation",
    "section": "",
    "text": "# Load the iris dataset\ndata(iris)\niris_numeric &lt;- iris[, 1:4]  # Exclude the species column\n\n# Euclidean distance\neuclidean_distances &lt;- dist(iris_numeric, method = \"euclidean\")\n\n# Manhattan distance\nmanhattan_distances &lt;- dist(iris_numeric, method = \"manhattan\")"
  },
  {
    "objectID": "course/2024/3_clustering.html#k-means-clustering",
    "href": "course/2024/3_clustering.html#k-means-clustering",
    "title": "Clustering concepts and correlation",
    "section": "K-means clustering",
    "text": "K-means clustering\nK-means clustering is a popular unsupervised machine learning algorithm used for partitioning a dataset into a predetermined number of clusters. The goal of k-means clustering is to group data points into clusters such that data points within the same cluster are more similar to each other than to those in other clusters. The algorithm works iteratively to assign each data point to the nearest cluster centroid (center point of a cluster) and then update the centroid based on the mean of all data points assigned to that cluster. This process continues until the centroids no longer change significantly, or a specified number of iterations is reached.\nK-means has some limitations, such as sensitivity to the initial random selection of centroids and the need to specify the number of clusters beforehand. Additionally, k-means may not perform well on datasets with non-spherical or irregularly shaped clusters.\nRunning on K-means on the iris data set:\n# Load data\ndata(iris)\niris_data &lt;- iris[, -5]\n\n# Run k-means\nset.seed(123)\nkmeans_result &lt;- kmeans(iris_data, centers = 3, nstart = 20)\n\n# Results\nprint(kmeans_result)\ntable(Cluster = kmeans_result$cluster, Species = iris$Species)\n\n# Visualization\nlibrary(ggplot2)\nggplot(iris, aes(Sepal.Length, Sepal.Width, color = factor(kmeans_result$cluster))) +\n  geom_point(alpha = 0.5) +\n  labs(color = 'Cluster') +\n  ggtitle(\"K-means Clustering of the Iris Dataset\")\n\n\n\nKmeans on iris data"
  },
  {
    "objectID": "course/2024/3_clustering.html#heirarchical-clustering",
    "href": "course/2024/3_clustering.html#heirarchical-clustering",
    "title": "Clustering concepts and correlation",
    "section": "Heirarchical Clustering",
    "text": "Heirarchical Clustering\nHierarchical clustering is a method used in unsupervised learning to group similar data points into clusters based on their pairwise distances or similarities. The main idea behind hierarchical clustering is to build a hierarchy of clusters, where each data point starts in its own cluster and pairs of clusters are progressively merged until all points belong to a single cluster.\nThe result of hierarchical clustering is often visualized using a dendrogram, which is a tree-like diagram that illustrates the hierarchical structure of the clusters.\nLets use hclust on a set of TCR sequences, where the distance between each sequence is defined as the edit distance. We can plot a dendrogram highlighting sequence similarity.\n# Install and load necessary packages\nif (!requireNamespace(\"stringdist\", quietly = TRUE)) {\n  install.packages(\"stringdist\")\n}\nlibrary(stringdist)\n\n# Define TCR sequences\ntcr_sequences &lt;- c(\"CASSLGTQYEQYF\", \"CASSLGTEAFF\", \"CASSQETQYEQYF\", \"CASSLRTDTQYF\")\nnames(tcr_sequences) &lt;- tcr_sequences  # Use sequences as labels\n\n# Calculate pairwise string distances using the Levenshtein method\ndist_matrix &lt;- stringdistmatrix(tcr_sequences, tcr_sequences, method = \"lv\")\n\n# Perform hierarchical clustering using the complete linkage method\nhc &lt;- hclust(as.dist(dist_matrix), method = \"complete\")\n\n# Plot the dendrogram\nplot(hc, main = \"Hierarchical Clustering of TCR Sequences\", sub = \"\", xlab = \"\", ylab = \"Distance\",\n     labels = names(tcr_sequences), hang = -1)  # Ensure labels hang below the plot\n\n\n\nSequence Distance Clustering\n\n\nWe can create a more complex simulated dataset of simulated single cell gene expression data. In this case, we have two cell types, and expect that the resulting dendrogram produced from the clustering should show clear differences between these cell types. Finally, we can plot the expression values in heatmap to visualize the difference between the genes across cells. The ordering of the rows is dictated by the dendrogram, drawing more similar cells closer together, allowing us to see the expression patterns that define each cell type.\n# Install and load pheatmap if not already installed\nif (!requireNamespace(\"pheatmap\", quietly = TRUE)) {\n    install.packages(\"pheatmap\")\n}\nlibrary(pheatmap)\n\n# Set seed for reproducibility\nset.seed(42)\n\n# Define parameters\nnum_genes &lt;- 100\nnum_samples &lt;- 50  # 25 T cells + 25 Cancer cells\n\n# Simulate base gene expression\ngene_expression &lt;- matrix(rnorm(num_genes * num_samples, mean = 10, sd = 2), \n                          nrow = num_genes, ncol = num_samples)\n\n# Introduce differences in expression between the two groups\ngene_expression[81:100, 1:25] &lt;- gene_expression[81:100, 1:25] + 2  # T cells\ngene_expression[81:100, 26:50] &lt;- gene_expression[81:100, 26:50] - 2  # Cancer cells\n\n# Label rows and columns\nrownames(gene_expression) &lt;- paste(\"Gene\", 1:num_genes, sep = \"\")\ncolnames(gene_expression) &lt;- c(paste(\"T_Cell\", 1:25, sep = \"\"), paste(\"Cancer_Cell\", 1:25, sep = \"\"))\n\n# Transpose the gene expression matrix\ntransposed_gene_expression &lt;- t(gene_expression)\n\n# Creating a heatmap with clustering and annotation\npheatmap(transposed_gene_expression, \n         show_rownames = TRUE, \n         show_colnames = FALSE, \n         clustering_distance_rows = \"euclidean\",\n         cluster_rows  = TRUE,\n         cluster_cols  = FALSE,\n         main = \"Heatmap of Gene Expression with Clustering\")"
  },
  {
    "objectID": "course/2024/3_clustering.html#principal-component-analysis-pca",
    "href": "course/2024/3_clustering.html#principal-component-analysis-pca",
    "title": "Clustering concepts and correlation",
    "section": "Principal Component Analysis (PCA)",
    "text": "Principal Component Analysis (PCA)\nPCA is a widely used dimension reduction technique that transforms high-dimensional data into a lower-dimensional representation by identifying the principal components that capture the maximum variance in the data. These principal components are orthogonal to each other and can be used to visualize the data in lower dimensions.\n# Load necessary packages\nlibrary(ggplot2)\n\n# Load data\ndata(iris)\niris_data &lt;- iris[, 1:4]\n\n# PCA\npca_results &lt;- prcomp(iris_data, center = TRUE, scale. = TRUE)\nprint(summary(pca_results))\n\n\n# Scatter plot of the first two PCs\npc_df &lt;- data.frame(PC1 = pca_results$x[,1], PC2 = pca_results$x[,2], Species = iris$Species)\nggplot(pc_df, aes(x = PC1, y = PC2, color = Species)) +\n    geom_point() +\n    labs(title = \"PCA of Iris Dataset\",\n         x = \"Principal Component 1\",\n         y = \"Principal Component 2\") +\n    theme_minimal()\n\n\n\nPCA example"
  },
  {
    "objectID": "course/2024/3_clustering.html#t-sne",
    "href": "course/2024/3_clustering.html#t-sne",
    "title": "Clustering concepts and correlation",
    "section": "t-SNE",
    "text": "t-SNE\nt-SNE (t-Distributed Stochastic Neighbor Embedding) is a dimensionality reduction technique commonly used in bioinformatics to visualize high-dimensional data, such as gene expression profiles or single-cell RNA sequencing (scRNA-seq) data, in a lower-dimensional space. t-SNE aims to preserve local structure and clusterings in the data by modeling similarities between data points in the high-dimensional space and embedding them into a lower-dimensional space. In t-SNE, similarities between data points are represented by conditional probabilities that two points are similar given their high-dimensional representations. t-SNE iteratively adjusts the positions of data points in the lower-dimensional space to minimize the difference between the conditional probabilities of pairwise similarities in the high-dimensional and low-dimensional spaces.\n# Load the Rtsne package\nlibrary(Rtsne)\n\nunique_iris &lt;- unique(iris[, 1:4])\n\n# Run t-SNE on the deduplicated data\nset.seed(42)  # for reproducibility\ntsne_results &lt;- Rtsne(unique_iris, dims = 2, perplexity = 30, verbose = TRUE)\n\n# Create a data frame for plotting (assuming you want to include species labels)\ntsne_data &lt;- data.frame(tsne_results$Y)\n\n# Assuming you want to add back the species information\n# This assumes that species information was not a factor in duplicates\n# If species data was part of the duplication, handle accordingly\nspecies_data &lt;- iris[!duplicated(iris[, 1:4]), \"Species\"]\ntsne_data$Species &lt;- species_data\n\n# Plot the results using ggplot2\nlibrary(ggplot2)\nggplot(tsne_data, aes(x = X1, y = X2, color = Species)) +\n  geom_point(alpha = 0.8) +\n  labs(title = \"Iris dataset t-SNE plot\",\n       x = \"t-SNE 1\", y = \"t-SNE 2\") +\n  theme_minimal()\n\n\n\nt-SNE example"
  },
  {
    "objectID": "course/2024/3_clustering.html#umap",
    "href": "course/2024/3_clustering.html#umap",
    "title": "Clustering concepts and correlation",
    "section": "UMAP",
    "text": "UMAP\nUMAP (Uniform Manifold Approximation and Projection) is a modern technique for dimensionality reduction that is particularly useful for visualizing clusters or groups in high-dimensional data. Similar to t-SNE, UMAP focuses on preserving the local structure of the data but also tries to retain more of the global structure compared to t-SNE. UMAP is based on manifold learning and operates under the assumption that the data is uniformly distributed on a locally connected Riemannian manifold.\n# Install and load umap if not already installed\nif (!requireNamespace(\"umap\", quietly = TRUE)) {\n    install.packages(\"umap\")\n}\n\nlibrary(umap)\nlibrary(ggplot2)\n\n# Load data\ndata(iris)\n\n# Run UMAP\nset.seed(42)  # for reproducibility\numap_results &lt;- umap(iris[, 1:4])\n\n# Create a data frame for plotting\niris_umap &lt;- data.frame(umap_results$layout)\niris_umap$Species = iris$Species\n\nggplot(iris_umap, aes(x = X1, y = X2, color = Species)) +\n  geom_point(alpha = 0.8) +\n  labs(title = \"Iris Dataset UMAP Plot\",\n       x = \"UMAP 1\", y = \"UMAP 2\") +\n  theme_minimal()\n\n\n\nUMAP example"
  },
  {
    "objectID": "course/2024/3_clustering.html#spearman-vs.-pearson-correlation",
    "href": "course/2024/3_clustering.html#spearman-vs.-pearson-correlation",
    "title": "Clustering concepts and correlation",
    "section": "Spearman vs. Pearson Correlation",
    "text": "Spearman vs. Pearson Correlation\n\nPearson Correlation: Measures the linear relationship between two variables. It assumes that the variables are normally distributed and have a linear relationship.\n\n# Generate example data\nset.seed(42)\nx &lt;- rnorm(100)  # Generate 100 random numbers from a standard normal distribution\ny &lt;- x + rnorm(100, mean = 0, sd = 0.5)  # Create y as a noisy version of x\n\n# Calculate Pearson correlation coefficient\npearson_correlation &lt;- cor(x, y, method = \"pearson\")\nprint(paste(\"Pearson correlation coefficient:\", round(pearson_correlation, 2)))\n\nSpearman Correlation: Measures the monotonic relationship between two variables. It does not assume linearity and is more robust to outliers and non-normal distributions.\n\n# Generate example data\nset.seed(42)\nx &lt;- rnorm(100)  # Generate 100 random numbers from a standard normal distribution\ny &lt;- x + rnorm(100, mean = 0, sd = 0.5)  # Create y as a noisy version of x\n\n# Calculate Spearman correlation coefficient\nspearman_correlation &lt;- cor(x, y, method = \"spearman\")\nprint(paste(\"Spearman correlation coefficient:\", round(spearman_correlation, 2)))"
  },
  {
    "objectID": "course/2024/3_clustering.html#geom_smooth",
    "href": "course/2024/3_clustering.html#geom_smooth",
    "title": "Clustering concepts and correlation",
    "section": "geom_smooth",
    "text": "geom_smooth\nSimple example using iris.\nlibrary(ggplot2)\n\n# Create a scatter plot with a smoothed line\nggplot(data = iris, aes(x = Sepal.Length, y = Sepal.Width, color = Species)) +\n  geom_point() +  # Add scatter points\n  geom_smooth(method = \"lm\", se = FALSE) +  # Add linear regression line\n  labs(title = \"Scatter Plot with Smoothed Line\", x = \"Sepal Length\", y = \"Sepal Width\")\n\n\n\ngeom_smooth iris\n\n\nSimulate data points from two groups and use geom_smooth with a different model for each group.\n# Load required libraries\nlibrary(ggplot2)\n\n# Generate example data\nset.seed(42)\nn &lt;- 100  # Number of data points per group\nx &lt;- 1:n  # x values\ngroup &lt;- rep(c(\"linear\", \"sinusoidal\"), each = n)  # Group labels\ny &lt;- c(0.5 * x + rnorm(n, mean = 0, sd = 5),  # Group 'linear' with a linear trend\n       2 * sin(seq(0, 2 * pi, length.out = n)) + rnorm(n, mean = 0, sd = 0.5))  # Group 'sinusoidal' with a sinusoidal trend\n\n# Standardize y-values within each group\ny &lt;- ave(y, group, FUN = scale)\n\n# Create a data frame\ndf &lt;- data.frame(x = rep(x, 2), y = y, group = rep(group, 2))\n\n# Plot the data with smoothed lines and confidence intervals for each group\nggplot(data = df, aes(x = x, y = y, color = group)) +\n  geom_point() +  # Add scatter points\n  geom_smooth(data = subset(df, group == \"linear\"), method = \"lm\", se = TRUE) +  # Add linear smoothed line with confidence intervals\n  geom_smooth(data = subset(df, group == \"sinusoidal\"), method = \"loess\", se = TRUE) +  # Add sinusoidal smoothed line with confidence intervals\n  labs(title = \"Scatter Plot with Smoothed Lines and Confidence Intervals by Group\", x = \"X\", y = \"Y (Standardized)\") +\n  theme_minimal()\n\n\n\ngeom_smooth Simulated"
  },
  {
    "objectID": "course/2024/3_clustering.html#linear-regression",
    "href": "course/2024/3_clustering.html#linear-regression",
    "title": "Clustering concepts and correlation",
    "section": "Linear Regression",
    "text": "Linear Regression\nStatistical model to estimate the linear relationship between a dependent variable and a set of independent variables. The goal is find the best fit line by fitting the observed data to a linear equation.\n\n# Generate example data with more noise\nset.seed(42)\ndf &lt;- data.frame(x = 1:100,                    # Independent variable\n                 y = 2 * df$x + rnorm(100, mean = 0, sd = 30))    # Dependent variable with more noise\n\nmodel &lt;- lm(y ~ x, data = df)\n\n# Visualize the data and fitted line\nggplot(df, aes(x = x, y = y)) +\n  geom_point() +                              # Add scatter points\n  geom_smooth(method = \"lm\", se = FALSE) +    # Add fitted line\n  labs(title = \"Linear Regression Example\", x = \"x\", y = \"y\")\n\n\n\nLinear regression example"
  },
  {
    "objectID": "course/2024/3_clustering.html#example-code",
    "href": "course/2024/3_clustering.html#example-code",
    "title": "Clustering concepts and correlation",
    "section": "Example Code",
    "text": "Example Code\n# Load required libraries\nlibrary(ggpubr)\n\n# Example dataframe\ndf &lt;- data.frame(\n  Gene1 = c(1, 2, 3, 4, 5),\n  Gene2 = c(5, 4, 3, 2, 1),\n  Gene3 = c(2, 3, 4, 5, 6)\n)\n\n# Perform PCA\npca_result &lt;- prcomp(df)\n\n# Plot PCA\nplot(pca_result$x[,1], pca_result$x[,2], \n     xlab = \"PC1\", ylab = \"PC2\", \n     main = \"PCA Plot\")\n\n# Perform clustering\n# Example clustering algorithm: k-means\nkmeans_result &lt;- kmeans(df, centers = 2)\n\n# Plot clustering\nplot(df, col = kmeans_result$cluster, \n     main = \"Clustering Plot\")\n\n# Add correlation statistics to a plot\n# Example plot\nggscatter(df, x = \"Gene1\", y = \"Gene2\", \n          add = \"reg.line\", \n          cor.coef = TRUE, \n          cor.method = \"spearman\", \n          cor.coeff.args = list(method = \"spearman\"))"
  },
  {
    "objectID": "course/2024/4_qctrouble.html",
    "href": "course/2024/4_qctrouble.html",
    "title": "Common challenges and additional resources",
    "section": "",
    "text": "The key to accomplishing any analysis is to start by understand what your data looks like and how it’s organized. This might include:\n\nWhat type of files are you working with?\nHow do they get loaded into R?\nWhat is the size of the dataset?\nWhat types of questions can I ask of the data?\n\n\nlibrary(dplyr)\n\nfile &lt;- \"/cloud/project/data/single_cell_rna/cancer_cell_id/mcb6c-exome-somatic.variants.annotated.clean.filtered.tsv\"\nprint(file)\n\n# File is a \"tsv\" file -&gt; Tab-delimited file\nread_tsv &lt;- read.csv(file, sep = '\\t')\n\n# Look at the file: View(), head(), or click on it to the right\nView(read_tsv)\nhead(read_tsv)\n\n# Understand the variables and data structure: typeof(), str(), colnames()\ntypeof(read_tsv)\nstr(read_tsv)\ncolnames(read_tsv)"
  },
  {
    "objectID": "course/2024/4_qctrouble.html#nan-and-missing-data",
    "href": "course/2024/4_qctrouble.html#nan-and-missing-data",
    "title": "Common challenges and additional resources",
    "section": "NaN and missing data",
    "text": "NaN and missing data\nDealing with missing data and NaN (Not a Number) values is a common challenge in R programming. These values can affect the results of your analyses and visualizations. It’s essential to handle missing data appropriately, either by imputing them or excluding them from the analysis, depending on the context.\nExample:\n# Creating a dataframe with missing values\ndf &lt;- data.frame(\n  x = c(1, 2, NA, 4),\n  y = c(5, NA, 7, 8)\n)\n\n# Check for missing values\nsum(is.na(df))\n\n# Understanding missing or sparse information\nsummary(read_tsv)\nread_tsv[!complete.cases(read_tsv),]"
  },
  {
    "objectID": "course/2024/4_qctrouble.html#data-organization-and-structure-factors",
    "href": "course/2024/4_qctrouble.html#data-organization-and-structure-factors",
    "title": "Common challenges and additional resources",
    "section": "Data organization and structure: Factors",
    "text": "Data organization and structure: Factors\nColumns that contain strings are automatically read in as character vectors and are arranged alphanumericall. Factors assign a logical order to a series of samples.\nfile &lt;- \"/cloud/project/data/single_cell_rna/cancer_cell_id/mcb6c-exome-somatic.variants.annotated.clean.filtered.tsv\"\nprint(file)\n\n# File is a \"tsv\" file -&gt; Tab-delimited file\nread_tsv &lt;- read.csv(file, sep = '\\t')\n\n# Organize the chromosomes to the correct order\n# If CHROM is a character vector, the chromosomes are not ordered properly\nstr(read_tsv) # Shows that CHROM variable is a character vector\nggplot(read_tsv, aes(x = CHROM)) +\n  geom_bar() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n# make CHROM a factor\nread_tsv$CHROM &lt;- factor(read_tsv$CHROM, levels = paste0(\"chr\", c(1:22, 'X','Y')))\nstr(read_tsv) # shows that CHROM variable is a factor with a set order of character strings\nggplot(read_tsv, aes(x = CHROM)) +\n  geom_bar() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))"
  },
  {
    "objectID": "course/2024/4_qctrouble.html#missing-packages-or-dependencies",
    "href": "course/2024/4_qctrouble.html#missing-packages-or-dependencies",
    "title": "Common challenges and additional resources",
    "section": "Missing packages or dependencies",
    "text": "Missing packages or dependencies\nWhen running R code, you may encounter errors related to missing packages or dependencies. This occurs when you try to use functions or libraries that are not installed on your system. This will commonly look like a red error stating “There is no package…” Installing the required packages using install.packages(“package_name”) can resolve this issue.\n# Trying to use a function from an uninstalled package\nlibrary(ggplot2)\nggplot(df, aes(x, y)) + geom_point()\n# Error: there is no package called 'ggplot2'"
  },
  {
    "objectID": "course/2024/4_qctrouble.html#stack-exchange",
    "href": "course/2024/4_qctrouble.html#stack-exchange",
    "title": "Common challenges and additional resources",
    "section": "Stack Exchange",
    "text": "Stack Exchange\nOnline communities like Stack Exchange, particularly the Stack Overflow platform, are excellent resources for getting help with R programming. You can search for solutions to specific problems or ask questions if you’re facing challenges. If you attempt to Google what you’re trying to accomplish with your dataset, StackExchange often includes responses from others who may have gone through this effort before. With the open crowdsourcing of troubleshooting, these responses that work for others are “upvoted” so that you can try to adapt the code to work for your dataset. Importantly, take care to change the names of variables and file paths in any code that you try to implement from others."
  },
  {
    "objectID": "course/2024/4_qctrouble.html#chatgpt",
    "href": "course/2024/4_qctrouble.html#chatgpt",
    "title": "Common challenges and additional resources",
    "section": "ChatGPT",
    "text": "ChatGPT\nChatGPT is an AI assistant that can provide guidance and answer questions related to R programming. You can ask for clarification on concepts, debugging assistance, or advice on best practices. If you copy/paste an error into ChatGPT, it often tries to debug without any other preface. However, if you try to explain exactly what your code is trying to accomplish, it can be a helpful way to debug your help."
  },
  {
    "objectID": "course/2024/4_qctrouble.html#good-practices",
    "href": "course/2024/4_qctrouble.html#good-practices",
    "title": "Common challenges and additional resources",
    "section": "Good practices",
    "text": "Good practices\n##Commenting your code Adding comments to your code is crucial for making it more understandable to yourself and others. Comments provide context and explanations for the code’s functionality, making it easier to troubleshoot and maintain."
  },
  {
    "objectID": "course/2024/4_qctrouble.html#publicly-accessible-resources",
    "href": "course/2024/4_qctrouble.html#publicly-accessible-resources",
    "title": "Common challenges and additional resources",
    "section": "Publicly accessible resources",
    "text": "Publicly accessible resources\n\nGithub\nGithub hosts numerous repositories containing R scripts, packages, and projects. Browsing through repositories and contributing to open-source projects can help you learn from others’ code and collaborate with the R community. This can be a direct way to make your code available upon publication, according to journal practices."
  },
  {
    "objectID": "course/2024/4_qctrouble.html#codefights",
    "href": "course/2024/4_qctrouble.html#codefights",
    "title": "Common challenges and additional resources",
    "section": "CodeFights",
    "text": "CodeFights\nCodeFights (now CodeSignal) offers coding challenges and exercises in R and other programming languages. Practicing coding problems can help reinforce your skills and improve your problem-solving abilities."
  },
  {
    "objectID": "course/2024/2_basicplotting.html",
    "href": "course/2024/2_basicplotting.html",
    "title": "Basic plotting and statistics",
    "section": "",
    "text": "Long and wide data formats are two common ways of structuring data, each with its own advantages and use cases.\n\n\nIn the long format, also known as the “tidy” format, each observation is represented by a single row in the dataset. This format is characterized by having:\n\nMultiple rows, each corresponding to a single observation or measurement.\nOne column for the variable being measured.\nAdditional columns to store metadata or grouping variables.\n\nAdvantages:\n\nFacilitates easy analysis and manipulation, especially when using tools like Tidyverse packages in R.\nSuitable for data that follow the “one observation per row” principle, such as time series or longitudinal data.\n\n\n\n\nIn the wide format, each observation is represented by a single row, but with multiple columns corresponding to different variables. This format is characterized by:\n\nOne row per observation.\nEach variable is represented by a separate column.\n\nAdvantages:\n\nCan be easier to understand for simple datasets with few variables.\nMay be more convenient for certain types of analysis or visualization.\n\n\n\n\nThe choice between long and wide formats depends on factors such as the nature of the data, the analysis tasks, and personal preference. Long format is often preferred for its flexibility and compatibility with modern data analysis tools, while wide format may be suitable for simpler datasets or specific analysis requirements."
  },
  {
    "objectID": "course/2024/2_basicplotting.html#long-and-wide-data-formats",
    "href": "course/2024/2_basicplotting.html#long-and-wide-data-formats",
    "title": "Basic plotting and statistics",
    "section": "",
    "text": "Long and wide data formats are two common ways of structuring data, each with its own advantages and use cases.\n\n\nIn the long format, also known as the “tidy” format, each observation is represented by a single row in the dataset. This format is characterized by having:\n\nMultiple rows, each corresponding to a single observation or measurement.\nOne column for the variable being measured.\nAdditional columns to store metadata or grouping variables.\n\nAdvantages:\n\nFacilitates easy analysis and manipulation, especially when using tools like Tidyverse packages in R.\nSuitable for data that follow the “one observation per row” principle, such as time series or longitudinal data.\n\n\n\n\nIn the wide format, each observation is represented by a single row, but with multiple columns corresponding to different variables. This format is characterized by:\n\nOne row per observation.\nEach variable is represented by a separate column.\n\nAdvantages:\n\nCan be easier to understand for simple datasets with few variables.\nMay be more convenient for certain types of analysis or visualization.\n\n\n\n\nThe choice between long and wide formats depends on factors such as the nature of the data, the analysis tasks, and personal preference. Long format is often preferred for its flexibility and compatibility with modern data analysis tools, while wide format may be suitable for simpler datasets or specific analysis requirements."
  },
  {
    "objectID": "course/2024/2_basicplotting.html#long-to-wide",
    "href": "course/2024/2_basicplotting.html#long-to-wide",
    "title": "Basic plotting and statistics",
    "section": "Long to Wide",
    "text": "Long to Wide\nlibrary(tidyr)\n\n# Example long format data\nlong_data &lt;- data.frame(\n  Subject = c(\"A\", \"A\", \"B\", \"B\"),\n  Time = c(1, 2, 1, 2),\n  Measurement = c(10, 15, 12, 18)\n)\n\n# Convert long format data to wide format\nwide_data &lt;- spread(long_data, key = Time, value = Measurement)\n\n# View the wide format data\nprint(wide_data)"
  },
  {
    "objectID": "course/2024/2_basicplotting.html#wide-to-long",
    "href": "course/2024/2_basicplotting.html#wide-to-long",
    "title": "Basic plotting and statistics",
    "section": "Wide to Long",
    "text": "Wide to Long\nlibrary(tidyr)\n\n# Example wide format data\nwide_data &lt;- data.frame(\n  Subject = c(\"A\", \"B\"),\n  Time1 = c(10, 12),\n  Time2 = c(15, 18)\n)\n\n# Convert wide format data to long format\nlong_data &lt;- gather(wide_data, key = Time, value = Measurement, -Subject)\n\n# View the long format data\nprint(long_data)"
  },
  {
    "objectID": "course/2024/2_basicplotting.html#joins-and-merging-of-data-in-tidyverse",
    "href": "course/2024/2_basicplotting.html#joins-and-merging-of-data-in-tidyverse",
    "title": "Basic plotting and statistics",
    "section": "Joins and Merging of Data in Tidyverse",
    "text": "Joins and Merging of Data in Tidyverse\nJoins and merging are common operations used to combine multiple datasets based on common variables or keys. In Tidyverse, these operations are typically performed using functions from the dplyr package.\n\nTypes of Joins:\n\n\n\nTypes of Joins\n\n\n\nInner Join (inner_join()):\nAn inner join combines rows from two datasets where there is a match based on a common key, retaining only the rows with matching keys from both datasets.\n\n\nLeft Join (left_join()):\nA left join combines all rows from the first (left) dataset with matching rows from the second (right) dataset based on a common key. If there is no match in the second dataset, missing values are filled in.\n\n\nRight Join (right_join()):\nSimilar to a left join, but it retains all rows from the second (right) dataset and fills in missing values for non-matching rows from the first (left) dataset.\n\n\nFull Join (full_join()):\nA full join combines all rows from both datasets, filling in missing values where there are no matches.\n\n\nSemi-Join (semi_join()):\nA semi-join returns only rows from the first dataset where there are matching rows in the second dataset, based on a common key.\n\n\nAnti-Join (anti_join()):\nAn anti-join returns only rows from the first dataset that do not have matching rows in the second dataset, based on a common key.\n\n\n\nMerging Data:\n\nMerge (merge()):\nThe merge() function is a base R function used to merge datasets based on common columns or keys. It performs similar operations to joins in dplyr, but with slightly different syntax and behavior.\n\n\n\nExample:\nlibrary(dplyr)\n\n# Example datasets\ndf1 &lt;- data.frame(ID = c(1, 2, 3), Name = c(\"Alice\", \"Bob\", \"Charlie\"))\ndf2 &lt;- data.frame(ID = c(2, 3, 4), Score = c(85, 90, 95))\n\n# Inner join\ninner_merged &lt;- inner_join(df1, df2, by = \"ID\")\n\n# Left join\nleft_merged &lt;- left_join(df1, df2, by = \"ID\")\n\n# Right join\nright_merged &lt;- right_join(df1, df2, by = \"ID\")\n\n# Full join\nfull_merged &lt;- full_join(df1, df2, by = \"ID\")\n\n# Semi-join\nsemi_merged &lt;- semi_join(df1, df2, by = \"ID\")\n\n# Anti-join\nanti_merged &lt;- anti_join(df1, df2, by = \"ID\")"
  },
  {
    "objectID": "course/2024/2_basicplotting.html#ggplot2",
    "href": "course/2024/2_basicplotting.html#ggplot2",
    "title": "Basic plotting and statistics",
    "section": "ggplot2",
    "text": "ggplot2\nThe core idea behind ggplot2 is the concept of a “grammar of graphics”. This concept provides a systematic way to describe and build graphical presentations such as charts and plots. The grammar itself is a set of independent components that can be composed in many different ways. This grammar includes elements like:\n\nData: The raw data that you want to visualize.\nAesthetics (aes): Defines how data are mapped to color, size, shape, and other visual properties.\nGeometries (geom): The geometric objects in a plot—lines, points, bars, etc.\nScales: Transformations applied to data before it is visualized, including scales for colors, sizes, and shapes.\nCoordinate systems: The space in which data is plotted.\nFacets: Used for creating plots with multiple panels (small multiple plots).\nStatistical transformations (stat): Summary statistics that can be applied to data before it is visualized, such as counting or averaging.\nThemes: Visual styles and layout configurations for the plot.\n\nHere’s how you generally use ggplot2 to create a plot:\n\nStart with ggplot(): Set up the data and, optionally, define default mappings between variables and their aesthetics.\nAdd layers: Add layers to the plot using geom_ functions, such as geom_point() for scatter plots, geom_line() for line graphs, and so on.\n\nAdjust the scales: Customize the scales used for aesthetics such as color, size, and x-y coordinates.\nModify the coordinate system: Choose a coordinate system.\nAdd facets: If necessary, add facets to create a multi-panel plot.\nApply a theme: Customize the appearance of the plot using themes.\n\nlibrary(ggplot2)\n\n# Sample data\ndf &lt;- data.frame(\n  x = rnorm(100),\n  y = rnorm(100),\n  group = factor(rep(1:2, each = 50))\n)\n\n# Creating a scatter plot\nggplot(df, aes(x = x, y = y, color = group)) + \n  geom_point() +\n  theme_minimal() +\n  labs(title = \"Scatter Plot Example\", x = \"X Axis\", y = \"Y Axis\")\n\n\n\nScatter"
  },
  {
    "objectID": "course/2024/2_basicplotting.html#histogram",
    "href": "course/2024/2_basicplotting.html#histogram",
    "title": "Basic plotting and statistics",
    "section": "Histogram",
    "text": "Histogram\nLet’s simulate some TCR clonotype data. We will create a dataset where each TCR has a randomly generated number of cells associated with it, representing the clone size. After generating the data, we’ll use the hist() function from base R to plot a histogram of the clone sizes.\nlibrary(dplyr)\n\n# Step 1: Simulate data\nset.seed(123)  # Set seed for reproducibility\nnum_clonotypes &lt;- 100  # Specify the number of different clonotypes\n\n# Create a data frame with random cell counts for each clonotype\ntcr_data &lt;- tibble(\n  clonotype = paste(\"TCR\", seq_len(num_clonotypes), sep=\"\"),\n  cell_count = sample(1:1000, num_clonotypes, replace=TRUE)  # Random cell counts between 1 and 1000\n)\n\n# Step 2: Create a histogram of clone sizes\nhist(tcr_data$cell_count, \n     breaks=20,  # You can adjust the number of breaks to change bin sizes\n     col=\"skyblue\", \n     main=\"Histogram of TCR Clone Sizes\", \n     xlab=\"Clone Size (Number of Cells)\", \n     ylab=\"Frequency\")\nWe can perform the same task using ggplot2:\nlibrary(ggplot2)\nlibrary(dplyr)\n\n# Step 1: Simulate data\nset.seed(123)  # Set seed for reproducibility\nnum_clonotypes &lt;- 100  # Specify the number of different clonotypes\n\n# Create a data frame with random cell counts for each clonotype\ntcr_data &lt;- tibble(\n  clonotype = paste(\"TCR\", seq_len(num_clonotypes), sep=\"\"),\n  cell_count = sample(1:1000, num_clonotypes, replace=TRUE)  # Random cell counts between 1 and 1000\n)\n\n# Step 2: Create a histogram using ggplot2\nggplot(tcr_data, aes(x = cell_count)) + \n  geom_histogram(bins = 20, fill = \"skyblue\", color = \"black\") + \n  theme_minimal() + \n  labs(\n    title = \"Histogram of TCR Clone Sizes\",\n    x = \"Clone Size (Number of Cells)\",\n    y = \"Frequency\"\n  ) + \n  theme(\n    plot.title = element_text(hjust = 0.5)  # Center the plot title\n  )\n\n\n\nHistogram"
  },
  {
    "objectID": "course/2024/2_basicplotting.html#boxplot",
    "href": "course/2024/2_basicplotting.html#boxplot",
    "title": "Basic plotting and statistics",
    "section": "Boxplot",
    "text": "Boxplot\nLet’s simulate some gene expression data for key CD8 T cell genes.\n# Define genes and number of cells\ngenes &lt;- c(\"GZMB\", \"GZMA\", \"GNLY\", \"PRF1\", \"TOX\", \"ENTPD1\", \"LAG3\", \"HAVCR2\", \"TIGIT\", \"CXCL13\", \"IL7R\", \"SELL\", \"LEF1\", \"TCF7\")\nnum_cells &lt;- 20\n\n# Parameters for negative binomial\nsize &lt;- 2  # Dispersion parameter\nmu_pre &lt;- 20  # Mean for pre-treatment\nmu_post &lt;- 30  # Mean for post-treatment\n\n# Simulate gene expression data\nset.seed(42)\npre_treatment &lt;- sapply(rep(mu_pre, length(genes)), function(mu) rnbinom(num_cells, size, mu = mu))\npost_treatment &lt;- sapply(rep(mu_post, length(genes)), function(mu) rnbinom(num_cells, size, mu = mu))\n\n# Ensure data frames have proper column names\ncolnames(pre_treatment) &lt;- genes\ncolnames(post_treatment) &lt;- genes\n\n# Format as data frame\npre_data &lt;- as_tibble(pre_treatment) %&gt;% \n  mutate(Treatment = \"Pre\") %&gt;% \n  pivot_longer(cols = -Treatment, names_to = \"Gene\", values_to = \"Expression\")\n\npost_data &lt;- as_tibble(post_treatment) %&gt;% \n  mutate(Treatment = \"Post\") %&gt;% \n  pivot_longer(cols = -Treatment, names_to = \"Gene\", values_to = \"Expression\")\n\n# Combine the datasets\ncombined_data &lt;- bind_rows(pre_data, post_data)\n\n# Printing to see the combined data (optional)\nprint(combined_data)\nNow let’s use this data to build a boxplot of TOX expression pre and post treatment.\n# Filter data for the TOX gene\ntox_data &lt;- combined_data %&gt;% \n  filter(Gene == \"TOX\")\n\n# Plot\nggplot(tox_data, aes(x=Treatment, y=Expression, fill=Treatment)) +\n  geom_boxplot() +\n  labs(title=\"Expression of TOX pre and post treatment\", x=\"Treatment Condition\", y=\"Expression Level\") +\n  theme_minimal() +\n  scale_fill_brewer(palette=\"Pastel1\")  # Enhance aesthetics with color\n\n\n\nBoxplot of TOX Expression"
  },
  {
    "objectID": "course/2024/2_basicplotting.html#violin-plot",
    "href": "course/2024/2_basicplotting.html#violin-plot",
    "title": "Basic plotting and statistics",
    "section": "Violin plot",
    "text": "Violin plot\nSame thing a violin plot.\nlibrary(ggplot2)\n\n# Filter data for the TOX gene\ntox_data &lt;- combined_data %&gt;% \n  filter(Gene == \"TOX\")\n\n# Create the violin plot\nggplot(tox_data, aes(x=Treatment, y=Expression, fill=Treatment)) +\n  geom_violin(trim=FALSE) +  # Trim set to FALSE to show the full range of data\n  labs(title=\"Expression of TOX pre and post treatment\", x=\"Treatment Condition\", y=\"Expression Level\") +\n  theme_minimal() +\n  scale_fill_brewer(palette=\"Pastel1\") +\n  geom_boxplot(width=0.1, fill=\"white\")  # Overlay boxplot to show median and quartiles\n\n\n\nViolin of TOX Expression"
  },
  {
    "objectID": "course/2024/2_basicplotting.html#t-test",
    "href": "course/2024/2_basicplotting.html#t-test",
    "title": "Basic plotting and statistics",
    "section": "t-Test",
    "text": "t-Test\nA t-test could be used to compare the means of two groups, for example, the level of a specific immune marker in patients with and without a certain mutation.\n# Randomly generated sample data: Immune marker levels in two patient groups\ngroup1 &lt;- rnorm(30, mean = 5, sd = 1.5) # Patients with a mutation\ngroup2 &lt;- rnorm(30, mean = 4.5, sd = 1.2) # Patients without the mutation\n\n# Perform a t-test\ntest &lt;- t.test(group1, group2)\n\n# Print the result\nprint(test)\nOutput:\nWelch Two Sample t-test\n\ndata:  group1 and group2\nt = 0.83457, df = 49.381, p-value = 0.408\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -0.3593549  0.8699988\nsample estimates:\nmean of x mean of y \n 4.951171  4.695849"
  },
  {
    "objectID": "course/2024/2_basicplotting.html#fishers-exact-test",
    "href": "course/2024/2_basicplotting.html#fishers-exact-test",
    "title": "Basic plotting and statistics",
    "section": "Fisher’s Exact Test",
    "text": "Fisher’s Exact Test\nAssume you’ve identified a TCR clonotype and quantified the number of cells expressing this clonotype at two timepoints:\n\nTimepoint 1 (Pre-treatment): X number of cells\nTimepoint 2 (Post-treatment): Y number of cells\n\nYou also need the total number of cells sequenced at each timepoint to complete the contingency table for the Fisher’s Exact Test. Let’s say:\n\nTotal cells at Timepoint 1: N_pre\nTotal cells at Timepoint 2: N_post\n\nX &lt;- 20\nY &lt;- 300\n\nN_pre &lt;- 2450\nN_post &lt;- 3001\n\n# Number of cells with the specific clonotype at each timepoint\ncells_with_clone &lt;- c(X, Y)  \n\n# Number of cells without the clonotype (total cells minus cells with the clonotype)\ncells_without_clone &lt;- c(N_pre - X, N_post - Y)\n\n# Create the contingency table\ndata &lt;- matrix(c(cells_with_clone, cells_without_clone), ncol = 2,\n              dimnames = list(c(\"With Clone\", \"Without Clone\"),\n                              c(\"Pre-Treatment\", \"Post-Treatment\")))\n\n# Perform Fisher's Exact Test\ntest &lt;- fisher.test(data)\n\n# Print the result\nprint(test)\nOutput:\n    Fisher's Exact Test for Count Data\n\ndata:  data\np-value &lt; 2.2e-16\nalternative hypothesis: true odds ratio is not equal to 1\n95 percent confidence interval:\n 0.04445414 0.11701865\nsample estimates:\nodds ratio \n0.07410471 \n\nThe matrix data has two rows (“With Clone” and “Without Clone”) and two columns (“Pre-Treatment” and “Post-Treatment”). This matrix is filled with the counts of cells with and without the specific TCR clonotype at each timepoint.\nfisher.test(data) calculates whether the proportions of cells with the clonotype are significantly different between the two timepoints.\nThe output includes a p-value which indicates the probability that any observed difference in proportions occurred by chance."
  },
  {
    "objectID": "course/2024/test_newfile.html",
    "href": "course/2024/test_newfile.html",
    "title": "CRI Bioinformatics Workshop",
    "section": "",
    "text": "# Install the latest version of DEseq2\n# if (!requireNamespace(\"BiocManager\", quietly = TRUE))\n#   install.packages(\"BiocManager\")\n# BiocManager::install(\"DESeq2\")\n\n# define working dir paths\ndatadir = \"/cloud/project/data/bulk_rna\"\noutdir = \"/cloud/project/outdir\"\n\n# load R libraries we will use in this section\nlibrary(DESeq2)\nlibrary(data.table)\n\n# set working directory to data dir\nsetwd(datadir)\n\n# read in the RNAseq read counts for each gene (produced by htseq-count)\nhtseqCounts &lt;- fread(\"gene_read_counts_table_all_final.tsv\")\n# view class of the data\nclass(htseqCounts)\n\n# convert the data.table to matrix format\nhtseqCounts &lt;- as.matrix(htseqCounts)\nclass(htseqCounts)\n\n# set the gene ID values to be the row names for the matrix\nrownames(htseqCounts) &lt;- htseqCounts[,\"GeneID\"]\n\n# now that the gene IDs are the row names, remove the redundant column that contains them\nhtseqCounts &lt;- htseqCounts[, colnames(htseqCounts) != \"GeneID\"]\n\n# convert the actual count values from strings (with spaces) to integers, because originally the gene column contained characters, the entire matrix was set to character\nclass(htseqCounts) &lt;- \"integer\"\n\n# view the first few lines of the gene count matrix\nhead(htseqCounts)\n\n# it can also be useful to view interactively (if in Rstudio)\nView(htseqCounts)\n# run a filtering step\n# i.e. require that for every gene: at least 1 of 6 samples must have counts greater than 10\n# get index of rows that meet this criterion and use that to subset the matrix\n# note the dimensions of the matrix before and after filtering with dim\n\ndim(htseqCounts)\nhtseqCounts &lt;- htseqCounts[which(rowSums(htseqCounts &gt;= 10) &gt;=1),]\ndim(htseqCounts)\n\n# Hint! if you find the above command confusing, break it into pieces and observe the result\n# \n# what does \"rowSums(htseqCounts &gt;= 10)\" do?\n#\n# what does \"rowSums(htseqCounts &gt;= 10) &gt;=1\" do?\n# construct a mapping of the meta data for our experiment (comparing UHR cell lines to HBR brain tissues)\n# in simple terms this is defining the biological condition/label for each experimental replicate\n# create a simple one column dataframe to start\nmetaData &lt;- data.frame('Condition'=c('UHR', 'UHR', 'UHR', 'HBR', 'HBR', 'HBR'))\n\n# convert the \"Condition\" column to a factor data type, this will determine the direction of log2 fold-changes for the genes (i.e. up or down regulated)\nmetaData$Condition &lt;- factor(metaData$Condition, levels=c('HBR', 'UHR'))\n\n# set the row names of the metaData dataframe to be the names of our sample replicates from the read counts matrix\nrownames(metaData) &lt;- colnames(htseqCounts)\n\n# view the metadata dataframe\nhead(metaData)\n\n# check that names of htseq count columns match the names of the meta data rows\n# use the \"all\" function which tests whether an entire logical vector is TRUE\nall(rownames(metaData) == colnames(htseqCounts))\n# make deseq2 data sets\n# here we are setting up our experiment by supplying: (1) the gene counts matrix, (2) the sample/replicate for each column, and (3) the biological conditions we wish to compare.\n# this is a simple example that works for many experiments but these can also get more complex\n# for example, including designs with multiple variables such as \"~ group + condition\",\n# and designs with interactions such as \"~ genotype + treatment + genotype:treatment\".\n\ndds &lt;- DESeqDataSetFromMatrix(countData = htseqCounts, colData = metaData, design = ~Condition)\n# run the DESeq2 analysis on the \"dds\" object\ndds &lt;- DESeq(dds)\n\n# view the DE results\nres &lt;- results(dds)\nView(res)\n# shrink the log2 fold change estimates\n#   shrinkage of effect size (log fold change estimates) is useful for visualization and ranking of genes\n\n#   In simplistic terms, the goal of calculating \"dispersion estimates\" and \"shrinkage\" is also to account for the problem that\n#   genes with low mean counts across replicates tend of have higher variability than those with higher mean counts.\n#   Shrinkage attempts to correct for this. For a more detailed discussion of shrinkage refer to the DESeq2 vignette\n\n# first get the name of the coefficient (log fold change) to shrink\nresultsNames(dds)\n\n# now apply the shrinkage approach\nresLFC &lt;- lfcShrink(dds, coef=\"Condition_UHR_vs_HBR\", type=\"apeglm\")\n\n# make a copy of the shrinkage results to manipulate\ndeGeneResult &lt;- resLFC\n\n#contrast the values for a few genes before and after shrinkage\nhead(res)\nhead(deGeneResult)\n# read in gene ID to name mappings (using \"fread\" an alternative to \"read.table\")\nmapping &lt;- fread(\"ENSG_ID2Name.txt\", header=F)\n\n# add names to the columns in the \"mapping\" dataframe\nsetnames(mapping, c('ensemblID', 'Symbol'))\n\n# view the first few lines of the gene ID to name mappings\nhead(mapping)\n\n# merge on gene names\ndeGeneResult$ensemblID &lt;- rownames(deGeneResult)\ndeGeneResult &lt;- as.data.table(deGeneResult)\ndeGeneResult &lt;- merge(deGeneResult, mapping, by='ensemblID', all.x=T)\n\n# merge the original raw count values onto this final dataframe to aid interpretation\noriginal_counts = as.data.frame(htseqCounts)\noriginal_counts[,\"ensemblID\"] = rownames(htseqCounts)\ndeGeneResult = merge(deGeneResult, original_counts, by='ensemblID', all.x=T)\n\n# view the final merged dataframe\n# based on the raw counts and fold change values what does a negative fold change mean with respect to our two conditions HBR and UHR?\nhead(deGeneResult)\n# view the top genes according to adjusted p-value\ndeGeneResult[order(deGeneResult$padj),]\n\n# view the top genes according to fold change\ndeGeneResult[order(deGeneResult$log2FoldChange),]\n\n# determine the number of up/down significant genes at FDR &lt; 0.05 significance level\ndim(deGeneResult)[1] # number of genes tested\ndim(deGeneResult[deGeneResult$padj &lt; 0.05])[1] #number of significant genes\n\n# order the DE results by adjusted p-value\ndeGeneResultSorted = deGeneResult[order(deGeneResult$padj),]\n\n# create a filtered data frame that limits to only the significant DE genes (adjusted p.value &lt; 0.05)\ndeGeneResultSignificant = deGeneResultSorted[deGeneResultSorted$padj &lt; 0.05]\n# set the working directory to the output dir where we will store any results files\nsetwd(outdir)\n\n# save the final DE result (all genes)  to an output file\nfwrite(deGeneResultSorted, file='DE_all_genes_DESeq2.tsv', sep=\"\\t\")\n\n# save the final DE result (significant genes only)  to an output file\nfwrite(deGeneResultSignificant, file='DE_sig_genes_DESeq2.tsv', sep=\"\\t\")\n\n# save the DESeq2 objects for the data visualization section\nsaveRDS(dds, 'dds.rds')\nsaveRDS(res, 'res.rds')\nsaveRDS(resLFC, 'resLFC.rds')\n\n# to exit R type the following\n#quit(save=\"no\")"
  },
  {
    "objectID": "course/2_basicplotting.html",
    "href": "course/2_basicplotting.html",
    "title": "Basic plotting and statistics",
    "section": "",
    "text": "Long and wide data formats are two common ways of structuring data, each with its own advantages and use cases.\n\nIn the long format, also known as the “tidy” format, each observation is represented by a single row in the dataset. This format is characterized by having:\n\nMultiple rows, each corresponding to a single observation or measurement.\nOne column for the variable being measured.\nAdditional columns to store metadata or grouping variables.\n\nAdvantages:\n\nFacilitates easy analysis and manipulation, especially when using tools like Tidyverse packages in R.\nSuitable for data that follow the “one observation per row” principle, such as time series or longitudinal data.\n\nIn the wide format, each observation is represented by a single row, but with multiple columns corresponding to different variables. This format is characterized by:\n\nOne row per observation.\nEach variable is represented by a separate column.\n\nAdvantages:\n\nCan be easier to understand for simple datasets with few variables.\nMay be more convenient for certain types of analysis or visualization.\n\nThe choice between long and wide formats depends on factors such as the nature of the data, the analysis tasks, and personal preference. Long format is often preferred for its flexibility and compatibility with modern data analysis tools, while wide format may be suitable for simpler datasets or specific analysis requirements.",
    "crumbs": [
      "Intro to R",
      "Basic plotting and statistics"
    ]
  },
  {
    "objectID": "course/2_basicplotting.html#long-and-wide-data-formats",
    "href": "course/2_basicplotting.html#long-and-wide-data-formats",
    "title": "Basic plotting and statistics",
    "section": "",
    "text": "Long and wide data formats are two common ways of structuring data, each with its own advantages and use cases.\n\nIn the long format, also known as the “tidy” format, each observation is represented by a single row in the dataset. This format is characterized by having:\n\nMultiple rows, each corresponding to a single observation or measurement.\nOne column for the variable being measured.\nAdditional columns to store metadata or grouping variables.\n\nAdvantages:\n\nFacilitates easy analysis and manipulation, especially when using tools like Tidyverse packages in R.\nSuitable for data that follow the “one observation per row” principle, such as time series or longitudinal data.\n\nIn the wide format, each observation is represented by a single row, but with multiple columns corresponding to different variables. This format is characterized by:\n\nOne row per observation.\nEach variable is represented by a separate column.\n\nAdvantages:\n\nCan be easier to understand for simple datasets with few variables.\nMay be more convenient for certain types of analysis or visualization.\n\nThe choice between long and wide formats depends on factors such as the nature of the data, the analysis tasks, and personal preference. Long format is often preferred for its flexibility and compatibility with modern data analysis tools, while wide format may be suitable for simpler datasets or specific analysis requirements.",
    "crumbs": [
      "Intro to R",
      "Basic plotting and statistics"
    ]
  },
  {
    "objectID": "course/2_basicplotting.html#long-to-wide",
    "href": "course/2_basicplotting.html#long-to-wide",
    "title": "Basic plotting and statistics",
    "section": "Long to Wide",
    "text": "Long to Wide\nlibrary(tidyr)\n\n# Example long format data\nlong_data &lt;- data.frame(\n  Subject = c(\"A\", \"A\", \"B\", \"B\"),\n  Time = c(1, 2, 1, 2),\n  Measurement = c(10, 15, 12, 18)\n)\n\n# Convert long format data to wide format\nwide_data &lt;- spread(long_data, key = Time, value = Measurement)\n\n# View the wide format data\nprint(wide_data)",
    "crumbs": [
      "Intro to R",
      "Basic plotting and statistics"
    ]
  },
  {
    "objectID": "course/2_basicplotting.html#wide-to-long",
    "href": "course/2_basicplotting.html#wide-to-long",
    "title": "Basic plotting and statistics",
    "section": "Wide to Long",
    "text": "Wide to Long\nlibrary(tidyr)\n\n# Example wide format data\nwide_data &lt;- data.frame(\n  Subject = c(\"A\", \"B\"),\n  Time1 = c(10, 12),\n  Time2 = c(15, 18)\n)\n\n# Convert wide format data to long format\nlong_data &lt;- gather(wide_data, key = Time, value = Measurement, -Subject)\n\n# View the long format data\nprint(long_data)",
    "crumbs": [
      "Intro to R",
      "Basic plotting and statistics"
    ]
  },
  {
    "objectID": "course/2_basicplotting.html#joins-and-merging-of-data-in-tidyverse",
    "href": "course/2_basicplotting.html#joins-and-merging-of-data-in-tidyverse",
    "title": "Basic plotting and statistics",
    "section": "Joins and Merging of Data in Tidyverse",
    "text": "Joins and Merging of Data in Tidyverse\nJoins and merging are common operations used to combine multiple datasets based on common variables or keys. In Tidyverse, these operations are typically performed using functions from the dplyr package.\nTypes of Joins:\n\n\nTypes of Joins\n\nInner Join (inner_join()):\nAn inner join combines rows from two datasets where there is a match based on a common key, retaining only the rows with matching keys from both datasets.\nLeft Join (left_join()):\nA left join combines all rows from the first (left) dataset with matching rows from the second (right) dataset based on a common key. If there is no match in the second dataset, missing values are filled in.\nRight Join (right_join()):\nSimilar to a left join, but it retains all rows from the second (right) dataset and fills in missing values for non-matching rows from the first (left) dataset.\nFull Join (full_join()):\nA full join combines all rows from both datasets, filling in missing values where there are no matches.\nSemi-Join (semi_join()):\nA semi-join returns only rows from the first dataset where there are matching rows in the second dataset, based on a common key.\nAnti-Join (anti_join()):\nAn anti-join returns only rows from the first dataset that do not have matching rows in the second dataset, based on a common key.\nMerging Data:\nMerge (merge()):\nThe merge() function is a base R function used to merge datasets based on common columns or keys. It performs similar operations to joins in dplyr, but with slightly different syntax and behavior.\nExample:\nlibrary(dplyr)\n\n# Example datasets\ndf1 &lt;- data.frame(ID = c(1, 2, 3), Name = c(\"Alice\", \"Bob\", \"Charlie\"))\ndf2 &lt;- data.frame(ID = c(2, 3, 4), Score = c(85, 90, 95))\n\n# Inner join\ninner_merged &lt;- inner_join(df1, df2, by = \"ID\")\n\n# Left join\nleft_merged &lt;- left_join(df1, df2, by = \"ID\")\n\n# Right join\nright_merged &lt;- right_join(df1, df2, by = \"ID\")\n\n# Full join\nfull_merged &lt;- full_join(df1, df2, by = \"ID\")\n\n# Semi-join\nsemi_merged &lt;- semi_join(df1, df2, by = \"ID\")\n\n# Anti-join\nanti_merged &lt;- anti_join(df1, df2, by = \"ID\")",
    "crumbs": [
      "Intro to R",
      "Basic plotting and statistics"
    ]
  },
  {
    "objectID": "course/2_basicplotting.html#ggplot2",
    "href": "course/2_basicplotting.html#ggplot2",
    "title": "Basic plotting and statistics",
    "section": "ggplot2",
    "text": "ggplot2\nThe core idea behind ggplot2 is the concept of a “grammar of graphics”. This concept provides a systematic way to describe and build graphical presentations such as charts and plots. The grammar itself is a set of independent components that can be composed in many different ways. This grammar includes elements like:\n\nData: The raw data that you want to visualize.\nAesthetics (aes): Defines how data are mapped to color, size, shape, and other visual properties.\nGeometries (geom): The geometric objects in a plot—lines, points, bars, etc.\nScales: Transformations applied to data before it is visualized, including scales for colors, sizes, and shapes.\nCoordinate systems: The space in which data is plotted.\nFacets: Used for creating plots with multiple panels (small multiple plots).\nStatistical transformations (stat): Summary statistics that can be applied to data before it is visualized, such as counting or averaging.\nThemes: Visual styles and layout configurations for the plot.\n\nHere’s how you generally use ggplot2 to create a plot:\n\nStart with ggplot(): Set up the data and, optionally, define default mappings between variables and their aesthetics.\nAdd layers: Add layers to the plot using geom_ functions, such as geom_point() for scatter plots, geom_line() for line graphs, and so on.\n\nAdjust the scales: Customize the scales used for aesthetics such as color, size, and x-y coordinates.\nModify the coordinate system: Choose a coordinate system.\nAdd facets: If necessary, add facets to create a multi-panel plot.\nApply a theme: Customize the appearance of the plot using themes.\n\nlibrary(ggplot2)\n\n# Sample data\ndf &lt;- data.frame(\n  x = rnorm(100),\n  y = rnorm(100),\n  group = factor(rep(1:2, each = 50))\n)\n\n# Creating a scatter plot\nggplot(df, aes(x = x, y = y, color = group)) + \n  geom_point() +\n  theme_minimal() +\n  labs(title = \"Scatter Plot Example\", x = \"X Axis\", y = \"Y Axis\")\n\n\nScatter",
    "crumbs": [
      "Intro to R",
      "Basic plotting and statistics"
    ]
  },
  {
    "objectID": "course/2_basicplotting.html#histogram",
    "href": "course/2_basicplotting.html#histogram",
    "title": "Basic plotting and statistics",
    "section": "Histogram",
    "text": "Histogram\nLet’s simulate some TCR clonotype data. We will create a dataset where each TCR has a randomly generated number of cells associated with it, representing the clone size. After generating the data, we’ll use the hist() function from base R to plot a histogram of the clone sizes.\nlibrary(dplyr)\n\n# Step 1: Simulate data\nset.seed(123)  # Set seed for reproducibility\nnum_clonotypes &lt;- 100  # Specify the number of different clonotypes\n\n# Create a data frame with random cell counts for each clonotype\ntcr_data &lt;- tibble(\n  clonotype = paste(\"TCR\", seq_len(num_clonotypes), sep=\"\"),\n  cell_count = sample(1:1000, num_clonotypes, replace=TRUE)  # Random cell counts between 1 and 1000\n)\n\n# Step 2: Create a histogram of clone sizes\nhist(tcr_data$cell_count, \n     breaks=20,  # You can adjust the number of breaks to change bin sizes\n     col=\"skyblue\", \n     main=\"Histogram of TCR Clone Sizes\", \n     xlab=\"Clone Size (Number of Cells)\", \n     ylab=\"Frequency\")\nWe can perform the same task using ggplot2:\nlibrary(ggplot2)\nlibrary(dplyr)\n\n# Step 1: Simulate data\nset.seed(123)  # Set seed for reproducibility\nnum_clonotypes &lt;- 100  # Specify the number of different clonotypes\n\n# Create a data frame with random cell counts for each clonotype\ntcr_data &lt;- tibble(\n  clonotype = paste(\"TCR\", seq_len(num_clonotypes), sep=\"\"),\n  cell_count = sample(1:1000, num_clonotypes, replace=TRUE)  # Random cell counts between 1 and 1000\n)\n\n# Step 2: Create a histogram using ggplot2\nggplot(tcr_data, aes(x = cell_count)) + \n  geom_histogram(bins = 20, fill = \"skyblue\", color = \"black\") + \n  theme_minimal() + \n  labs(\n    title = \"Histogram of TCR Clone Sizes\",\n    x = \"Clone Size (Number of Cells)\",\n    y = \"Frequency\"\n  ) + \n  theme(\n    plot.title = element_text(hjust = 0.5)  # Center the plot title\n  )\n\n\nHistogram",
    "crumbs": [
      "Intro to R",
      "Basic plotting and statistics"
    ]
  },
  {
    "objectID": "course/2_basicplotting.html#boxplot",
    "href": "course/2_basicplotting.html#boxplot",
    "title": "Basic plotting and statistics",
    "section": "Boxplot",
    "text": "Boxplot\nLet’s simulate some gene expression data for key CD8 T cell genes.\n# Define genes and number of cells\ngenes &lt;- c(\"GZMB\", \"GZMA\", \"GNLY\", \"PRF1\", \"TOX\", \"ENTPD1\", \"LAG3\", \"HAVCR2\", \"TIGIT\", \"CXCL13\", \"IL7R\", \"SELL\", \"LEF1\", \"TCF7\")\nnum_cells &lt;- 20\n\n# Parameters for negative binomial\nsize &lt;- 2  # Dispersion parameter\nmu_pre &lt;- 20  # Mean for pre-treatment\nmu_post &lt;- 30  # Mean for post-treatment\n\n# Simulate gene expression data\nset.seed(42)\npre_treatment &lt;- sapply(rep(mu_pre, length(genes)), function(mu) rnbinom(num_cells, size, mu = mu))\npost_treatment &lt;- sapply(rep(mu_post, length(genes)), function(mu) rnbinom(num_cells, size, mu = mu))\n\n# Ensure data frames have proper column names\ncolnames(pre_treatment) &lt;- genes\ncolnames(post_treatment) &lt;- genes\n\n# Format as data frame\npre_data &lt;- as_tibble(pre_treatment) %&gt;% \n  mutate(Treatment = \"Pre\") %&gt;% \n  pivot_longer(cols = -Treatment, names_to = \"Gene\", values_to = \"Expression\")\n\npost_data &lt;- as_tibble(post_treatment) %&gt;% \n  mutate(Treatment = \"Post\") %&gt;% \n  pivot_longer(cols = -Treatment, names_to = \"Gene\", values_to = \"Expression\")\n\n# Combine the datasets\ncombined_data &lt;- bind_rows(pre_data, post_data)\n\n# Printing to see the combined data (optional)\nprint(combined_data)\nNow let’s use this data to build a boxplot of TOX expression pre and post treatment.\n# Filter data for the TOX gene\ntox_data &lt;- combined_data %&gt;% \n  filter(Gene == \"TOX\")\n\n# Plot\nggplot(tox_data, aes(x=Treatment, y=Expression, fill=Treatment)) +\n  geom_boxplot() +\n  labs(title=\"Expression of TOX pre and post treatment\", x=\"Treatment Condition\", y=\"Expression Level\") +\n  theme_minimal() +\n  scale_fill_brewer(palette=\"Pastel1\")  # Enhance aesthetics with color\n\n\nBoxplot of TOX Expression",
    "crumbs": [
      "Intro to R",
      "Basic plotting and statistics"
    ]
  },
  {
    "objectID": "course/2_basicplotting.html#violin-plot",
    "href": "course/2_basicplotting.html#violin-plot",
    "title": "Basic plotting and statistics",
    "section": "Violin plot",
    "text": "Violin plot\nSame thing a violin plot.\nlibrary(ggplot2)\n\n# Filter data for the TOX gene\ntox_data &lt;- combined_data %&gt;% \n  filter(Gene == \"TOX\")\n\n# Create the violin plot\nggplot(tox_data, aes(x=Treatment, y=Expression, fill=Treatment)) +\n  geom_violin(trim=FALSE) +  # Trim set to FALSE to show the full range of data\n  labs(title=\"Expression of TOX pre and post treatment\", x=\"Treatment Condition\", y=\"Expression Level\") +\n  theme_minimal() +\n  scale_fill_brewer(palette=\"Pastel1\") +\n  geom_boxplot(width=0.1, fill=\"white\")  # Overlay boxplot to show median and quartiles\n\n\nViolin of TOX Expression",
    "crumbs": [
      "Intro to R",
      "Basic plotting and statistics"
    ]
  },
  {
    "objectID": "course/2_basicplotting.html#t-test",
    "href": "course/2_basicplotting.html#t-test",
    "title": "Basic plotting and statistics",
    "section": "t-Test",
    "text": "t-Test\nA t-test could be used to compare the means of two groups, for example, the level of a specific immune marker in patients with and without a certain mutation.\n# Randomly generated sample data: Immune marker levels in two patient groups\ngroup1 &lt;- rnorm(30, mean = 5, sd = 1.5) # Patients with a mutation\ngroup2 &lt;- rnorm(30, mean = 4.5, sd = 1.2) # Patients without the mutation\n\n# Perform a t-test\ntest &lt;- t.test(group1, group2)\n\n# Print the result\nprint(test)\nOutput:\nWelch Two Sample t-test\n\ndata:  group1 and group2\nt = 0.83457, df = 49.381, p-value = 0.408\nalternative hypothesis: true difference in means is not equal to 0\n95 percent confidence interval:\n -0.3593549  0.8699988\nsample estimates:\nmean of x mean of y \n 4.951171  4.695849",
    "crumbs": [
      "Intro to R",
      "Basic plotting and statistics"
    ]
  },
  {
    "objectID": "course/2_basicplotting.html#fishers-exact-test",
    "href": "course/2_basicplotting.html#fishers-exact-test",
    "title": "Basic plotting and statistics",
    "section": "Fisher’s Exact Test",
    "text": "Fisher’s Exact Test\nAssume you’ve identified a TCR clonotype and quantified the number of cells expressing this clonotype at two timepoints:\n\nTimepoint 1 (Pre-treatment): X number of cells\nTimepoint 2 (Post-treatment): Y number of cells\n\nYou also need the total number of cells sequenced at each timepoint to complete the contingency table for the Fisher’s Exact Test. Let’s say:\n\nTotal cells at Timepoint 1: N_pre\n\nTotal cells at Timepoint 2: N_post\n\n\nX &lt;- 20\nY &lt;- 300\n\nN_pre &lt;- 2450\nN_post &lt;- 3001\n\n# Number of cells with the specific clonotype at each timepoint\ncells_with_clone &lt;- c(X, Y)  \n\n# Number of cells without the clonotype (total cells minus cells with the clonotype)\ncells_without_clone &lt;- c(N_pre - X, N_post - Y)\n\n# Create the contingency table\ndata &lt;- matrix(c(cells_with_clone, cells_without_clone), ncol = 2,\n              dimnames = list(c(\"With Clone\", \"Without Clone\"),\n                              c(\"Pre-Treatment\", \"Post-Treatment\")))\n\n# Perform Fisher's Exact Test\ntest &lt;- fisher.test(data)\n\n# Print the result\nprint(test)\nOutput:\n    Fisher's Exact Test for Count Data\n\ndata:  data\np-value &lt; 2.2e-16\nalternative hypothesis: true odds ratio is not equal to 1\n95 percent confidence interval:\n 0.04445414 0.11701865\nsample estimates:\nodds ratio \n0.07410471 \n\nThe matrix data has two rows (“With Clone” and “Without Clone”) and two columns (“Pre-Treatment” and “Post-Treatment”). This matrix is filled with the counts of cells with and without the specific TCR clonotype at each timepoint.\n\nfisher.test(data) calculates whether the proportions of cells with the clonotype are significantly different between the two timepoints.\nThe output includes a p-value which indicates the probability that any observed difference in proportions occurred by chance.",
    "crumbs": [
      "Intro to R",
      "Basic plotting and statistics"
    ]
  },
  {
    "objectID": "course/test_newfile.html",
    "href": "course/test_newfile.html",
    "title": "CRI Bioinformatics Workshop",
    "section": "",
    "text": "# Install the latest version of DEseq2\n# if (!requireNamespace(\"BiocManager\", quietly = TRUE))\n#   install.packages(\"BiocManager\")\n# BiocManager::install(\"DESeq2\")\n\n# define working dir paths\ndatadir = \"/cloud/project/data/bulk_rna\"\noutdir = \"/cloud/project/outdir\"\n\n# load R libraries we will use in this section\nlibrary(DESeq2)\nlibrary(data.table)\n\n# set working directory to data dir\nsetwd(datadir)\n\n# read in the RNAseq read counts for each gene (produced by htseq-count)\nhtseqCounts &lt;- fread(\"gene_read_counts_table_all_final.tsv\")\n# view class of the data\nclass(htseqCounts)\n\n# convert the data.table to matrix format\nhtseqCounts &lt;- as.matrix(htseqCounts)\nclass(htseqCounts)\n\n# set the gene ID values to be the row names for the matrix\nrownames(htseqCounts) &lt;- htseqCounts[,\"GeneID\"]\n\n# now that the gene IDs are the row names, remove the redundant column that contains them\nhtseqCounts &lt;- htseqCounts[, colnames(htseqCounts) != \"GeneID\"]\n\n# convert the actual count values from strings (with spaces) to integers, because originally the gene column contained characters, the entire matrix was set to character\nclass(htseqCounts) &lt;- \"integer\"\n\n# view the first few lines of the gene count matrix\nhead(htseqCounts)\n\n# it can also be useful to view interactively (if in Rstudio)\nView(htseqCounts)\n# run a filtering step\n# i.e. require that for every gene: at least 1 of 6 samples must have counts greater than 10\n# get index of rows that meet this criterion and use that to subset the matrix\n# note the dimensions of the matrix before and after filtering with dim\n\ndim(htseqCounts)\nhtseqCounts &lt;- htseqCounts[which(rowSums(htseqCounts &gt;= 10) &gt;=1),]\ndim(htseqCounts)\n\n# Hint! if you find the above command confusing, break it into pieces and observe the result\n# \n# what does \"rowSums(htseqCounts &gt;= 10)\" do?\n#\n# what does \"rowSums(htseqCounts &gt;= 10) &gt;=1\" do?\n# construct a mapping of the meta data for our experiment (comparing UHR cell lines to HBR brain tissues)\n# in simple terms this is defining the biological condition/label for each experimental replicate\n# create a simple one column dataframe to start\nmetaData &lt;- data.frame('Condition'=c('UHR', 'UHR', 'UHR', 'HBR', 'HBR', 'HBR'))\n\n# convert the \"Condition\" column to a factor data type, this will determine the direction of log2 fold-changes for the genes (i.e. up or down regulated)\nmetaData$Condition &lt;- factor(metaData$Condition, levels=c('HBR', 'UHR'))\n\n# set the row names of the metaData dataframe to be the names of our sample replicates from the read counts matrix\nrownames(metaData) &lt;- colnames(htseqCounts)\n\n# view the metadata dataframe\nhead(metaData)\n\n# check that names of htseq count columns match the names of the meta data rows\n# use the \"all\" function which tests whether an entire logical vector is TRUE\nall(rownames(metaData) == colnames(htseqCounts))\n# make deseq2 data sets\n# here we are setting up our experiment by supplying: (1) the gene counts matrix, (2) the sample/replicate for each column, and (3) the biological conditions we wish to compare.\n# this is a simple example that works for many experiments but these can also get more complex\n# for example, including designs with multiple variables such as \"~ group + condition\",\n# and designs with interactions such as \"~ genotype + treatment + genotype:treatment\".\n\ndds &lt;- DESeqDataSetFromMatrix(countData = htseqCounts, colData = metaData, design = ~Condition)\n# run the DESeq2 analysis on the \"dds\" object\ndds &lt;- DESeq(dds)\n\n# view the DE results\nres &lt;- results(dds)\nView(res)\n# shrink the log2 fold change estimates\n#   shrinkage of effect size (log fold change estimates) is useful for visualization and ranking of genes\n\n#   In simplistic terms, the goal of calculating \"dispersion estimates\" and \"shrinkage\" is also to account for the problem that\n#   genes with low mean counts across replicates tend of have higher variability than those with higher mean counts.\n#   Shrinkage attempts to correct for this. For a more detailed discussion of shrinkage refer to the DESeq2 vignette\n\n# first get the name of the coefficient (log fold change) to shrink\nresultsNames(dds)\n\n# now apply the shrinkage approach\nresLFC &lt;- lfcShrink(dds, coef=\"Condition_UHR_vs_HBR\", type=\"apeglm\")\n\n# make a copy of the shrinkage results to manipulate\ndeGeneResult &lt;- resLFC\n\n#contrast the values for a few genes before and after shrinkage\nhead(res)\nhead(deGeneResult)\n# read in gene ID to name mappings (using \"fread\" an alternative to \"read.table\")\nmapping &lt;- fread(\"ENSG_ID2Name.txt\", header=F)\n\n# add names to the columns in the \"mapping\" dataframe\nsetnames(mapping, c('ensemblID', 'Symbol'))\n\n# view the first few lines of the gene ID to name mappings\nhead(mapping)\n\n# merge on gene names\ndeGeneResult$ensemblID &lt;- rownames(deGeneResult)\ndeGeneResult &lt;- as.data.table(deGeneResult)\ndeGeneResult &lt;- merge(deGeneResult, mapping, by='ensemblID', all.x=T)\n\n# merge the original raw count values onto this final dataframe to aid interpretation\noriginal_counts = as.data.frame(htseqCounts)\noriginal_counts[,\"ensemblID\"] = rownames(htseqCounts)\ndeGeneResult = merge(deGeneResult, original_counts, by='ensemblID', all.x=T)\n\n# view the final merged dataframe\n# based on the raw counts and fold change values what does a negative fold change mean with respect to our two conditions HBR and UHR?\nhead(deGeneResult)\n# view the top genes according to adjusted p-value\ndeGeneResult[order(deGeneResult$padj),]\n\n# view the top genes according to fold change\ndeGeneResult[order(deGeneResult$log2FoldChange),]\n\n# determine the number of up/down significant genes at FDR &lt; 0.05 significance level\ndim(deGeneResult)[1] # number of genes tested\ndim(deGeneResult[deGeneResult$padj &lt; 0.05])[1] #number of significant genes\n\n# order the DE results by adjusted p-value\ndeGeneResultSorted = deGeneResult[order(deGeneResult$padj),]\n\n# create a filtered data frame that limits to only the significant DE genes (adjusted p.value &lt; 0.05)\ndeGeneResultSignificant = deGeneResultSorted[deGeneResultSorted$padj &lt; 0.05]\n# set the working directory to the output dir where we will store any results files\nsetwd(outdir)\n\n# save the final DE result (all genes)  to an output file\nfwrite(deGeneResultSorted, file='DE_all_genes_DESeq2.tsv', sep=\"\\t\")\n\n# save the final DE result (significant genes only)  to an output file\nfwrite(deGeneResultSignificant, file='DE_sig_genes_DESeq2.tsv', sep=\"\\t\")\n\n# save the DESeq2 objects for the data visualization section\nsaveRDS(dds, 'dds.rds')\nsaveRDS(res, 'res.rds')\nsaveRDS(resLFC, 'resLFC.rds')\n\n# to exit R type the following\n#quit(save=\"no\")",
    "crumbs": [
      "Bulk RNA Analysis",
      "Code blocks for DE"
    ]
  },
  {
    "objectID": "schedule.html",
    "href": "schedule.html",
    "title": "Schedule",
    "section": "",
    "text": "The CRI Bioinformatics Bootcamp will take place Saturday, May 17, 2025 through Thursday, May 22, 2025 at Wyndham Grand Bonnet Creek in Orlando, Florida.\n\n\n\n\n\n\n\n\n\n\n\nDay\nTime\nDuration\nModule\nTopic\n\n\n\n\n1 (Sat)\n9:00AM-9:30AM\n0:30\nR Workshop\nIntroduction to R and RStudio\n\n\n\n9:30AM-12:00PM\n2:30\n\nHands-on: Reading, writing, and interpreting data structures\n\n\n\n12:00PM-1:30PM\n1:30\nOffice Hours\nLunch\n\n\n\n1:30PM-2:15PM\n0:45\n\nIntroduction to plotting and statistics\n\n\n\n2:15PM-3:15PM\n1:00\n\nHands-on: Plotting information from data structures for real-time analysis\n\n\n\n3:15PM-3:30PM\n0:15\n\nBreak\n\n\n\n3:30PM-5:00PM\n1:30\n\nHands-on (cont’d): Plotting\n\n\n2 (Sun)\n9:00AM-9:30AM\n0:30\nR Workshop\nTypes of analysis: Clustering concepts and correlation\n\n\n\n9:45AM-12:00PM\n2:30\n\nHands-on: Dimension reduction methods, computing correlation\n\n\n\n12:00PM-1:30PM\n1:30\nOffice Hours\nLunch\n\n\n\n1:30PM-2:15PM\n0:45\n\nCommon challenges and resources\n\n\n\n2:15PM-3:15PM\n1:00\n\nHands-on: Creating different types of plots, continued practice\n\n\n\n3:15PM-3:30PM\n0:15\n\nBreak\n\n\n\n3:30PM-5:00PM\n1:30\n\nHands-on (cont’d): Continued practice\n\n\n3 (Mon)\n9:00AM-9:45AM\n0:45\nBulk RNA sequencing\nIntroduction to sequencing data processing and analysis\n\n\n\n9:45AM-10:15AM\n0:30\n\nIntroduction to rnabio.org\n\n\n\n10:15AM-10:30AM\n0:15\n\nIntroduction to bulk RNAseq dataset\n\n\n\n10:30AM-11:30AM\n1:00\n\nHands-on: Exploring RNAseq data in IGV\n\n\n\n11:30AM-12:00PM\n0:30\n\nHands-on: DE analysis with DESeq2\n\n\n\n12:00PM-1:30PM\n1:30\nOffice Hours\nLunch\n\n\n\n1:30PM-2:15PM\n0:45\n\nHands-on: DE visualization with DESeq2\n\n\n\n2:15PM-3:15PM\n1:00\n\nHands-on: DE visualization advanced\n\n\n\n3:15PM-4:45PM\n1:30\n\nHands-on: Pathway analysis\n\n\n\n4:45PM-5:00PM\n0:15\n\nDay 3 Summary/Questions\n\n\n\n5:30PM-7:00PM\n1:30\nOffice Hours\n\n\n\n4 (Tue)\n9:00AM-9:45AM\n0:45\nSingle cell RNA sequencing\nIntroduction to scRNAseq methods and analysis\n\n\n\n9:45AM-10:15AM\n0:30\n\nIntroduction to scRNAseq dataset and loupe demonstration\n\n\n\n10:15AM-12:00PM\n1:45\n\nHands-on: Quality assesment and clustering\n\n\n\n12:00PM-1:30PM\n1:30\nOffice Hours\nLunch\n\n\n\n1:30PM-2:15PM\n0:45\n\nHands-on: Cell typing\n\n\n\n2:15PM-3:30PM\n1:15\n\nHands-on: DE analysis\n\n\n\n3:30PM-4:45PM\n1:15\n\nHands-on: Gene set and regulatory pathway\n\n\n\n4:45PM-5:00PM\n0:15\n\nDay 4 Summary/Questions\n\n\n\n5:30PM-7:00PM\n1:30\nOffice hours\n\n\n\n5 (Wed)\n9:00AM-9:45AM\n0:45\nSingle cell RNA sequencing\nAdvanced analyses in scRNAseq and multi-omic integration\n\n\n\n9:45AM-12:00PM\n2:00\n\nHands-on: Cancer cell identification\n\n\n\n12:00PM-1:30PM\n1:30\nOffice Hours\nLunch\n\n\n\n1:30PM-3:00PM\n1:30\n\nHands-on: Trajectory analysis\n\n\n\n3:00PM-4:15PM\n1:15\n\nHands-on: TCR/BCR analysis\n\n\n\n4:15PM-4:45PM\n0:30\n\nvLoupe Browser demonstration\n\n\n\n4:45PM-5:00PM\n0:15\n\nDay 5 Summary/Questions\n\n\n6 (Thu)\n9:00AM-10:00AM\n1:00\nSpatial Transcriptomics\nIntroduction to spatial transcriptomics: Visium, Xenium, CosMx\n\n\n\n10:00AM-10:30AM\n0:30\n\nSample pre-processing\n\n\n\n10:30AM-11:30PM\n1:00\n\nSeurat objects, motifs and neighborhood clustering (Banksy, Grafiti)\n\n\n\n11:30AM-12:00PM\n0:30\n\nCustom binning\n\n\n\n12:00PM-1:30PM\n1:30\nOffice Hours\nLunch\n\n\n\n1:30PM-2:30PM\n1:00\n\nCell typing, CNV, trajectory analysis\n\n\n\n2:30PM-4:30PM\n2:00\n\nHands-on: Unsupervised clustering, cell typing, loupe browser\n\n\n\n4:30PM-4:45PM\n0:15\n\nDay 6 Summary/Questions\n\n\n\n3:45PM-4:00PM\n0:15\nConclusion\nClosing and feedback"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "course.html",
    "href": "course.html",
    "title": "Course overview",
    "section": "",
    "text": "R Workshop (Day 1, Sat. - Day 2, Sun.)\n\nIntroduction to R\nBasic plotting and statistics\nClustering concepts and correlation\nReading, navigating, and plotting data\nLecture Slides Available Here\n\n\n\nBulk RNA sequence analysis (Day 3, Mon.)\n\nIntroduction to DNA and RNA sequencing lecture slides (Obi Griffith and Malachi Griffith)\n\n\nIntroduction to rnabio.org and bulk RNAseq dataset (Lead: Obi/Malachi)\nIntroduction to IGV (Lead: Malachi)\nDifferential Expression Analysis (DESeq2) (Lead: Zach)\nDifferential Expression Visualization (DESeq2) (Lead: Zach)\nDifferential Expression Visualization (Advanced R) (Lead: Obi/Charles)\nPathway Analysis (Lead: Obi)\nBatch Correction - brief introduction (Lead: Malachi)\n\n\n\nSingle cell RNA sequencing (Day 4, Tue. - Day 5, Wed.)\n\nIntroduction to scRNA sequencing technology lecture slides (Obi Griffith)\nIntroduction to scRNA sequencing analysis lecture slides (Malachi Griffith)\n\n\nIntroduction to scRNAseq dataset (Lead: Malachi)\nQA/QC and Clustering (Lead: Evelyn)\nCell Type Annotation (Lead: Kelsy)\nDifferential Expression (Lead: Kartik)\nGene Set Enrichment (Lead: Kartik)\nCancer Cell Identification (Lead: Kartik)\nTrajectory Analysis (Lead: Evelyn/Malachi)\nTCR/BCR Repertoire Analysis (Lead: Obi)\n\n\n\nSpatial Transcriptomics\n\nIntroduction to spatial transcriptomics: Visium, Xenium, CosMx\nSample pre-processing\nSeurat objects, motifs and neighborhood clustering (Banksy, Grafiti)\nCustom binning\nCell typing, CNV, trajectory analysis\nUnsupervise clustering, cell tying, loupe browser\n\n\n\nPrior versions of the CRI Bioinformatics Bootcamp\n\n2024"
  },
  {
    "objectID": "course/spatial_demo.html",
    "href": "course/spatial_demo.html",
    "title": "Intro to Spatial Analysis",
    "section": "",
    "text": "In this notebook, we will be going through some basic spatial analyses. The goal is to provide you with an intuition of the logic behind each of these functions. Try not to get bogged down in understanding every single line of the code, but focus more on the overall reasoning behind what is being done. The code in this notebook is not optimized for large datasets, but simplified versions of more complex functions, to make it more clear what is actually being done.\nIf you would like to run similar analyses on your own multiplexed imaging datasets, please see our lab’s poipeline here: https://github.com/angelolab/ark-analysis. There are also other toolkits for spatial analysis, including Squidpy and MCMICRO.\nimport os\nimport numpy as np\nimport pandas as pd\nimport skimage.io as io\nfrom skimage.segmentation import find_boundaries\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\nfrom matplotlib import colors\nfrom mpl_toolkits.axes_grid1 import make_axes_locatable\nfrom scipy.spatial.distance import cdist\nfrom scipy import stats\nimport random\nfrom sklearn.cluster import KMeans\nimport seaborn as sns",
    "crumbs": [
      "Spatial Analysis",
      "Intro to Spatial Analysis"
    ]
  },
  {
    "objectID": "course/spatial_demo.html#inspect-your-data",
    "href": "course/spatial_demo.html#inspect-your-data",
    "title": "Intro to Spatial Analysis",
    "section": "1. Inspect your data",
    "text": "1. Inspect your data\nMantis Viewer or napari can be useful for visualizing your data, but it’s always a good idea to be able to open, view, and manipulate your data in Python, as it will give you more flexibility when analyzing your data.\n\n%matplotlib notebook\n\n# Directory where the data lives\ndata_dir = \"example_data\"\n\n# Example image\nex_fov = \"fov1\"\n\n# Look at a few markers\nmarkers = [\"CD45\",\"Collagen1\"]\n\n# Set-up plots\nplt.rcParams['figure.figsize'] = [10, 5]\nfig, ax = plt.subplots(1,len(markers), sharex=True, sharey=True)\nfor i,mark in enumerate(markers):\n    im_array = np.array(io.imread(os.path.join(data_dir, ex_fov, \"image_data\", mark+\".tiff\")))\n    ax[i].imshow(im_array, origin=\"lower\", cmap='gray', vmax=np.quantile(im_array,0.99))\n    ax[i].set_title(mark)\n    ax[i].axis('off')\nplt.tight_layout()\n\nWe have already segmented these images to identify the location of single cells in the image using Mesmer. If you are interested in applying Mesmer to your own data, you can see the notebook here.\nWe can inspect the output of Mesmer here. In the segmentation output, each cell has a unique label. For example, all pixels with the value of 1 belong to the same cell, all pixels with the value of 2 belong to another cell, etc.\n\n# Read in segmentation mask\nseg_path = os.path.join(data_dir, ex_fov, \"masks\", \"segmentation_whole_cell.tiff\")\nseg_array = np.array(io.imread(seg_path)).squeeze() #squeeze changes dimensions from (1,2048,2048) to (2048,2048)\n\n# Set up plot\nfig, ax = plt.subplots(figsize=[5,5])\nplt.imshow(seg_array)\nplt.axis('off')\nplt.tight_layout()\n\nOnce we have segmented our data, we can generate cell tables where each row corresponds to one cell that was identified in the image. We have already identified the phenotype of each cell using Pixie. If you are interested in running Pixie on your own data, see the notebook here.\nThe ‘label’ column in the table corresponds to the cell IDs (pixel values in the segmentation output), ‘centroid-0’ and ‘centroid-1’ correspond to the center point of each cell, and the ‘cell_cluster’ column is the cell phenotype that we determined using Pixie.\n\n# Read in cell table\ncell_table_path = os.path.join(data_dir, \"cell_table.csv\")\ncell_table = pd.read_csv(cell_table_path)\n\n# Subset for only the example fov we're looking at here\nfov_cell_table = cell_table.loc[cell_table['fov'] == ex_fov]\nfov_cell_table\n\nWe can then create a cell phenotype map, where each cell is colored according to its cell phenotype.\n\n# Define colors we want for each cell type\nall_colors = {}\nall_colors['APC'] = '#4E79A7'\nall_colors['B'] = '#A0CBE8'\nall_colors['Cancer'] = '#F28E2B'\nall_colors['Cancer_EMT'] = '#FFBE7D'\nall_colors['Cancer_Other'] = '#59A14F'\nall_colors['CD4T'] = '#8CD17D'\nall_colors['CD8T'] = '#B6992D'\nall_colors['Endothelium'] = '#F1CE63'\nall_colors['Fibroblast'] = '#499894'\nall_colors['Immune_Other'] = '#86BCB6'\nall_colors['M1_Mac'] = '#E15759'\nall_colors['M2_Mac'] = '#FF9D9A'\nall_colors['Mac_Other'] = '#79706E'\nall_colors['Mast'] = '#D4A6C8'\nall_colors['Monocyte'] = '#D37295'\nall_colors['Neutrophil'] = '#FABFD2'\nall_colors['NK'] = '#B07AA1'\nall_colors['Other'] = '#BAB0AC'\nall_colors['Stroma'] = '#9D7660'\nall_colors['T_Other'] = '#D7B5A6'\nall_colors['Treg'] = '#FFFF99'\n\n# Create table matching each color to a unique ID\ncolors_list = [(key,value) for key,value in all_colors.items()]\nall_colors_df = pd.DataFrame(colors_list, columns=['cell_cluster','color'])\nall_colors_df['pheno_id'] = all_colors_df.index + 1\n\n# Make color map for plotting\nmycols = all_colors_df['color'].tolist()\nmycols.insert(0,'#000000') # add black for empty slide, will have id 0\nmycols.append('#FFFFFF') # add white for cell borders, will have id max_n+1\ncolmap = colors.ListedColormap(mycols)\nmax_n = np.max(all_colors_df['pheno_id'])\nbounds = [i-0.5 for i in np.linspace(0,max_n+2, max_n+3)]\nnorm = colors.BoundaryNorm(bounds, colmap.N)\n\n# Define function for making cell phenotype map (CPM)\ndef create_cpm(fov_name, cell_table, all_colors_df, seg_array, fig, ax):\n    # Subset cell table for this FOV\n    one_fov_cell_table = cell_table.loc[cell_table['fov'] == fov_name]\n    # Combine with cell table\n    one_fov_cell_table = one_fov_cell_table.merge(all_colors_df, how='left')\n    # Make dictionary mapping each cell to its phenotype id\n    fov_cell_dict = dict(zip(one_fov_cell_table['label'], one_fov_cell_table['pheno_id']))\n    # Add 0 for empty slide\n    fov_cell_dict[0] = 0\n    \n    # Make new image where each pxiel corresponds to its phenotype id\n    # Use 'vectorize' in numpy package to speed up this operation\n    cpm_array = np.vectorize(fov_cell_dict.get)(seg_array)\n\n    # Find the borders of cells\n    predicted_contour_mask = find_boundaries(seg_array, connectivity=1, mode='inner').astype(np.uint8)\n    # Color this  border white\n    cpm_array[predicted_contour_mask &gt; 0] = max_n+1\n\n    # Plot\n    cpm_image = colmap(norm(cpm_array))\n    ax.imshow(cpm_image)\n    \n    return\n\n\n# Create CPM for example FOV\nfig, ax = plt.subplots(figsize=[8,8])\ncpm = create_cpm(ex_fov, cell_table, all_colors_df, seg_array, fig, ax)\nplt.axis('off')\n\n# Add colorbar to image\ndivider = make_axes_locatable(fig.gca())\ncax = divider.append_axes(position=\"right\", size=\"5%\", pad=\"3%\")\ncbar = fig.colorbar(cm.ScalarMappable(norm=norm, cmap=colmap),\n                    cax=cax, orientation=\"vertical\", use_gridspec=True, pad=0.1,\n                    shrink=0.9, drawedges=True)\ncbar_labels = all_colors_df['cell_cluster'].to_list()\ncbar_labels.insert(0,'Empty') # add black for empty slide, will have id 0\ncbar_labels.append('Cell border') # add white for cell borders, will have id max_n+1\n\ncbar.ax.set_yticks(\n    ticks=np.arange(len(cbar_labels)),\n    labels=cbar_labels\n)\ncbar.minorticks_off()\n\nplt.tight_layout()",
    "crumbs": [
      "Spatial Analysis",
      "Intro to Spatial Analysis"
    ]
  },
  {
    "objectID": "course/spatial_demo.html#quanitfying-cell-populations",
    "href": "course/spatial_demo.html#quanitfying-cell-populations",
    "title": "Intro to Spatial Analysis",
    "section": "2. Quanitfying cell populations",
    "text": "2. Quanitfying cell populations\nThere are many methods for cell enumeration, including counting the number of cells of each cell type, calculating cell frequency by dividing by the total number of cells, and calculating cell density by dividing by the total tissue area. While the first approach is the most straightforward, simply counting the number of cells depends on the amount of tissue present in the image. Cell frequencies normalize the number of each cell type to the total number of cells in the image such that all cell types sum to 1. As a result, cell frequencies are not confounded by differences in the size of the section. However, one drawback of cell frequencies is that they can obscure the reason that the amount of one cell type differs between samples. For example, consider a simple example of comparing the number of tissue resident macrophages in healthy and inflamed tissue. Even if the number of macrophages is the same in both images, it may seem like tissue resident macrophages are decreasing in the latter relative to the former. In reality, however, the absolute count of tissue resident macrophages is not changing; they are just outnumbered by infiltrating immune cells. Only considering cell frequencies in such a scenario would lead to the incorrect conclusion that macrophages are decreasing in the inflamed state. One solution is to also examine the density of each cell population by dividing by the total tissue area.\nHere, we are showing an example of two FOVs that have different amounts of immune infiltrate, ECM content, and total area (as determined using the slide background mask). We have generated masks of the ECM (using the composite signal of ECM markers) and empty sllide (for MIBI, we can determine empty slide by measuring gold signal, since we use gold sputtered slides). Using the empty slide mask, we can estimate tissue area (by taking the inverse). In the ECM and empty slide masks shown here, white indicates ECM or empty slide, respectively.\n\n# Example images\nfov1 = \"fov1\"\nfov2 = \"fov2\"\n\n# Set-up plots\nplt.rcParams['figure.figsize'] = [10,6]\nfig, ax = plt.subplots(2,3)\n\n# Look at cell phenotype maps, ECM mask, and gold mask\nfor i,fov in enumerate([fov1,fov2]):\n    # CPMs\n    seg_path = os.path.join(data_dir,fov,\"masks\",\"segmentation_whole_cell.tiff\")\n    seg_array = np.array(io.imread(seg_path)).squeeze()\n    create_cpm(fov, cell_table, all_colors_df, seg_array, fig, ax[i,0])\n    ax[i,0].set_title(\"Cell phenotype map\")\n    ax[i,0].set_ylabel(fov)\n    ax[i,0].set_yticklabels([])\n    ax[i,0].set_xticklabels([])\n    ax[i,0].set_yticklabels([])\n    ax[i,0].set_xticks([])\n    ax[i,0].set_yticks([])\n    \n    # ECM\n    ecm_path = os.path.join(data_dir,fov,\"masks\",\"total_ecm.tiff\")\n    ecm_array = np.array(io.imread(ecm_path))\n    ax[i,1].imshow(ecm_array, cmap='gray')\n    ax[i,1].axis('off')\n    ax[i,1].set_title(\"ECM\")\n    \n    # Empty slide\n    empty_slide_path = os.path.join(data_dir,fov,\"masks\",\"empty_slide.tiff\")\n    empty_slide_array = np.array(io.imread(empty_slide_path))\n    ax[i,2].imshow(empty_slide_array, cmap='gray')\n    ax[i,2].axis('off')\n    ax[i,2].set_title(\"Empty slide\")\n\nplt.tight_layout()\n\n\nSimply count the number of each cell type\n\ncell_table_keep = cell_table.loc[cell_table['fov'].isin([fov1,fov2])]\ncount_cells = cell_table_keep.groupby('fov')['cell_cluster'].value_counts().reset_index(name='count')\ncount_cells\n\n\n\nCalculate frequency (divided by total number of cells)\n\n# Get total number of cells per FOV\ntotal_counts = cell_table_keep.groupby('fov').size().to_frame('total_cells')\ntotal_counts = total_counts.reset_index()\n\n# Calculate frequency\ncount_cells = count_cells.merge(total_counts, on='fov')\ncount_cells['frequency'] = count_cells['count'] / count_cells['total_cells']\ncount_cells\n\n\n\nCalculate density (divided by tissue area)\n\n# Calculate tissue area (inverse of empty slide)\ndef get_tissue_area(fov_name):\n    empty_slide_path = os.path.join(data_dir,fov_name,\"masks\",\"empty_slide.tiff\")\n    empty_slide_array = np.array(io.imread(empty_slide_path))\n    # Get total pixels that belong to empty slide\n    empty_slide_px = np.sum(empty_slide_array) #pixel value is 1 if it is empty slide\n    # Get number of pixels that belong to tissue (total image - empty slide)\n    total_size = empty_slide_array.shape\n    tissue_area_px = (total_size[0]*total_size[1]) - empty_slide_px\n    # These images are 2048px x 2048px, imaged at 800um x 800um\n    # Therefore, the size of 1 pixel is 800um/2048px = 0.39 um/px, area is 0.15 um^2\n    return (fov_name, tissue_area_px*0.15)\n\n# Calculate for all FOVs\nall_tissue_px = [get_tissue_area(fov) for fov in [fov1,fov2]]\nall_tissue_df = pd.DataFrame(all_tissue_px, columns=['fov','area'])\n\n# Calculate density\ncount_cells = count_cells.merge(all_tissue_df, on='fov')\ncount_cells['density'] = count_cells['count'] / count_cells['area']\ncount_cells\n\n\n\nCompare\n\ncell_type = 'CD4T'\nprint(cell_type)\nprint('FOV:', fov1,\n      ', total_counts:', count_cells.loc[(count_cells['fov']==fov1) & (count_cells['cell_cluster']==cell_type)]['count'].values[0],\n      ', frequency:', count_cells.loc[(count_cells['fov']==fov1) & (count_cells['cell_cluster']==cell_type)]['frequency'].values[0],\n      ', density:', count_cells.loc[(count_cells['fov']==fov1) & (count_cells['cell_cluster']==cell_type)]['density'].values[0])\nprint('FOV:', fov2,\n      ', total_counts:', count_cells.loc[(count_cells['fov']==fov2) & (count_cells['cell_cluster']==cell_type)]['count'].values[0],\n      ', frequency:', count_cells.loc[(count_cells['fov']==fov2) & (count_cells['cell_cluster']==cell_type)]['frequency'].values[0],\n      ', density:', count_cells.loc[(count_cells['fov']==fov2) & (count_cells['cell_cluster']==cell_type)]['density'].values[0])\n\n\n\nAdditional exercises\n\nChange the “cell_type” in the code block above and evaluate how the quantification method is different for different cell types.\nCreate a stacked bar plot showing the cell type composition of each fov (x axis should be fov1 and fov2, y axis is cell type composition). Try making this plot with counts, frequency, and density.",
    "crumbs": [
      "Spatial Analysis",
      "Intro to Spatial Analysis"
    ]
  },
  {
    "objectID": "course/spatial_demo.html#cell-cell-enrichment-global",
    "href": "course/spatial_demo.html#cell-cell-enrichment-global",
    "title": "Intro to Spatial Analysis",
    "section": "3. Cell-cell enrichment (global)",
    "text": "3. Cell-cell enrichment (global)\nThe goal of pairwise enrichment analysis is to assess whether any two given cell populations colocalize with each other. For example, this approach could be used to determine if T cells preferentially colocalize with tumor cells. In asking such questions, we recommend taking this one step further: Do two cell populations colocalize with each other more often than would be expected by chance? Depending on the frequency of the cell populations and native tissue structure, it may not be possible to determine whether some pairwise relationships are truly preferential or random. The goal of this approach is to minimize potential confounding effects specific to each image that might not be related to the biological question of interest. For example, in a tissue section equally composed of only two cell populations, pairwise enrichment is likely to occur simply by random chance.\nWith this in mind, we can assess the statistical significance of pairwise enrichment by comparing how often two cell populations are found in close proximity to each other compared with a null distribution. To construct a null distribution, we use a bootstrapping approach by selectively randomizing the location of the cell populations of interest, calculating pairwise enrichment, and repeating this process a large number of times (&gt;100 times is typical). In the assessment of preferential enrichment for two cell populations defined by their expression of marker A or B, the simplest way to generate a null distribution is to randomize the location of one cell population across all cells in the image while keeping the location of the other cell population fixed. For both the original and randomized images, cells positive for A or B are colocalized if the distance between them is less than or equal to a user-defined value. The number of A–B interactions in the original image is then compared with the null distribution to determine statistical significance.\n\n# Example image\nex_fov = \"fov2\"\n\n# Determine cell types to look at\npheno1 = \"Cancer\"\npheno2 = \"CD8T\"\n\n# Threshold to determine if two cells are \"close\"\ndist_thresh = 50\n\n# Number of bootstraps for generating null distribution\nbootstrap_n = 100\n\n\nCalculate the distance between all cells in the FOV\n\n# Subset cell table for only cells in this FOV\nfov_cell_table = cell_table.loc[cell_table['fov'] == ex_fov].reset_index(drop=True)\n# Make list of all cell centroids\nall_centroids = list(zip(fov_cell_table['centroid-0'],fov_cell_table['centroid-1']))\n# Get distance between all cells\ndist_mat = cdist(all_centroids, all_centroids, 'euclidean')\n# Print dimensions of distance matrix\nprint(\"Dimensions of dist_mat: \", dist_mat.shape)\n\ndist_mat\n\nThis distance matrix has dimensions of total number of cells x total number of cells. The values in the matrix indicate the distance between any 2 cells. The indices of the distance matrix correspond to the indices of the cell table. So the first row in the distance matrix corresponds to the distance between cell label 1 and all other cells in the image. After calculating the distance between all cells, we can subset it for our cell type of choice. We can then count the number of close contacts between two cell types.\n\n\nCount number of close contacts between pheno1 and pheno2\n\n# Get index of cells belonging to pheno1 and pheno2\npheno1_idx = fov_cell_table[fov_cell_table['cell_cluster'] == pheno1].index.to_list()\npheno2_idx = fov_cell_table[fov_cell_table['cell_cluster'] == pheno2].index.to_list()\n\n# Only keep pheno1 cells in x-axis of distance matrix\npheno1_dist_mat = dist_mat[pheno1_idx,:]\n# Binarize the distance matrix for distances that are within the defined threshold\nbin_mask = (pheno1_dist_mat &lt; dist_thresh) & (pheno1_dist_mat &gt; 0)\n# Change true/false to 1/0\npheno1_dist_mat_bin = bin_mask*1\n\n# Subset this distance matrix for pheno2 cells in y-axis of distance matrix\ntrue_dist_mat_bin = pheno1_dist_mat_bin[:,pheno2_idx]\n# Inspect the shape of this matrix, should be number of cells of pheno1 x number of cells of pheno2\n# Each element in the matrix is the distance between a pheno1 cell and a pheno2 cell\nprint(\"Shape of subsetted distance matrix: \", true_dist_mat_bin.shape)\n\n# For each pheno1 cell, count number of \"close\" contacts with pheno2 cells\ntrue_close_contacts = np.sum(true_dist_mat_bin, axis=1)\n# Take the average across all pheno1 cells\ntrue_close_contacts_mean = np.mean(true_close_contacts)\nprint(\"Average number of close contacts between pheno1 and pheno2 cells: \", true_close_contacts_mean)\n\nAbove, we have determined the average number of close contacts between pheno1 and pheno2 cells in this image. To determine whether this is a significant number, we can compare it against a null distribution, generated by randomly permuting cell labels. We keep the pheno1 cells constant, then randomize the location of pheno2 cells in the image. For each randomization, we calculate the number of close contacts. We repeat this many times to generate the null distribution.\n\n\nGenerate null distribution by bootstrapping\n\n# Get all possible cell indices (total pool of available cells to randomize)\nall_idx = fov_cell_table.index.to_list()\n# Remove cells that are of pheno1 from this pool (since they are held constant in this randomization)\nall_idx = [x for x in all_idx if x not in pheno1_idx]\n# Get total number of cells that are pheno2\nnum_pheno2 = len(pheno2_idx)\n\n# Randomly sample all cells to be labeled as pheno2 (bootstrapping)\nall_bootstrap = []\nfor _ in range(bootstrap_n):\n    # Select num_pheno2 random numbers, represents the indices of the randomly selected cells\n    random_pheno2_idx = random.sample(all_idx, num_pheno2)\n    # Subset the distance matrix to only keep these randomly selected cells\n    keep_dist_mat_bin = pheno1_dist_mat_bin[:,random_pheno2_idx]\n    # Find the total number of close contacts between pheno1 cells and randomly selected cells\n    close_contacts = np.sum(keep_dist_mat_bin, axis=1)\n    # Take the mean across all cells of pheno1\n    close_contacts_mean = np.mean(close_contacts)\n    # Add this value to the list of all bootstraps\n    all_bootstrap.append(close_contacts_mean)\n\n\n\nCompare null distrbution to actual number of close contacts\n\nfig, ax = plt.subplots(figsize=(5,3))\n# Blue histogram is null distribution\nax.hist(all_bootstrap, density=True,  bins=10, alpha=0.5)\n# Red line is actual number of close contacts\nplt.axvline(x=true_close_contacts_mean, color='red', linestyle='--', linewidth=2)\nplt.show()\n\n\n# Calculate statistics of null distribution\nmuhat, sigmahat = stats.norm.fit(all_bootstrap)\n# Calculate z score based on distribution\nz = (true_close_contacts_mean - muhat) / sigmahat\nprint(\"z-score: \", z)\n\n\n\nAdditional exercises\n\nFlip “pheno1” and “pheno2” - is this calculation symmetric? Conceptually, should it be symmetric?\nPlay around with the distance threshold for determing “close” cells.\nPlay around with “pheno1” and “pheno2” and evaluate these metrics for various pairs of cells.\nPick a few different pheno1-pheno2 pairs. For one FOV, loop through this pairwise enrichment function for these pairs of cells. Store this output in a table (hint: column names could be “pheno1”, “pheno2”, “z-score”).\nRepeat this calculation for the second FOV.\nCompare z-scores between fov1 and fov2.",
    "crumbs": [
      "Spatial Analysis",
      "Intro to Spatial Analysis"
    ]
  },
  {
    "objectID": "course/spatial_demo.html#cell-cell-enrichment-context-dependent",
    "href": "course/spatial_demo.html#cell-cell-enrichment-context-dependent",
    "title": "Intro to Spatial Analysis",
    "section": "4. Cell-cell enrichment (context dependent)",
    "text": "4. Cell-cell enrichment (context dependent)\nThis randomization strategy is robust for mitigating confounding effects attributable to the frequency of each cell population. However, this approach does not control for biases inherent to tissue structure. Consequently, interactions between two proteins may appear to be spatially enriched as a result of tissue structure or cell-specific expression, as markers expressed exclusively by certain cells will be heavily influenced by the location of those cells. For example, two immune cell populations that are known to preferentially localize in germinal centers are biased to be enriched with each other. While one possibility is that this relationship is indeed preferential, another possibility is that spatial enrichment is merely a result of both cell types being restricted to a smaller histological compartment.\nTo address this possibility, null distributions can also be generated in a context-dependent manner. In context-dependent spatial enrichment analysis, randomizations are restricted to occur only within a given cellular compartment or cell subset. In the germinal center example, randomizations can be restricted to occur only within the germinal center. As a result, spurious enrichments that might occur between noninteracting cell populations that happen to colocalize to germinal centers are accounted for in the null distribution.\nFor this dataset, we have generated masks for the cancer and stroma, core and border regions (using composite channels representative for each region).\n\nLook at cancer and stroma masks\n\ncancer_core = io.imread(os.path.join(data_dir, ex_fov, \"masks\", \"cancer_core.tiff\"))\ncancer_border = io.imread(os.path.join(data_dir, ex_fov, \"masks\", \"cancer_border.tiff\"))\nstroma_core = io.imread(os.path.join(data_dir, ex_fov, \"masks\", \"stroma_core.tiff\"))\nstroma_border = io.imread(os.path.join(data_dir, ex_fov, \"masks\", \"stroma_border.tiff\"))\n\nfig, ax = plt.subplots(1,4,figsize=[12,4])\nax[0].imshow(cancer_core, cmap='gray')\nax[0].axis('off')\nax[0].set_title(\"Cancer core\")\nax[1].imshow(cancer_border, cmap='gray')\nax[1].axis('off')\nax[1].set_title(\"Cancer border\")\nax[2].imshow(stroma_core, cmap='gray')\nax[2].axis('off')\nax[2].set_title(\"Stroma core\")\nax[3].imshow(stroma_border, cmap='gray')\nax[3].axis('off')\nax[3].set_title(\"Stroma border\")\nplt.tight_layout()\nplt.show()\n\n\n\nRepeat calculation for number of close contacts but subset for cells within the cancer core only\n\n# Get segmentation mask\nseg_path = os.path.join(data_dir, ex_fov, \"masks\", \"segmentation_whole_cell.tiff\")\nseg_array = np.array(io.imread(seg_path)).squeeze()\n\n# Make new segmentation mask where everything outside of the cancer core is 0\nmasked_seg_array = np.copy(seg_array)\nmasked_seg_array[cancer_core == 0] = 0\n# Only keep cell labels that are within the cancer core\nmasked_cell_labels = np.unique(masked_seg_array)\nmasked_cell_labels = [x for x in masked_cell_labels if x != 0] #remove 0 (which is empty slide)\n# Subset cell table for only the cells within the cancer core\nfov_cell_table_masked = fov_cell_table[fov_cell_table['label'].isin(masked_cell_labels)]\n\n\n# Get index of cells belonging to pheno1 and pheno2, but this time, only keep labels that are in the cancer core mask\npheno1_idx = fov_cell_table_masked[fov_cell_table_masked['cell_cluster'] == pheno1].index.to_list()\npheno2_idx = fov_cell_table_masked[fov_cell_table_masked['cell_cluster'] == pheno2].index.to_list()\n\n# Only keep pheno1 cells in x-axis of distance matrix\npheno1_dist_mat = dist_mat[pheno1_idx,:]\n# Binarize the distance matrix for distances that are within the defined threshold\nbin_mask = (pheno1_dist_mat &lt; dist_thresh) & (pheno1_dist_mat &gt; 0)\n# Change true/false to 1/0\npheno1_dist_mat_bin = bin_mask*1\n\n# Subset this distance matrix for pheno2 cells in y-axis of distance matrix\ntrue_dist_mat_bin = pheno1_dist_mat_bin[:,pheno2_idx]\n\n# For each pheno1 cell, count number of \"close\" contacts with pheno2 cells\ntrue_close_contacts = np.sum(true_dist_mat_bin, axis=1)\ntrue_close_contacts_mean = np.mean(true_close_contacts)\nprint(\"Average number of close contacts between pheno1 and pheno2 cells: \", true_close_contacts_mean)\n\n\n\nGenerate null distribution, only randomizing within cancer core\n\n# Get all possible cell indices (total pool of available cells to randomize)\nall_idx = fov_cell_table_masked.index.to_list()\n# Remove cells that are of pheno1 from this pool (since they are held constant in this randomization)\nall_idx = [x for x in all_idx if x not in pheno1_idx]\n# Get total number of cells that are pheno2\nnum_pheno2 = len(pheno2_idx)\n\n# Randomly sample all cells to be labeled as pheno2 (bootstrapping)\nall_bootstrap = []\nfor _ in range(bootstrap_n):\n    # Select num_pheno2 random numbers, represents the indices of the randomly selected cells\n    random_pheno2_idx = random.sample(all_idx, num_pheno2)\n    # Subset the distance matrix to only keep these randomly selected cells\n    keep_dist_mat_bin = pheno1_dist_mat_bin[:,random_pheno2_idx]\n    # Find the total number of close contacts between pheno1 cells and randomly selected cells\n    close_contacts = np.sum(keep_dist_mat_bin, axis=1)\n    # Take the mean across all cells of pheno1\n    close_contacts_mean = np.mean(close_contacts)\n    # Add this value to the list of all bootstraps\n    all_bootstrap.append(close_contacts_mean)\n\n\n\nCompare null distrbution to actual number of close contacts\n\nfig, ax = plt.subplots(figsize=(5,3))\n# Blue histogram is null distribution\nax.hist(all_bootstrap, density=True,  bins=10, alpha=0.5)\n# Red line is actual number of close contacts\nplt.axvline(x=true_close_contacts_mean, color='red', linestyle='--', linewidth=2)\nplt.show()\n\n\n# Calculate statistics of null distribution\nmuhat, sigmahat = stats.norm.fit(all_bootstrap)\n# Calculate z score based on distribution\nz = (true_close_contacts_mean - muhat) / sigmahat\nprint(\"z-score: \", z)\n\n\n\nAdditional exercises\n\nPlay with different regions and phenotype pairs.",
    "crumbs": [
      "Spatial Analysis",
      "Intro to Spatial Analysis"
    ]
  },
  {
    "objectID": "course/spatial_demo.html#cellular-microenvironments",
    "href": "course/spatial_demo.html#cellular-microenvironments",
    "title": "Intro to Spatial Analysis",
    "section": "5. Cellular microenvironments",
    "text": "5. Cellular microenvironments\nCell phenotypes are regulated in part by their local cellular niche, or microenvironment, which is defined as a collection of cell lineages found to be spatially colocalized and conserved across the data set. We can employ several methods to identify these microenvironments. The first step in this process is to quantify the local distribution of cell lineages around each cell in the data set. Here, we are enumerating the number of cells in each lineage within a set pixel radius from the index cell. Various machine learning methods can then be used to identify distinct cellular microenvironments that occur repeatedly across the data set. Here, we are using k-means clustering.\n\n# FOVs to include in this analysis\nall_fovs = [\"fov1\", \"fov2\"]\n\n# Set distance for the neighborhood around the cell\nnh_dist = 50\n\n# Set the number of \"expected\" microenvironments\nk = 3\n\n# Set a seed for reproducibility\nseed = 2024\n\n# Get list of all cell types in our data\nall_cell_types = cell_table['cell_cluster'].unique()\n\n\nFind neighbors for each cell\n\n# Define function to count the neighbors of each cell (where each cell is one row in the distance matrix)\ndef one_cell_nh(row):\n    # Get index of neighbors (defined as \"close\" in the distance matrix)\n    # In the distance matrix ('row' is just one row in the distance matrix), 1 indicates that the two cells were \"close\"\n    neighbor_idx = np.where(row == 1)[0]\n    # Initialize output as a dictionary, start with count of 0 for every cell type\n    nh = {key:0 for key in all_cell_types}\n    # Convert these indices to cell types\n    if len(neighbor_idx) &gt; 0:\n        nh_cells = fov_cell_table.loc[neighbor_idx]['cell_cluster'].values\n        # Count the number of each cell type\n        unique_cell_type, counts = np.unique(nh_cells, return_counts=True)\n        # Store the data in the dictionary\n        for i,one_cell_type in enumerate(unique_cell_type):\n            nh[one_cell_type] = counts[i]\n    return nh\n\n# Iterate through all fovs to get neighbors of all cells\nall_nh_list = [] #store information for each fov in a list\nfor fov in all_fovs:\n    # Subset cell table for only cells in this FOV\n    fov_cell_table = cell_table.loc[cell_table['fov'] == fov].reset_index(drop=True)\n    # Make list of all cell centroids\n    all_centroids = list(zip(fov_cell_table['centroid-0'],fov_cell_table['centroid-1']))\n    # Get distance between all cells\n    dist_mat = cdist(all_centroids, all_centroids, 'euclidean')\n\n    # Binarize distance matrix (1 if two cells are \"close\")\n    dist_mat_bin = dist_mat &lt; nh_dist\n    dist_mat_bin = dist_mat_bin*1\n\n    # Remove itself as its own neighbor\n    dist_mat_bin[dist_mat == 0] = 0\n    \n    # Apply function to every cell in the data\n    nh_dicts = np.apply_along_axis(one_cell_nh, axis=1, arr=dist_mat_bin)\n    # Turn into dataframe\n    nh_df = pd.DataFrame(nh_dicts.tolist())\n    # Get total number of neighbors\n    nh_df['total_neighbors'] = nh_df.sum(axis=1)\n    # Combine with data from cell table\n    nh_df = pd.merge(fov_cell_table, nh_df, left_index=True, right_index=True)\n    all_nh_list.append(nh_df)\n\n# Turn data into dataframe\nall_nh_df = pd.concat(all_nh_list)\nall_nh_df\n\n\n\nk-means clustering\nWhen finding the neighborhood, you can either use the number of cells in the neighborhood of each cell, or the frequency (divided by the total number of cells). The interpretation of each of these is slightly different. We encourage you to try both ways (you can comment/uncomment the line of code that calculates frequency below). By clustering on the neighborhoods, we can identify distinct microenvironments.\n\n# If you want to try clustering using frequency (meaning dividing by the total number of cells, uncomment this line\n# all_nh_df[all_cell_types] = all_nh_df[all_cell_types].divide(all_nh_df['total_neighbors'], axis=0)\n\n# Remove cells that have no neighbors\nall_nh_df_zeros_removed = all_nh_df.loc[all_nh_df['total_neighbors'] &gt; 0].copy()\n\n# Only keep columns we want for clustering\nkmeans_input = all_nh_df_zeros_removed[all_cell_types]\n\n# Cluster\ncluster_fit = KMeans(n_clusters=k, random_state=seed, n_init=10).fit(kmeans_input)\n# Add 1 to labels to avoid cluster number 0 (because output of kmeans is 0-indexed)\ncluster_labels = [\"ME\"+str(x) for x in cluster_fit.labels_+1]\n\n# Add cluster labels to cell table\nall_nh_df_zeros_removed['me_cluster'] = cluster_labels\n\n# Merge with the big cell table, assign unassigned cells (becaue no neighbors) to k+1\nall_nh_df = all_nh_df.merge(all_nh_df_zeros_removed, how=\"left\")\nall_nh_df['me_cluster'] = all_nh_df['me_cluster'].fillna(\"Unassigned\")\nall_nh_df\n\n\n\nVisualize microenvironments\nWe can visualize the number of each cell type that is assigned to each microenvironment.\n\n# Count the number of each cell type assigned to each microenvironment\nnum_cell_types_dat_long = all_nh_df_zeros_removed.groupby(['me_cluster', 'cell_cluster']).size().reset_index(name='counts')\n# Reformat this table\nnum_cell_types_dat = num_cell_types_dat_long.pivot(index='me_cluster', columns='cell_cluster', values='counts')\nnum_cell_types_dat.fillna(0, inplace=True)\n\n# Make heatmap\nheatmap = sns.clustermap(\n    num_cell_types_dat.apply(stats.zscore, axis=1), # Apply a z-score for better visualization\n    cmap = \"vlag\",\n    center = 0,\n    vmin = -3,\n    vmax = 3\n)\n\n\n\nVisualize cell phenotypes maps, colored by microenvironment\n\n# Define colors we want for each microenvironment (add colors here if you have larger k, you need k+1 colors)\nall_colors = {}\nall_colors['ME1'] = '#4E79A7'\nall_colors['ME2'] = '#59A14F'\nall_colors['ME3'] = '#D37295'\nall_colors['Unassigned'] = '#79706E'\n\n# Create table matching each color to a unique ID\ncolors_list = [(key,value) for key,value in all_colors.items()]\nall_colors_df = pd.DataFrame(colors_list, columns=['me_cluster','color'])\nall_colors_df['pheno_id'] = all_colors_df.index + 1\n\n# Make color map for plotting\nmycols = all_colors_df['color'].tolist()\nmycols.insert(0,'#000000') # add black for empty slide, will have id 0\nmycols.append('#FFFFFF') # add white for cell borders, will have id max_n+1\ncolmap = colors.ListedColormap(mycols)\nmax_n = np.max(all_colors_df['pheno_id'])\nbounds = [i-0.5 for i in np.linspace(0,max_n+2, max_n+3)]\nnorm = colors.BoundaryNorm(bounds, colmap.N)\n\n\n# Create microenvironment masks\nfor fov in all_fovs:\n    # Get segentation array\n    seg_path = os.path.join(data_dir, fov, \"masks\", \"segmentation_whole_cell.tiff\")\n    seg_array = io.imread(seg_path).squeeze()\n    \n    # Make CPM using function we created above\n    fig, ax = plt.subplots(figsize=[8,8])\n    cpm = create_cpm(fov, all_nh_df, all_colors_df, seg_array, fig, ax)\n    plt.axis('off')\n    ax.set_title(fov)\n\n    # Add colorbar\n    divider = make_axes_locatable(fig.gca())\n    cax = divider.append_axes(position=\"right\", size=\"5%\", pad=\"3%\")\n    cbar = fig.colorbar(cm.ScalarMappable(norm=norm, cmap=colmap),\n                        cax=cax, orientation=\"vertical\", use_gridspec=True, pad=0.1,\n                        shrink=0.9, drawedges=True)\n    cbar_labels = all_colors_df['me_cluster'].to_list()\n    cbar_labels.insert(0,'Empty') # add black for empty slide, will have id 0\n    cbar_labels.append('Cell border') # add white for cell borders, will have id max_n+1\n\n    cbar.ax.set_yticks(\n        ticks=np.arange(len(cbar_labels)),\n        labels=cbar_labels\n    )\n    cbar.minorticks_off()\n\n    plt.tight_layout()\n\n\n\nAdditional exercises\n\nCompare generating neighborhoods using counts vs. frequency.\nChange the k used for k-means clustering. How do the results change?\nInertia and silhouette score are two metrics that could be useful for helping us choose the best k. Try different k’s and extracting the inertia value (see https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html, intertia_ is an attribute of the output of k-means). Plot k on the x axis and inertia on the y axis. What trend do you see?",
    "crumbs": [
      "Spatial Analysis",
      "Intro to Spatial Analysis"
    ]
  },
  {
    "objectID": "course/1_introR.html",
    "href": "course/1_introR.html",
    "title": "Introduction to R",
    "section": "",
    "text": "RStudio is an integrated development environment (IDE) for R. It provides a user-friendly interface for coding, debugging, and data analysis. We use RStudio for its convenience and powerful features.\n\n\n\nConsole: Where you can directly type and execute R commands.\n\nScript Editor: Where you write and save your R scripts.\n\nEnvironment and History: Displays objects in your workspace and your command history.\n\nFiles and Plots: Helps manage files and view plots.\n\nPackages: Shows installed packages and allows you to install and load new ones.",
    "crumbs": [
      "Intro to R",
      "Introduction to R"
    ]
  },
  {
    "objectID": "course/1_introR.html#interface-orientation",
    "href": "course/1_introR.html#interface-orientation",
    "title": "Introduction to R",
    "section": "",
    "text": "Console: Where you can directly type and execute R commands.\n\nScript Editor: Where you write and save your R scripts.\n\nEnvironment and History: Displays objects in your workspace and your command history.\n\nFiles and Plots: Helps manage files and view plots.\n\nPackages: Shows installed packages and allows you to install and load new ones.",
    "crumbs": [
      "Intro to R",
      "Introduction to R"
    ]
  },
  {
    "objectID": "course/1_introR.html#numeric",
    "href": "course/1_introR.html#numeric",
    "title": "Introduction to R",
    "section": "Numeric",
    "text": "Numeric\n# Numeric variable\nnum_var &lt;- 10\nprint(num_var)  # Output: 10\n\n# Arithmetic operations\nresult &lt;- num_var * 2\nprint(result)  # Output: 20",
    "crumbs": [
      "Intro to R",
      "Introduction to R"
    ]
  },
  {
    "objectID": "course/1_introR.html#character",
    "href": "course/1_introR.html#character",
    "title": "Introduction to R",
    "section": "Character",
    "text": "Character\n# Character variable\nchar_var &lt;- \"Hello, World!\"\nprint(char_var)  # Output: Hello, World!\n\n# Concatenation\nnew_string &lt;- paste(char_var, \"This is R programming.\")\nprint(new_string)  # Output: Hello, World! This is R programming.",
    "crumbs": [
      "Intro to R",
      "Introduction to R"
    ]
  },
  {
    "objectID": "course/1_introR.html#integer",
    "href": "course/1_introR.html#integer",
    "title": "Introduction to R",
    "section": "Integer",
    "text": "Integer\n# Integer variable\nint_var &lt;- 20L  # The 'L' suffix indicates an integer\nprint(int_var)  # Output: 20\n\n# Integer arithmetic\nresult &lt;- int_var / 5\nprint(result)  # Output: 4",
    "crumbs": [
      "Intro to R",
      "Introduction to R"
    ]
  },
  {
    "objectID": "course/1_introR.html#matrices",
    "href": "course/1_introR.html#matrices",
    "title": "Introduction to R",
    "section": "Matrices",
    "text": "Matrices\nGene expression data from single-cell RNA sequencing (scRNA-seq) experiments is typically represented as a matrix, where rows correspond to genes and columns correspond to cells. Each cell contains the expression level of a gene, quantified as counts or normalized values. In R, there are several matrix data types commonly used for storing and manipulating gene expression data:\n\nMatrix (matrix): The basic matrix data type in R. It is a two-dimensional array with elements of the same data type.\nData Frame (data.frame): A special type of matrix where columns can contain different data types (e.g., numeric, character, factor). Data frames are commonly used for storing tabular data, including gene expression matrices with additional metadata.\nSparse Matrix (Matrix package): A matrix format optimized for storing large, sparse datasets with many zero values. It conserves memory and speeds up computation for large-scale analyses.\n\nBasic Operations on Matrix Objects\nCreating a Matrix:\n# Create a matrix with random values\nmat &lt;- matrix(rnorm(20), nrow = 4, ncol = 5)\nMatrix operations\nelement &lt;- mat[1, 2]\nprint(element)\n\n# Calculate row sums\nrow_sums &lt;- rowSums(mat)\nprint(row_sums)\n\n# Calculate column sums\ncol_sums &lt;- colSums(mat)\nprint(col_sums)\n\n# Create another matrix\nmat2 &lt;- matrix(rnorm(20), nrow = 5, ncol = 4)\nprint(mat2)\n\n# Perform matrix multiplication\nmat_mult &lt;- mat %*% mat2\nprint(mat_mult)\n\n# Transpose the matrix\nmat_transpose &lt;- t(mat)\nprint(mat_transpose)\n\n# Select the first two rows\nfirst_two_rows &lt;- mat[1:2, ]\nprint(first_two_rows)\n\n# Select the first three columns\nfirst_three_cols &lt;- mat[, 1:3]\nprint(first_three_cols)",
    "crumbs": [
      "Intro to R",
      "Introduction to R"
    ]
  },
  {
    "objectID": "course/1_introR.html#logical",
    "href": "course/1_introR.html#logical",
    "title": "Introduction to R",
    "section": "Logical",
    "text": "Logical\n# Logical variable\nlogical_var &lt;- TRUE\nprint(logical_var)  # Output: TRUE\n\n# Logical operations\nresult &lt;- logical_var & FALSE\nprint(result)  # Output: FALSE\n\n\n# Logical operations\na &lt;- TRUE\nb &lt;- FALSE\n\n# AND operation\nresult_and &lt;- a & b\nprint(result_and)  # Output: FALSE\n\n# OR operation\nresult_or &lt;- a | b\nprint(result_or)   # Output: TRUE\n\n# NOT operation\nresult_not &lt;- !a\nprint(result_not)  # Output: FALSE\n\n# Comparison operators\nx &lt;- 5\ny &lt;- 10\n\n# Greater than\nresult_gt &lt;- x &gt; y\nprint(result_gt)  # Output: FALSE\n\n# Less than\nresult_lt &lt;- x &lt; y\nprint(result_lt)  # Output: TRUE\n\n# Equal to\nresult_eq &lt;- x == y\nprint(result_eq)  # Output: FALSE\n\n# Not equal to\nresult_neq &lt;- x != y\nprint(result_neq)  # Output: TRUE",
    "crumbs": [
      "Intro to R",
      "Introduction to R"
    ]
  },
  {
    "objectID": "course/1_introR.html#while",
    "href": "course/1_introR.html#while",
    "title": "Introduction to R",
    "section": "while",
    "text": "while\nA while loop is a control flow statement that allows code to be executed repeatedly based on a given Boolean condition. The loop executes the loop body as long as the condition remains true. When the condition becomes false, the loop terminates.\n# Example of a while loop\nx &lt;- 1\nwhile (x &lt;= 5) {\n  print(x)\n  x &lt;- x + 1\n}\nConsiderations:\n\nEnsure that the loop has an exit condition that is guaranteed to be met to avoid infinite loops.\nAvoid complex conditions that can make the loop difficult to read and maintain.\nUse while loops when the number of iterations is not known before the loop starts, as opposed to for loops, which are better suited for a known number of iterations.\nManage loop variables carefully to ensure they are updated correctly and the loop condition changes as expected.",
    "crumbs": [
      "Intro to R",
      "Introduction to R"
    ]
  },
  {
    "objectID": "course/1_introR.html#for",
    "href": "course/1_introR.html#for",
    "title": "Introduction to R",
    "section": "for",
    "text": "for\nA for loop is a control flow statement used in many programming languages to repeat a block of code multiple times. It is particularly useful for iterating over sequences (like lists, arrays, or strings) and executing a piece of code for each element in the sequence.\n# Example of a for loop\nfor (i in 1:5) {\n  print(i)\n}\nConsiderations\n\nEnsure the loop has a condition that eventually becomes false to prevent infinite loops.\nBe careful with the loop’s scope and variables to avoid unintended side effects.",
    "crumbs": [
      "Intro to R",
      "Introduction to R"
    ]
  },
  {
    "objectID": "course/1_introR.html#apply",
    "href": "course/1_introR.html#apply",
    "title": "Introduction to R",
    "section": "apply",
    "text": "apply\nThe apply() function in R is a powerful tool for applying a function to the rows or columns of a matrix or data frame. It is particularly useful for performing operations across a dataset without needing to write explicit loops. The syntax for apply() is:\napply(X, margin, function, ...)\n\n# X: This is the array or matrix on which you want to apply the function.\n# margin: A value that specifies whether to apply the function over rows (1), columns (2), or both (c(1, 2)).\n# function: The function you want to apply to each row or column.\nTo calculate the sum of each row in a matrix:\n# Create a matrix\nmy_matrix &lt;- matrix(1:9, nrow=3)\n\n# Apply sum function across rows\nrow_sums &lt;- apply(my_matrix, 1, sum)\nprint(row_sums)\nTo find the mean of each column in a data frame:\n# Create a data frame\ndf &lt;- data.frame(a = c(1, 2, 3), b = c(4, 5, 6))\n\n# Apply mean function across columns\ncolumn_means &lt;- apply(df, 2, mean)\nprint(column_means)\n\nsapply and lappy\n\n\n\nlapply() returns a list, regardless of the output of each application of the function.\n\nsapply() attempts to simplify the result into a vector or matrix if possible. If simplification is not possible, it returns a list similar to lapply().\n\nSuppose you have a list of numerical vectors and you want to compute the sum of each vector. Here’s how you could use lapply():\n# Define a list of vectors\nnum_list &lt;- list(c(1, 2, 3), c(4, 5), c(6, 7, 8, 9))\n\n# Use lapply to apply the sum function\nlist_sums &lt;- lapply(num_list, sum)\nprint(list_sums)\nUsing the same list of numerical vectors, if you use sapply() to compute the sum, the function will try to simplify the output into a vector:\n# Use sapply to apply the sum function\nvector_sums &lt;- sapply(num_list, sum)\nprint(vector_sums)\nWhen to Use Each\n\n\nlapply(): When you need the robustness of a list output, especially when dealing with heterogeneous data or when the function can return variable lengths or types.\n\nsapply(): When you are working with homogeneous data and prefer a simplified output such as a vector or matrix, assuming the lengths and types are consistent across elements.",
    "crumbs": [
      "Intro to R",
      "Introduction to R"
    ]
  },
  {
    "objectID": "course/1_introR.html#finding-patterns",
    "href": "course/1_introR.html#finding-patterns",
    "title": "Introduction to R",
    "section": "Finding Patterns",
    "text": "Finding Patterns\nFinding specific sequences or motifs within biological sequences is a common task.\nlibrary(stringr)\nsequence &lt;- \"ATGCGTACGTTGACA\"\nmotif &lt;- \"CGT\"\nstr_locate(sequence, motif)",
    "crumbs": [
      "Intro to R",
      "Introduction to R"
    ]
  },
  {
    "objectID": "course/1_introR.html#replacing-substrings",
    "href": "course/1_introR.html#replacing-substrings",
    "title": "Introduction to R",
    "section": "Replacing Substrings",
    "text": "Replacing Substrings\nModifying sequences by replacing specific nucleotides or amino acids.\ndna_sequence &lt;- \"ATGCGTACGTTGACT\"\nrna_sequence &lt;- str_replace_all(dna_sequence, \"T\", \"U\")\nprint(rna_sequence)",
    "crumbs": [
      "Intro to R",
      "Introduction to R"
    ]
  },
  {
    "objectID": "course/1_introR.html#substring-extraction",
    "href": "course/1_introR.html#substring-extraction",
    "title": "Introduction to R",
    "section": "Substring Extraction",
    "text": "Substring Extraction\nExtracting parts of sequences, such as cutting out genes or regions of interest.\nextracted_sequence &lt;- str_sub(sequence, 3, 8)\nprint(extracted_sequence)",
    "crumbs": [
      "Intro to R",
      "Introduction to R"
    ]
  },
  {
    "objectID": "course/1_introR.html#length-calculation",
    "href": "course/1_introR.html#length-calculation",
    "title": "Introduction to R",
    "section": "Length Calculation",
    "text": "Length Calculation\nDetermining the length of sequences.\nsequence_length &lt;- str_length(sequence)\nprint(sequence_length)",
    "crumbs": [
      "Intro to R",
      "Introduction to R"
    ]
  },
  {
    "objectID": "course/1_introR.html#case-conversion",
    "href": "course/1_introR.html#case-conversion",
    "title": "Introduction to R",
    "section": "Case Conversion",
    "text": "Case Conversion\nConverting uppercase to lowercase, or vice versa.\nsequence_upper &lt;- str_to_upper(sequence)\nprint(sequence_upper)",
    "crumbs": [
      "Intro to R",
      "Introduction to R"
    ]
  },
  {
    "objectID": "course/1_introR.html#splitting-strings",
    "href": "course/1_introR.html#splitting-strings",
    "title": "Introduction to R",
    "section": "Splitting Strings",
    "text": "Splitting Strings\nSplitting sequences into arrays, useful for reading fasta files or analyzing codons.\ncodons &lt;- str_sub(sequence, seq(1, str_length(sequence), by = 3), seq(3, str_length(sequence), by = 3))\nprint(codons)\nBonus Challenge: What if our sequence length wasn’t a multiple of three? For an example of how to approach this situation please check out the Bonus challenge.",
    "crumbs": [
      "Intro to R",
      "Introduction to R"
    ]
  },
  {
    "objectID": "course/1_introR.html#counting-specific-characters",
    "href": "course/1_introR.html#counting-specific-characters",
    "title": "Introduction to R",
    "section": "Counting Specific Characters",
    "text": "Counting Specific Characters\nCounting occurrences of specific nucleotides or amino acids.\nguanine_count &lt;- str_count(sequence, \"G\")\nprint(guanine_count)",
    "crumbs": [
      "Intro to R",
      "Introduction to R"
    ]
  },
  {
    "objectID": "course/1_introR.html#tidyverse",
    "href": "course/1_introR.html#tidyverse",
    "title": "Introduction to R",
    "section": "Tidyverse",
    "text": "Tidyverse\nTidyverse is a collection of R packages designed for data science. It includes packages like ggplot2 for data visualization and dplyr for data manipulation.\nTidyverse Data Frames\nTidyverse is a collection of R packages designed for data science, developed with a focus on simplicity, consistency, and ease of use. One of the key components of Tidyverse is the concept of tidy data frames.\nA tidyverse data frame is a rectangular table-like structure where:\n\nEach row represents an observation.\nEach column represents a variable.\nEach cell holds a single value.\nPrinciples of Tidy Data:\n\nEach variable forms a column: In a tidy data frame, each variable is placed in its own column. This makes it easy to work with the dataset as each variable is explicitly represented.\nEach observation forms a row: Each row corresponds to a distinct observation, entity, or case. This makes it straightforward to perform operations on individual observations.\nEach type of observational unit forms a table: Different types of data should be stored in separate tables, with relationships between tables represented using unique identifiers.\nWhy Tidy Data Frames are Important:\n\nEase of Analysis: Tidy data frames make it easier to perform data manipulation, visualization, and analysis using Tidyverse packages such as dplyr, ggplot2, and tidyr.\nReadability and Interpretability: Tidy data frames have a consistent structure, making it easier for others to understand and interpret your data.\nEfficiency: Tidy data frames facilitate efficient and concise code, reducing the complexity of data manipulation tasks.\nTidyverse Packages for Data Manipulation:\n\ndplyr: Provides a grammar of data manipulation for data frames, enabling easy filtering, selecting, mutating, summarizing, and arranging of data.\ntidyr: Offers tools for reshaping and tidying messy datasets, such as gather() and spread() functions for converting between wide and long formats.\nggplot2: Allows for the creation of sophisticated data visualizations using a layered grammar of graphics.\n\nlibrary(ggplot2)\nlibrary(tidyr)\nlibrary(dplyr)\n\n# Example dataset\ndata &lt;- data.frame(\n  ID = 1:3,\n  Name = c(\"Alice\", \"Bob\", \"Charlie\"),\n  Math = c(90, 85, 95),\n  Science = c(88, 92, 89)\n)\n\n# Original dataset\nprint(\"Original dataset:\")\nprint(data)\n\n# Tidy the dataset using gather() function from tidyr\ntidy_data &lt;- gather(data, Subject, Score, -ID, -Name)\n\n# Tidied dataset\nprint(\"Tidied dataset:\")\nprint(tidy_data)",
    "crumbs": [
      "Intro to R",
      "Introduction to R"
    ]
  },
  {
    "objectID": "course/1_introR.html#selecting-columns",
    "href": "course/1_introR.html#selecting-columns",
    "title": "Introduction to R",
    "section": "Selecting Columns",
    "text": "Selecting Columns\nSelecting columns allows you to choose specific columns from your dataset. It helps you focus on the variables of interest and ignore the rest.\nselected_data &lt;- select(data, ID, Math)\nprint(selected_data)",
    "crumbs": [
      "Intro to R",
      "Introduction to R"
    ]
  },
  {
    "objectID": "course/1_introR.html#filtering-rows",
    "href": "course/1_introR.html#filtering-rows",
    "title": "Introduction to R",
    "section": "Filtering Rows",
    "text": "Filtering Rows\nFiltering rows allows you to subset your dataset based on specific conditions. It helps you extract only the rows that meet certain criteria.\n# Filtering rows based on conditions\nfiltered_data &lt;- filter(data, Math &gt; 90)\nprint(filtered_data)",
    "crumbs": [
      "Intro to R",
      "Introduction to R"
    ]
  },
  {
    "objectID": "course/1_introR.html#summarizing-data",
    "href": "course/1_introR.html#summarizing-data",
    "title": "Introduction to R",
    "section": "Summarizing Data",
    "text": "Summarizing Data\nSummarizing data involves calculating summary statistics or aggregating data to get a concise overview of your dataset. It helps you understand the overall characteristics of your data.\nsummary_data &lt;- summarize(data, \n                          Mean_Math = mean(Math), \n                          Mean_Science = mean(Science))\nprint(summary_data)",
    "crumbs": [
      "Intro to R",
      "Introduction to R"
    ]
  },
  {
    "objectID": "course/1_introR.html#sorting-arranging",
    "href": "course/1_introR.html#sorting-arranging",
    "title": "Introduction to R",
    "section": "Sorting (Arranging)",
    "text": "Sorting (Arranging)\nArranging rows involves sorting your dataset based on the values of one or more columns. It helps you reorder your data for better visualization or analysis.\narranged_data &lt;- arrange(data, desc(Math))\nprint(arranged_data)",
    "crumbs": [
      "Intro to R",
      "Introduction to R"
    ]
  },
  {
    "objectID": "course/1_introR.html#mutate",
    "href": "course/1_introR.html#mutate",
    "title": "Introduction to R",
    "section": "Mutate",
    "text": "Mutate\nThe mutate() function in the dplyr package is essential for transforming data frames in R. It allows you to add new columns to a data frame or modify existing ones, using existing data. mutate() is part of the tidyverse, a collection of R packages designed for data science that makes data manipulation, exploration, and visualization easy and intuitive.\nAdding columns\nCalculating the GC content of DNA sequences.\nlibrary(dplyr)\nlibrary(stringr)\n\n# Sample data\nsequences &lt;- tibble(\n  id = c(\"seq1\", \"seq2\", \"seq3\"),\n  dna = c(\"ATGCGC\", \"GCGTACGT\", \"ATATATAT\")\n)\n\n# Calculate GC content\nsequences &lt;- sequences %&gt;%\n  mutate(gc_content = (str_count(dna, \"[GC]\") / str_length(dna)) * 100)\n\nprint(sequences)\nReplacing existing columns\nTranscription of DNA sequences into RNA sequences involves replacing thymine (T) with uracil (U).\n# Convert DNA to RNA\nsequences &lt;- sequences %&gt;%\n  mutate(rna = str_replace_all(dna, \"T\", \"U\"))\n\nprint(sequences)\nMultiple Transformations\nIdentifying potential neoantigens by finding motifs associated with high mutation frequencies or specific mutation patterns.\n# Sample DNA sequences\nsequences &lt;- tibble(\n  id = c(\"seq1\", \"seq2\", \"seq3\"),\n  dna = c(\"ATGCGCATC\", \"GCGTACGTAGT\", \"ATATATATAT\")\n)\n\n# Assume a simple motif that might indicate a neoantigen\nmotif = \"ATG\"\n\n# Annotate sequences with potential neoantigen presence\nsequences &lt;- sequences %&gt;%\n  mutate(\n    start_position = str_locate(dna, motif)[,1],\n    is_neoantigen_candidate = ifelse(start_position &gt; 0 & str_count(dna, \"[GC]\") / str_length(dna) &gt; 0.5, \"Yes\", \"No\")\n  )\n\nprint(sequences)",
    "crumbs": [
      "Intro to R",
      "Introduction to R"
    ]
  },
  {
    "objectID": "course/1_introR.html#definition",
    "href": "course/1_introR.html#definition",
    "title": "Introduction to R",
    "section": "Definition",
    "text": "Definition\nFunctions (function) in R perform specific tasks. They take input (arguments), perform operations, and return output.\nfunction_name &lt;- function(argument1, argument2, ...) {\n  # Function body\n  # Perform operations\n  # Return a value (optional)\n}\n\nfunction_name: Name of the function.\nargument1, argument2, …: Arguments passed to the function (optional).\nFunction body: Code block where you define what the function should do.\nReturn a value (optional): Use the return() statement to specify what the function should return (optional).\n\nHere, we define a function and call it!\n# Define a function to calculate the square of a number\nsquare &lt;- function(x) {\n  return(x^2)\n}\n\n# Call the function\nresult &lt;- square(5)\nprint(result)  # Output: 25",
    "crumbs": [
      "Intro to R",
      "Introduction to R"
    ]
  },
  {
    "objectID": "course/1_introR.html#example",
    "href": "course/1_introR.html#example",
    "title": "Introduction to R",
    "section": "Example",
    "text": "Example\nLet’s bring together concepts by writing the function analyze_tcr_data.\nThe function will:\n\nFilter T cell sequences based on a specified length threshold.\nSort the remaining data by clonality in descending order to identify the most prevalent TCRs.\nCreate a new column that indicates the presence of a specific motif within the TCR sequence, a common task in sequence analysis.\n\nlibrary(dplyr)\nlibrary(stringr)\n\n# Define the function\nanalyze_tcr_data &lt;- function(tcr_tibble, length_threshold) {\n  # Validate input\n  if (!inherits(tcr_tibble, \"tbl_df\")) {\n    stop(\"Input must be a tibble.\")\n  }\n  \n  # Filter TCR sequences longer than the threshold and sort by clonality\n  filtered_and_sorted &lt;- tcr_tibble %&gt;%\n    filter(str_length(tcr_sequence) &gt; length_threshold) %&gt;%\n    arrange(desc(clonality))\n  \n  # Add a column to indicate the presence of a specific motif (e.g., 'CASS')\n  enhanced_tcr_tibble &lt;- filtered_and_sorted %&gt;%\n    mutate(has_motif = if_else(str_detect(tcr_sequence, \"CASS\"), \"Yes\", \"No\"))\n  \n  # Return the transformed tibble\n  return(enhanced_tcr_tibble)\n}\n\n# Example usage\n# Assuming a tibble with TCR sequences and clonality metrics\ntcr_data &lt;- tibble(\n  tcr_sequence = c(\"CASSLGGTDTQYF\", \"CASSLGDETQYF\", \"CASSLG\", \"CASSEGTDTQYF\"),\n  clonality = c(0.25, 0.15, 0.05, 0.55)\n)\n\n# Apply the function with a length threshold of 10\nresult_data &lt;- analyze_tcr_data(tcr_data, 10)\nprint(result_data)\nExplanation\n\nValidation: The function starts by checking if the provided data is a tibble to ensure type safety.\nFiltering: Uses filter() to retain only TCR sequences longer than the specified length_threshold.\nSorting: Uses arrange() to sort the data by clonality in descending order.\nString Manipulation: Adding has_motif Column: Uses mutate() combined with str_detect() from the stringr package to add a column that indicates whether each TCR sequence contains the motif “CASS”.\nReturn Value: Outputs a tibble that’s been filtered, sorted, and enhanced with additional string-based analysis.",
    "crumbs": [
      "Intro to R",
      "Introduction to R"
    ]
  },
  {
    "objectID": "course/1_introR.html#bonus-challenge",
    "href": "course/1_introR.html#bonus-challenge",
    "title": "Introduction to R",
    "section": "Bonus challenge",
    "text": "Bonus challenge\nIf our sequence isn’t length three, this code will throw an error because the function expects a sequence that is a multiple of 3:\nlibrary(stringr)\nsequence &lt;- \"ATGCGTACGTTGAC\" # Length 14\n# sequence &lt;- \"ATGCGTACGTTGACA\" # Length 15\n\ncodons &lt;- str_sub(sequence, seq(1, str_length(sequence), by = 3), seq(3, str_length(sequence), by = 3))\n\n#&gt; Error in `str_sub()`:\n#&gt; ! Can't recycle `string` (size 5) to match `end` (size 4).\n#&gt; Run `rlang::last_trace()` to see where the error occurred.\nCan you try to code up a solution to this problem?\n\nHINT# We can use the modulus operator \"%%\". It yields the remainder when the first operand is divided by the second.\n\n15 %% 3 \n#&gt; [1] 0\n\n14 %% 3\n#&gt; [1] 2\n\n\nHere are a few options for how to solve this (only peek after trying yourself).\n\nShow me the solutionsequence &lt;- \"ATGCGTACGTTGAC\" # Length 14\n\n# sequence &lt;- \"ATGCGTACGTTGACA\" # Length 15\n\n# Option 1: Check for length 3 and make sure before running the code, otherwise print a warning.\n\nsequence_length &lt;- nchar(sequence) # Get sequence length\n\nif (sequence_length %% 3 != 0) {\n  print(paste(\"Error! The sequence is length:\", sequence_length))\n  codons &lt;- \"ERROR\"\n} else {\n  codons &lt;- str_sub(string = sequence, \n  start = seq(1, str_length(sequence), by = 3), \n  end = seq(3, str_length(sequence), by = 3))\n}\n\nprint(codons)\n\n# Option 2: Add N to the sequence to make in a multiple of 3, and then any seqeuence will run properly.\n\nif (sequence_length %% 3 != 0) {\n  leftover &lt;- 3 - nchar(sequence) %% 3\n  \n  leftover_n &lt;- paste(rep(\"N\", leftover), collapse = \"\")\n  \n  sequence &lt;- paste0(sequence, leftover_n)\n}\n\ncodons &lt;- str_sub(sequence, seq(1, str_length(sequence), by = 3), seq(3, str_length(sequence), by = 3))\n\nprint(codons)",
    "crumbs": [
      "Intro to R",
      "Introduction to R"
    ]
  },
  {
    "objectID": "course/2024/spatial_demo.html",
    "href": "course/2024/spatial_demo.html",
    "title": "Intro to Spatial Analysis",
    "section": "",
    "text": "In this notebook, we will be going through some basic spatial analyses. The goal is to provide you with an intuition of the logic behind each of these functions. Try not to get bogged down in understanding every single line of the code, but focus more on the overall reasoning behind what is being done. The code in this notebook is not optimized for large datasets, but simplified versions of more complex functions, to make it more clear what is actually being done.\nIf you would like to run similar analyses on your own multiplexed imaging datasets, please see our lab’s poipeline here: https://github.com/angelolab/ark-analysis. There are also other toolkits for spatial analysis, including Squidpy and MCMICRO.\nimport os\nimport numpy as np\nimport pandas as pd\nimport skimage.io as io\nfrom skimage.segmentation import find_boundaries\nimport matplotlib.pyplot as plt\nimport matplotlib.cm as cm\nfrom matplotlib import colors\nfrom mpl_toolkits.axes_grid1 import make_axes_locatable\nfrom scipy.spatial.distance import cdist\nfrom scipy import stats\nimport random\nfrom sklearn.cluster import KMeans\nimport seaborn as sns"
  },
  {
    "objectID": "course/2024/spatial_demo.html#inspect-your-data",
    "href": "course/2024/spatial_demo.html#inspect-your-data",
    "title": "Intro to Spatial Analysis",
    "section": "1. Inspect your data",
    "text": "1. Inspect your data\nMantis Viewer or napari can be useful for visualizing your data, but it’s always a good idea to be able to open, view, and manipulate your data in Python, as it will give you more flexibility when analyzing your data.\n\n%matplotlib notebook\n\n# Directory where the data lives\ndata_dir = \"example_data\"\n\n# Example image\nex_fov = \"fov1\"\n\n# Look at a few markers\nmarkers = [\"CD45\",\"Collagen1\"]\n\n# Set-up plots\nplt.rcParams['figure.figsize'] = [10, 5]\nfig, ax = plt.subplots(1,len(markers), sharex=True, sharey=True)\nfor i,mark in enumerate(markers):\n    im_array = np.array(io.imread(os.path.join(data_dir, ex_fov, \"image_data\", mark+\".tiff\")))\n    ax[i].imshow(im_array, origin=\"lower\", cmap='gray', vmax=np.quantile(im_array,0.99))\n    ax[i].set_title(mark)\n    ax[i].axis('off')\nplt.tight_layout()\n\nWe have already segmented these images to identify the location of single cells in the image using Mesmer. If you are interested in applying Mesmer to your own data, you can see the notebook here.\nWe can inspect the output of Mesmer here. In the segmentation output, each cell has a unique label. For example, all pixels with the value of 1 belong to the same cell, all pixels with the value of 2 belong to another cell, etc.\n\n# Read in segmentation mask\nseg_path = os.path.join(data_dir, ex_fov, \"masks\", \"segmentation_whole_cell.tiff\")\nseg_array = np.array(io.imread(seg_path)).squeeze() #squeeze changes dimensions from (1,2048,2048) to (2048,2048)\n\n# Set up plot\nfig, ax = plt.subplots(figsize=[5,5])\nplt.imshow(seg_array)\nplt.axis('off')\nplt.tight_layout()\n\nOnce we have segmented our data, we can generate cell tables where each row corresponds to one cell that was identified in the image. We have already identified the phenotype of each cell using Pixie. If you are interested in running Pixie on your own data, see the notebook here.\nThe ‘label’ column in the table corresponds to the cell IDs (pixel values in the segmentation output), ‘centroid-0’ and ‘centroid-1’ correspond to the center point of each cell, and the ‘cell_cluster’ column is the cell phenotype that we determined using Pixie.\n\n# Read in cell table\ncell_table_path = os.path.join(data_dir, \"cell_table.csv\")\ncell_table = pd.read_csv(cell_table_path)\n\n# Subset for only the example fov we're looking at here\nfov_cell_table = cell_table.loc[cell_table['fov'] == ex_fov]\nfov_cell_table\n\nWe can then create a cell phenotype map, where each cell is colored according to its cell phenotype.\n\n# Define colors we want for each cell type\nall_colors = {}\nall_colors['APC'] = '#4E79A7'\nall_colors['B'] = '#A0CBE8'\nall_colors['Cancer'] = '#F28E2B'\nall_colors['Cancer_EMT'] = '#FFBE7D'\nall_colors['Cancer_Other'] = '#59A14F'\nall_colors['CD4T'] = '#8CD17D'\nall_colors['CD8T'] = '#B6992D'\nall_colors['Endothelium'] = '#F1CE63'\nall_colors['Fibroblast'] = '#499894'\nall_colors['Immune_Other'] = '#86BCB6'\nall_colors['M1_Mac'] = '#E15759'\nall_colors['M2_Mac'] = '#FF9D9A'\nall_colors['Mac_Other'] = '#79706E'\nall_colors['Mast'] = '#D4A6C8'\nall_colors['Monocyte'] = '#D37295'\nall_colors['Neutrophil'] = '#FABFD2'\nall_colors['NK'] = '#B07AA1'\nall_colors['Other'] = '#BAB0AC'\nall_colors['Stroma'] = '#9D7660'\nall_colors['T_Other'] = '#D7B5A6'\nall_colors['Treg'] = '#FFFF99'\n\n# Create table matching each color to a unique ID\ncolors_list = [(key,value) for key,value in all_colors.items()]\nall_colors_df = pd.DataFrame(colors_list, columns=['cell_cluster','color'])\nall_colors_df['pheno_id'] = all_colors_df.index + 1\n\n# Make color map for plotting\nmycols = all_colors_df['color'].tolist()\nmycols.insert(0,'#000000') # add black for empty slide, will have id 0\nmycols.append('#FFFFFF') # add white for cell borders, will have id max_n+1\ncolmap = colors.ListedColormap(mycols)\nmax_n = np.max(all_colors_df['pheno_id'])\nbounds = [i-0.5 for i in np.linspace(0,max_n+2, max_n+3)]\nnorm = colors.BoundaryNorm(bounds, colmap.N)\n\n# Define function for making cell phenotype map (CPM)\ndef create_cpm(fov_name, cell_table, all_colors_df, seg_array, fig, ax):\n    # Subset cell table for this FOV\n    one_fov_cell_table = cell_table.loc[cell_table['fov'] == fov_name]\n    # Combine with cell table\n    one_fov_cell_table = one_fov_cell_table.merge(all_colors_df, how='left')\n    # Make dictionary mapping each cell to its phenotype id\n    fov_cell_dict = dict(zip(one_fov_cell_table['label'], one_fov_cell_table['pheno_id']))\n    # Add 0 for empty slide\n    fov_cell_dict[0] = 0\n    \n    # Make new image where each pxiel corresponds to its phenotype id\n    # Use 'vectorize' in numpy package to speed up this operation\n    cpm_array = np.vectorize(fov_cell_dict.get)(seg_array)\n\n    # Find the borders of cells\n    predicted_contour_mask = find_boundaries(seg_array, connectivity=1, mode='inner').astype(np.uint8)\n    # Color this  border white\n    cpm_array[predicted_contour_mask &gt; 0] = max_n+1\n\n    # Plot\n    cpm_image = colmap(norm(cpm_array))\n    ax.imshow(cpm_image)\n    \n    return\n\n\n# Create CPM for example FOV\nfig, ax = plt.subplots(figsize=[8,8])\ncpm = create_cpm(ex_fov, cell_table, all_colors_df, seg_array, fig, ax)\nplt.axis('off')\n\n# Add colorbar to image\ndivider = make_axes_locatable(fig.gca())\ncax = divider.append_axes(position=\"right\", size=\"5%\", pad=\"3%\")\ncbar = fig.colorbar(cm.ScalarMappable(norm=norm, cmap=colmap),\n                    cax=cax, orientation=\"vertical\", use_gridspec=True, pad=0.1,\n                    shrink=0.9, drawedges=True)\ncbar_labels = all_colors_df['cell_cluster'].to_list()\ncbar_labels.insert(0,'Empty') # add black for empty slide, will have id 0\ncbar_labels.append('Cell border') # add white for cell borders, will have id max_n+1\n\ncbar.ax.set_yticks(\n    ticks=np.arange(len(cbar_labels)),\n    labels=cbar_labels\n)\ncbar.minorticks_off()\n\nplt.tight_layout()"
  },
  {
    "objectID": "course/2024/spatial_demo.html#quanitfying-cell-populations",
    "href": "course/2024/spatial_demo.html#quanitfying-cell-populations",
    "title": "Intro to Spatial Analysis",
    "section": "2. Quanitfying cell populations",
    "text": "2. Quanitfying cell populations\nThere are many methods for cell enumeration, including counting the number of cells of each cell type, calculating cell frequency by dividing by the total number of cells, and calculating cell density by dividing by the total tissue area. While the first approach is the most straightforward, simply counting the number of cells depends on the amount of tissue present in the image. Cell frequencies normalize the number of each cell type to the total number of cells in the image such that all cell types sum to 1. As a result, cell frequencies are not confounded by differences in the size of the section. However, one drawback of cell frequencies is that they can obscure the reason that the amount of one cell type differs between samples. For example, consider a simple example of comparing the number of tissue resident macrophages in healthy and inflamed tissue. Even if the number of macrophages is the same in both images, it may seem like tissue resident macrophages are decreasing in the latter relative to the former. In reality, however, the absolute count of tissue resident macrophages is not changing; they are just outnumbered by infiltrating immune cells. Only considering cell frequencies in such a scenario would lead to the incorrect conclusion that macrophages are decreasing in the inflamed state. One solution is to also examine the density of each cell population by dividing by the total tissue area.\nHere, we are showing an example of two FOVs that have different amounts of immune infiltrate, ECM content, and total area (as determined using the slide background mask). We have generated masks of the ECM (using the composite signal of ECM markers) and empty sllide (for MIBI, we can determine empty slide by measuring gold signal, since we use gold sputtered slides). Using the empty slide mask, we can estimate tissue area (by taking the inverse). In the ECM and empty slide masks shown here, white indicates ECM or empty slide, respectively.\n\n# Example images\nfov1 = \"fov1\"\nfov2 = \"fov2\"\n\n# Set-up plots\nplt.rcParams['figure.figsize'] = [10,6]\nfig, ax = plt.subplots(2,3)\n\n# Look at cell phenotype maps, ECM mask, and gold mask\nfor i,fov in enumerate([fov1,fov2]):\n    # CPMs\n    seg_path = os.path.join(data_dir,fov,\"masks\",\"segmentation_whole_cell.tiff\")\n    seg_array = np.array(io.imread(seg_path)).squeeze()\n    create_cpm(fov, cell_table, all_colors_df, seg_array, fig, ax[i,0])\n    ax[i,0].set_title(\"Cell phenotype map\")\n    ax[i,0].set_ylabel(fov)\n    ax[i,0].set_yticklabels([])\n    ax[i,0].set_xticklabels([])\n    ax[i,0].set_yticklabels([])\n    ax[i,0].set_xticks([])\n    ax[i,0].set_yticks([])\n    \n    # ECM\n    ecm_path = os.path.join(data_dir,fov,\"masks\",\"total_ecm.tiff\")\n    ecm_array = np.array(io.imread(ecm_path))\n    ax[i,1].imshow(ecm_array, cmap='gray')\n    ax[i,1].axis('off')\n    ax[i,1].set_title(\"ECM\")\n    \n    # Empty slide\n    empty_slide_path = os.path.join(data_dir,fov,\"masks\",\"empty_slide.tiff\")\n    empty_slide_array = np.array(io.imread(empty_slide_path))\n    ax[i,2].imshow(empty_slide_array, cmap='gray')\n    ax[i,2].axis('off')\n    ax[i,2].set_title(\"Empty slide\")\n\nplt.tight_layout()\n\n\nSimply count the number of each cell type\n\ncell_table_keep = cell_table.loc[cell_table['fov'].isin([fov1,fov2])]\ncount_cells = cell_table_keep.groupby('fov')['cell_cluster'].value_counts().reset_index(name='count')\ncount_cells\n\n\n\nCalculate frequency (divided by total number of cells)\n\n# Get total number of cells per FOV\ntotal_counts = cell_table_keep.groupby('fov').size().to_frame('total_cells')\ntotal_counts = total_counts.reset_index()\n\n# Calculate frequency\ncount_cells = count_cells.merge(total_counts, on='fov')\ncount_cells['frequency'] = count_cells['count'] / count_cells['total_cells']\ncount_cells\n\n\n\nCalculate density (divided by tissue area)\n\n# Calculate tissue area (inverse of empty slide)\ndef get_tissue_area(fov_name):\n    empty_slide_path = os.path.join(data_dir,fov_name,\"masks\",\"empty_slide.tiff\")\n    empty_slide_array = np.array(io.imread(empty_slide_path))\n    # Get total pixels that belong to empty slide\n    empty_slide_px = np.sum(empty_slide_array) #pixel value is 1 if it is empty slide\n    # Get number of pixels that belong to tissue (total image - empty slide)\n    total_size = empty_slide_array.shape\n    tissue_area_px = (total_size[0]*total_size[1]) - empty_slide_px\n    # These images are 2048px x 2048px, imaged at 800um x 800um\n    # Therefore, the size of 1 pixel is 800um/2048px = 0.39 um/px, area is 0.15 um^2\n    return (fov_name, tissue_area_px*0.15)\n\n# Calculate for all FOVs\nall_tissue_px = [get_tissue_area(fov) for fov in [fov1,fov2]]\nall_tissue_df = pd.DataFrame(all_tissue_px, columns=['fov','area'])\n\n# Calculate density\ncount_cells = count_cells.merge(all_tissue_df, on='fov')\ncount_cells['density'] = count_cells['count'] / count_cells['area']\ncount_cells\n\n\n\nCompare\n\ncell_type = 'CD4T'\nprint(cell_type)\nprint('FOV:', fov1,\n      ', total_counts:', count_cells.loc[(count_cells['fov']==fov1) & (count_cells['cell_cluster']==cell_type)]['count'].values[0],\n      ', frequency:', count_cells.loc[(count_cells['fov']==fov1) & (count_cells['cell_cluster']==cell_type)]['frequency'].values[0],\n      ', density:', count_cells.loc[(count_cells['fov']==fov1) & (count_cells['cell_cluster']==cell_type)]['density'].values[0])\nprint('FOV:', fov2,\n      ', total_counts:', count_cells.loc[(count_cells['fov']==fov2) & (count_cells['cell_cluster']==cell_type)]['count'].values[0],\n      ', frequency:', count_cells.loc[(count_cells['fov']==fov2) & (count_cells['cell_cluster']==cell_type)]['frequency'].values[0],\n      ', density:', count_cells.loc[(count_cells['fov']==fov2) & (count_cells['cell_cluster']==cell_type)]['density'].values[0])\n\n\n\nAdditional exercises\n\nChange the “cell_type” in the code block above and evaluate how the quantification method is different for different cell types.\nCreate a stacked bar plot showing the cell type composition of each fov (x axis should be fov1 and fov2, y axis is cell type composition). Try making this plot with counts, frequency, and density."
  },
  {
    "objectID": "course/2024/spatial_demo.html#cell-cell-enrichment-global",
    "href": "course/2024/spatial_demo.html#cell-cell-enrichment-global",
    "title": "Intro to Spatial Analysis",
    "section": "3. Cell-cell enrichment (global)",
    "text": "3. Cell-cell enrichment (global)\nThe goal of pairwise enrichment analysis is to assess whether any two given cell populations colocalize with each other. For example, this approach could be used to determine if T cells preferentially colocalize with tumor cells. In asking such questions, we recommend taking this one step further: Do two cell populations colocalize with each other more often than would be expected by chance? Depending on the frequency of the cell populations and native tissue structure, it may not be possible to determine whether some pairwise relationships are truly preferential or random. The goal of this approach is to minimize potential confounding effects specific to each image that might not be related to the biological question of interest. For example, in a tissue section equally composed of only two cell populations, pairwise enrichment is likely to occur simply by random chance.\nWith this in mind, we can assess the statistical significance of pairwise enrichment by comparing how often two cell populations are found in close proximity to each other compared with a null distribution. To construct a null distribution, we use a bootstrapping approach by selectively randomizing the location of the cell populations of interest, calculating pairwise enrichment, and repeating this process a large number of times (&gt;100 times is typical). In the assessment of preferential enrichment for two cell populations defined by their expression of marker A or B, the simplest way to generate a null distribution is to randomize the location of one cell population across all cells in the image while keeping the location of the other cell population fixed. For both the original and randomized images, cells positive for A or B are colocalized if the distance between them is less than or equal to a user-defined value. The number of A–B interactions in the original image is then compared with the null distribution to determine statistical significance.\n\n# Example image\nex_fov = \"fov2\"\n\n# Determine cell types to look at\npheno1 = \"Cancer\"\npheno2 = \"CD8T\"\n\n# Threshold to determine if two cells are \"close\"\ndist_thresh = 50\n\n# Number of bootstraps for generating null distribution\nbootstrap_n = 100\n\n\nCalculate the distance between all cells in the FOV\n\n# Subset cell table for only cells in this FOV\nfov_cell_table = cell_table.loc[cell_table['fov'] == ex_fov].reset_index(drop=True)\n# Make list of all cell centroids\nall_centroids = list(zip(fov_cell_table['centroid-0'],fov_cell_table['centroid-1']))\n# Get distance between all cells\ndist_mat = cdist(all_centroids, all_centroids, 'euclidean')\n# Print dimensions of distance matrix\nprint(\"Dimensions of dist_mat: \", dist_mat.shape)\n\ndist_mat\n\nThis distance matrix has dimensions of total number of cells x total number of cells. The values in the matrix indicate the distance between any 2 cells. The indices of the distance matrix correspond to the indices of the cell table. So the first row in the distance matrix corresponds to the distance between cell label 1 and all other cells in the image. After calculating the distance between all cells, we can subset it for our cell type of choice. We can then count the number of close contacts between two cell types.\n\n\nCount number of close contacts between pheno1 and pheno2\n\n# Get index of cells belonging to pheno1 and pheno2\npheno1_idx = fov_cell_table[fov_cell_table['cell_cluster'] == pheno1].index.to_list()\npheno2_idx = fov_cell_table[fov_cell_table['cell_cluster'] == pheno2].index.to_list()\n\n# Only keep pheno1 cells in x-axis of distance matrix\npheno1_dist_mat = dist_mat[pheno1_idx,:]\n# Binarize the distance matrix for distances that are within the defined threshold\nbin_mask = (pheno1_dist_mat &lt; dist_thresh) & (pheno1_dist_mat &gt; 0)\n# Change true/false to 1/0\npheno1_dist_mat_bin = bin_mask*1\n\n# Subset this distance matrix for pheno2 cells in y-axis of distance matrix\ntrue_dist_mat_bin = pheno1_dist_mat_bin[:,pheno2_idx]\n# Inspect the shape of this matrix, should be number of cells of pheno1 x number of cells of pheno2\n# Each element in the matrix is the distance between a pheno1 cell and a pheno2 cell\nprint(\"Shape of subsetted distance matrix: \", true_dist_mat_bin.shape)\n\n# For each pheno1 cell, count number of \"close\" contacts with pheno2 cells\ntrue_close_contacts = np.sum(true_dist_mat_bin, axis=1)\n# Take the average across all pheno1 cells\ntrue_close_contacts_mean = np.mean(true_close_contacts)\nprint(\"Average number of close contacts between pheno1 and pheno2 cells: \", true_close_contacts_mean)\n\nAbove, we have determined the average number of close contacts between pheno1 and pheno2 cells in this image. To determine whether this is a significant number, we can compare it against a null distribution, generated by randomly permuting cell labels. We keep the pheno1 cells constant, then randomize the location of pheno2 cells in the image. For each randomization, we calculate the number of close contacts. We repeat this many times to generate the null distribution.\n\n\nGenerate null distribution by bootstrapping\n\n# Get all possible cell indices (total pool of available cells to randomize)\nall_idx = fov_cell_table.index.to_list()\n# Remove cells that are of pheno1 from this pool (since they are held constant in this randomization)\nall_idx = [x for x in all_idx if x not in pheno1_idx]\n# Get total number of cells that are pheno2\nnum_pheno2 = len(pheno2_idx)\n\n# Randomly sample all cells to be labeled as pheno2 (bootstrapping)\nall_bootstrap = []\nfor _ in range(bootstrap_n):\n    # Select num_pheno2 random numbers, represents the indices of the randomly selected cells\n    random_pheno2_idx = random.sample(all_idx, num_pheno2)\n    # Subset the distance matrix to only keep these randomly selected cells\n    keep_dist_mat_bin = pheno1_dist_mat_bin[:,random_pheno2_idx]\n    # Find the total number of close contacts between pheno1 cells and randomly selected cells\n    close_contacts = np.sum(keep_dist_mat_bin, axis=1)\n    # Take the mean across all cells of pheno1\n    close_contacts_mean = np.mean(close_contacts)\n    # Add this value to the list of all bootstraps\n    all_bootstrap.append(close_contacts_mean)\n\n\n\nCompare null distrbution to actual number of close contacts\n\nfig, ax = plt.subplots(figsize=(5,3))\n# Blue histogram is null distribution\nax.hist(all_bootstrap, density=True,  bins=10, alpha=0.5)\n# Red line is actual number of close contacts\nplt.axvline(x=true_close_contacts_mean, color='red', linestyle='--', linewidth=2)\nplt.show()\n\n\n# Calculate statistics of null distribution\nmuhat, sigmahat = stats.norm.fit(all_bootstrap)\n# Calculate z score based on distribution\nz = (true_close_contacts_mean - muhat) / sigmahat\nprint(\"z-score: \", z)\n\n\n\nAdditional exercises\n\nFlip “pheno1” and “pheno2” - is this calculation symmetric? Conceptually, should it be symmetric?\nPlay around with the distance threshold for determing “close” cells.\nPlay around with “pheno1” and “pheno2” and evaluate these metrics for various pairs of cells.\nPick a few different pheno1-pheno2 pairs. For one FOV, loop through this pairwise enrichment function for these pairs of cells. Store this output in a table (hint: column names could be “pheno1”, “pheno2”, “z-score”).\nRepeat this calculation for the second FOV.\nCompare z-scores between fov1 and fov2."
  },
  {
    "objectID": "course/2024/spatial_demo.html#cell-cell-enrichment-context-dependent",
    "href": "course/2024/spatial_demo.html#cell-cell-enrichment-context-dependent",
    "title": "Intro to Spatial Analysis",
    "section": "4. Cell-cell enrichment (context dependent)",
    "text": "4. Cell-cell enrichment (context dependent)\nThis randomization strategy is robust for mitigating confounding effects attributable to the frequency of each cell population. However, this approach does not control for biases inherent to tissue structure. Consequently, interactions between two proteins may appear to be spatially enriched as a result of tissue structure or cell-specific expression, as markers expressed exclusively by certain cells will be heavily influenced by the location of those cells. For example, two immune cell populations that are known to preferentially localize in germinal centers are biased to be enriched with each other. While one possibility is that this relationship is indeed preferential, another possibility is that spatial enrichment is merely a result of both cell types being restricted to a smaller histological compartment.\nTo address this possibility, null distributions can also be generated in a context-dependent manner. In context-dependent spatial enrichment analysis, randomizations are restricted to occur only within a given cellular compartment or cell subset. In the germinal center example, randomizations can be restricted to occur only within the germinal center. As a result, spurious enrichments that might occur between noninteracting cell populations that happen to colocalize to germinal centers are accounted for in the null distribution.\nFor this dataset, we have generated masks for the cancer and stroma, core and border regions (using composite channels representative for each region).\n\nLook at cancer and stroma masks\n\ncancer_core = io.imread(os.path.join(data_dir, ex_fov, \"masks\", \"cancer_core.tiff\"))\ncancer_border = io.imread(os.path.join(data_dir, ex_fov, \"masks\", \"cancer_border.tiff\"))\nstroma_core = io.imread(os.path.join(data_dir, ex_fov, \"masks\", \"stroma_core.tiff\"))\nstroma_border = io.imread(os.path.join(data_dir, ex_fov, \"masks\", \"stroma_border.tiff\"))\n\nfig, ax = plt.subplots(1,4,figsize=[12,4])\nax[0].imshow(cancer_core, cmap='gray')\nax[0].axis('off')\nax[0].set_title(\"Cancer core\")\nax[1].imshow(cancer_border, cmap='gray')\nax[1].axis('off')\nax[1].set_title(\"Cancer border\")\nax[2].imshow(stroma_core, cmap='gray')\nax[2].axis('off')\nax[2].set_title(\"Stroma core\")\nax[3].imshow(stroma_border, cmap='gray')\nax[3].axis('off')\nax[3].set_title(\"Stroma border\")\nplt.tight_layout()\nplt.show()\n\n\n\nRepeat calculation for number of close contacts but subset for cells within the cancer core only\n\n# Get segmentation mask\nseg_path = os.path.join(data_dir, ex_fov, \"masks\", \"segmentation_whole_cell.tiff\")\nseg_array = np.array(io.imread(seg_path)).squeeze()\n\n# Make new segmentation mask where everything outside of the cancer core is 0\nmasked_seg_array = np.copy(seg_array)\nmasked_seg_array[cancer_core == 0] = 0\n# Only keep cell labels that are within the cancer core\nmasked_cell_labels = np.unique(masked_seg_array)\nmasked_cell_labels = [x for x in masked_cell_labels if x != 0] #remove 0 (which is empty slide)\n# Subset cell table for only the cells within the cancer core\nfov_cell_table_masked = fov_cell_table[fov_cell_table['label'].isin(masked_cell_labels)]\n\n\n# Get index of cells belonging to pheno1 and pheno2, but this time, only keep labels that are in the cancer core mask\npheno1_idx = fov_cell_table_masked[fov_cell_table_masked['cell_cluster'] == pheno1].index.to_list()\npheno2_idx = fov_cell_table_masked[fov_cell_table_masked['cell_cluster'] == pheno2].index.to_list()\n\n# Only keep pheno1 cells in x-axis of distance matrix\npheno1_dist_mat = dist_mat[pheno1_idx,:]\n# Binarize the distance matrix for distances that are within the defined threshold\nbin_mask = (pheno1_dist_mat &lt; dist_thresh) & (pheno1_dist_mat &gt; 0)\n# Change true/false to 1/0\npheno1_dist_mat_bin = bin_mask*1\n\n# Subset this distance matrix for pheno2 cells in y-axis of distance matrix\ntrue_dist_mat_bin = pheno1_dist_mat_bin[:,pheno2_idx]\n\n# For each pheno1 cell, count number of \"close\" contacts with pheno2 cells\ntrue_close_contacts = np.sum(true_dist_mat_bin, axis=1)\ntrue_close_contacts_mean = np.mean(true_close_contacts)\nprint(\"Average number of close contacts between pheno1 and pheno2 cells: \", true_close_contacts_mean)\n\n\n\nGenerate null distribution, only randomizing within cancer core\n\n# Get all possible cell indices (total pool of available cells to randomize)\nall_idx = fov_cell_table_masked.index.to_list()\n# Remove cells that are of pheno1 from this pool (since they are held constant in this randomization)\nall_idx = [x for x in all_idx if x not in pheno1_idx]\n# Get total number of cells that are pheno2\nnum_pheno2 = len(pheno2_idx)\n\n# Randomly sample all cells to be labeled as pheno2 (bootstrapping)\nall_bootstrap = []\nfor _ in range(bootstrap_n):\n    # Select num_pheno2 random numbers, represents the indices of the randomly selected cells\n    random_pheno2_idx = random.sample(all_idx, num_pheno2)\n    # Subset the distance matrix to only keep these randomly selected cells\n    keep_dist_mat_bin = pheno1_dist_mat_bin[:,random_pheno2_idx]\n    # Find the total number of close contacts between pheno1 cells and randomly selected cells\n    close_contacts = np.sum(keep_dist_mat_bin, axis=1)\n    # Take the mean across all cells of pheno1\n    close_contacts_mean = np.mean(close_contacts)\n    # Add this value to the list of all bootstraps\n    all_bootstrap.append(close_contacts_mean)\n\n\n\nCompare null distrbution to actual number of close contacts\n\nfig, ax = plt.subplots(figsize=(5,3))\n# Blue histogram is null distribution\nax.hist(all_bootstrap, density=True,  bins=10, alpha=0.5)\n# Red line is actual number of close contacts\nplt.axvline(x=true_close_contacts_mean, color='red', linestyle='--', linewidth=2)\nplt.show()\n\n\n# Calculate statistics of null distribution\nmuhat, sigmahat = stats.norm.fit(all_bootstrap)\n# Calculate z score based on distribution\nz = (true_close_contacts_mean - muhat) / sigmahat\nprint(\"z-score: \", z)\n\n\n\nAdditional exercises\n\nPlay with different regions and phenotype pairs."
  },
  {
    "objectID": "course/2024/spatial_demo.html#cellular-microenvironments",
    "href": "course/2024/spatial_demo.html#cellular-microenvironments",
    "title": "Intro to Spatial Analysis",
    "section": "5. Cellular microenvironments",
    "text": "5. Cellular microenvironments\nCell phenotypes are regulated in part by their local cellular niche, or microenvironment, which is defined as a collection of cell lineages found to be spatially colocalized and conserved across the data set. We can employ several methods to identify these microenvironments. The first step in this process is to quantify the local distribution of cell lineages around each cell in the data set. Here, we are enumerating the number of cells in each lineage within a set pixel radius from the index cell. Various machine learning methods can then be used to identify distinct cellular microenvironments that occur repeatedly across the data set. Here, we are using k-means clustering.\n\n# FOVs to include in this analysis\nall_fovs = [\"fov1\", \"fov2\"]\n\n# Set distance for the neighborhood around the cell\nnh_dist = 50\n\n# Set the number of \"expected\" microenvironments\nk = 3\n\n# Set a seed for reproducibility\nseed = 2024\n\n# Get list of all cell types in our data\nall_cell_types = cell_table['cell_cluster'].unique()\n\n\nFind neighbors for each cell\n\n# Define function to count the neighbors of each cell (where each cell is one row in the distance matrix)\ndef one_cell_nh(row):\n    # Get index of neighbors (defined as \"close\" in the distance matrix)\n    # In the distance matrix ('row' is just one row in the distance matrix), 1 indicates that the two cells were \"close\"\n    neighbor_idx = np.where(row == 1)[0]\n    # Initialize output as a dictionary, start with count of 0 for every cell type\n    nh = {key:0 for key in all_cell_types}\n    # Convert these indices to cell types\n    if len(neighbor_idx) &gt; 0:\n        nh_cells = fov_cell_table.loc[neighbor_idx]['cell_cluster'].values\n        # Count the number of each cell type\n        unique_cell_type, counts = np.unique(nh_cells, return_counts=True)\n        # Store the data in the dictionary\n        for i,one_cell_type in enumerate(unique_cell_type):\n            nh[one_cell_type] = counts[i]\n    return nh\n\n# Iterate through all fovs to get neighbors of all cells\nall_nh_list = [] #store information for each fov in a list\nfor fov in all_fovs:\n    # Subset cell table for only cells in this FOV\n    fov_cell_table = cell_table.loc[cell_table['fov'] == fov].reset_index(drop=True)\n    # Make list of all cell centroids\n    all_centroids = list(zip(fov_cell_table['centroid-0'],fov_cell_table['centroid-1']))\n    # Get distance between all cells\n    dist_mat = cdist(all_centroids, all_centroids, 'euclidean')\n\n    # Binarize distance matrix (1 if two cells are \"close\")\n    dist_mat_bin = dist_mat &lt; nh_dist\n    dist_mat_bin = dist_mat_bin*1\n\n    # Remove itself as its own neighbor\n    dist_mat_bin[dist_mat == 0] = 0\n    \n    # Apply function to every cell in the data\n    nh_dicts = np.apply_along_axis(one_cell_nh, axis=1, arr=dist_mat_bin)\n    # Turn into dataframe\n    nh_df = pd.DataFrame(nh_dicts.tolist())\n    # Get total number of neighbors\n    nh_df['total_neighbors'] = nh_df.sum(axis=1)\n    # Combine with data from cell table\n    nh_df = pd.merge(fov_cell_table, nh_df, left_index=True, right_index=True)\n    all_nh_list.append(nh_df)\n\n# Turn data into dataframe\nall_nh_df = pd.concat(all_nh_list)\nall_nh_df\n\n\n\nk-means clustering\nWhen finding the neighborhood, you can either use the number of cells in the neighborhood of each cell, or the frequency (divided by the total number of cells). The interpretation of each of these is slightly different. We encourage you to try both ways (you can comment/uncomment the line of code that calculates frequency below). By clustering on the neighborhoods, we can identify distinct microenvironments.\n\n# If you want to try clustering using frequency (meaning dividing by the total number of cells, uncomment this line\n# all_nh_df[all_cell_types] = all_nh_df[all_cell_types].divide(all_nh_df['total_neighbors'], axis=0)\n\n# Remove cells that have no neighbors\nall_nh_df_zeros_removed = all_nh_df.loc[all_nh_df['total_neighbors'] &gt; 0].copy()\n\n# Only keep columns we want for clustering\nkmeans_input = all_nh_df_zeros_removed[all_cell_types]\n\n# Cluster\ncluster_fit = KMeans(n_clusters=k, random_state=seed, n_init=10).fit(kmeans_input)\n# Add 1 to labels to avoid cluster number 0 (because output of kmeans is 0-indexed)\ncluster_labels = [\"ME\"+str(x) for x in cluster_fit.labels_+1]\n\n# Add cluster labels to cell table\nall_nh_df_zeros_removed['me_cluster'] = cluster_labels\n\n# Merge with the big cell table, assign unassigned cells (becaue no neighbors) to k+1\nall_nh_df = all_nh_df.merge(all_nh_df_zeros_removed, how=\"left\")\nall_nh_df['me_cluster'] = all_nh_df['me_cluster'].fillna(\"Unassigned\")\nall_nh_df\n\n\n\nVisualize microenvironments\nWe can visualize the number of each cell type that is assigned to each microenvironment.\n\n# Count the number of each cell type assigned to each microenvironment\nnum_cell_types_dat_long = all_nh_df_zeros_removed.groupby(['me_cluster', 'cell_cluster']).size().reset_index(name='counts')\n# Reformat this table\nnum_cell_types_dat = num_cell_types_dat_long.pivot(index='me_cluster', columns='cell_cluster', values='counts')\nnum_cell_types_dat.fillna(0, inplace=True)\n\n# Make heatmap\nheatmap = sns.clustermap(\n    num_cell_types_dat.apply(stats.zscore, axis=1), # Apply a z-score for better visualization\n    cmap = \"vlag\",\n    center = 0,\n    vmin = -3,\n    vmax = 3\n)\n\n\n\nVisualize cell phenotypes maps, colored by microenvironment\n\n# Define colors we want for each microenvironment (add colors here if you have larger k, you need k+1 colors)\nall_colors = {}\nall_colors['ME1'] = '#4E79A7'\nall_colors['ME2'] = '#59A14F'\nall_colors['ME3'] = '#D37295'\nall_colors['Unassigned'] = '#79706E'\n\n# Create table matching each color to a unique ID\ncolors_list = [(key,value) for key,value in all_colors.items()]\nall_colors_df = pd.DataFrame(colors_list, columns=['me_cluster','color'])\nall_colors_df['pheno_id'] = all_colors_df.index + 1\n\n# Make color map for plotting\nmycols = all_colors_df['color'].tolist()\nmycols.insert(0,'#000000') # add black for empty slide, will have id 0\nmycols.append('#FFFFFF') # add white for cell borders, will have id max_n+1\ncolmap = colors.ListedColormap(mycols)\nmax_n = np.max(all_colors_df['pheno_id'])\nbounds = [i-0.5 for i in np.linspace(0,max_n+2, max_n+3)]\nnorm = colors.BoundaryNorm(bounds, colmap.N)\n\n\n# Create microenvironment masks\nfor fov in all_fovs:\n    # Get segentation array\n    seg_path = os.path.join(data_dir, fov, \"masks\", \"segmentation_whole_cell.tiff\")\n    seg_array = io.imread(seg_path).squeeze()\n    \n    # Make CPM using function we created above\n    fig, ax = plt.subplots(figsize=[8,8])\n    cpm = create_cpm(fov, all_nh_df, all_colors_df, seg_array, fig, ax)\n    plt.axis('off')\n    ax.set_title(fov)\n\n    # Add colorbar\n    divider = make_axes_locatable(fig.gca())\n    cax = divider.append_axes(position=\"right\", size=\"5%\", pad=\"3%\")\n    cbar = fig.colorbar(cm.ScalarMappable(norm=norm, cmap=colmap),\n                        cax=cax, orientation=\"vertical\", use_gridspec=True, pad=0.1,\n                        shrink=0.9, drawedges=True)\n    cbar_labels = all_colors_df['me_cluster'].to_list()\n    cbar_labels.insert(0,'Empty') # add black for empty slide, will have id 0\n    cbar_labels.append('Cell border') # add white for cell borders, will have id max_n+1\n\n    cbar.ax.set_yticks(\n        ticks=np.arange(len(cbar_labels)),\n        labels=cbar_labels\n    )\n    cbar.minorticks_off()\n\n    plt.tight_layout()\n\n\n\nAdditional exercises\n\nCompare generating neighborhoods using counts vs. frequency.\nChange the k used for k-means clustering. How do the results change?\nInertia and silhouette score are two metrics that could be useful for helping us choose the best k. Try different k’s and extracting the inertia value (see https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html, intertia_ is an attribute of the output of k-means). Plot k on the x axis and inertia on the y axis. What trend do you see?"
  },
  {
    "objectID": "course/2024/1_introR.html",
    "href": "course/2024/1_introR.html",
    "title": "Introduction to R",
    "section": "",
    "text": "RStudio is an integrated development environment (IDE) for R. It provides a user-friendly interface for coding, debugging, and data analysis. We use RStudio for its convenience and powerful features.\n\n\n\nConsole: Where you can directly type and execute R commands.\nScript Editor: Where you write and save your R scripts.\nEnvironment and History: Displays objects in your workspace and your command history.\nFiles and Plots: Helps manage files and view plots.\nPackages: Shows installed packages and allows you to install and load new ones."
  },
  {
    "objectID": "course/2024/1_introR.html#interface-orientation",
    "href": "course/2024/1_introR.html#interface-orientation",
    "title": "Introduction to R",
    "section": "",
    "text": "Console: Where you can directly type and execute R commands.\nScript Editor: Where you write and save your R scripts.\nEnvironment and History: Displays objects in your workspace and your command history.\nFiles and Plots: Helps manage files and view plots.\nPackages: Shows installed packages and allows you to install and load new ones."
  },
  {
    "objectID": "course/2024/1_introR.html#numeric",
    "href": "course/2024/1_introR.html#numeric",
    "title": "Introduction to R",
    "section": "Numeric",
    "text": "Numeric\n# Numeric variable\nnum_var &lt;- 10\nprint(num_var)  # Output: 10\n\n# Arithmetic operations\nresult &lt;- num_var * 2\nprint(result)  # Output: 20"
  },
  {
    "objectID": "course/2024/1_introR.html#character",
    "href": "course/2024/1_introR.html#character",
    "title": "Introduction to R",
    "section": "Character",
    "text": "Character\n# Character variable\nchar_var &lt;- \"Hello, World!\"\nprint(char_var)  # Output: Hello, World!\n\n# Concatenation\nnew_string &lt;- paste(char_var, \"This is R programming.\")\nprint(new_string)  # Output: Hello, World! This is R programming."
  },
  {
    "objectID": "course/2024/1_introR.html#integer",
    "href": "course/2024/1_introR.html#integer",
    "title": "Introduction to R",
    "section": "Integer",
    "text": "Integer\n# Integer variable\nint_var &lt;- 20L  # The 'L' suffix indicates an integer\nprint(int_var)  # Output: 20\n\n# Integer arithmetic\nresult &lt;- int_var / 5\nprint(result)  # Output: 4"
  },
  {
    "objectID": "course/2024/1_introR.html#matrices",
    "href": "course/2024/1_introR.html#matrices",
    "title": "Introduction to R",
    "section": "Matrices",
    "text": "Matrices\nGene expression data from single-cell RNA sequencing (scRNA-seq) experiments is typically represented as a matrix, where rows correspond to genes and columns correspond to cells. Each cell contains the expression level of a gene, quantified as counts or normalized values. In R, there are several matrix data types commonly used for storing and manipulating gene expression data:\n\nMatrix (matrix): The basic matrix data type in R. It is a two-dimensional array with elements of the same data type.\nData Frame (data.frame): A special type of matrix where columns can contain different data types (e.g., numeric, character, factor). Data frames are commonly used for storing tabular data, including gene expression matrices with additional metadata.\nSparse Matrix (Matrix package): A matrix format optimized for storing large, sparse datasets with many zero values. It conserves memory and speeds up computation for large-scale analyses.\n\n\nBasic Operations on Matrix Objects\n\nCreating a Matrix:\n# Create a matrix with random values\nmat &lt;- matrix(rnorm(20), nrow = 4, ncol = 5)\nMatrix operations\nelement &lt;- mat[1, 2]\nprint(element)\n\n# Calculate row sums\nrow_sums &lt;- rowSums(mat)\nprint(row_sums)\n\n# Calculate column sums\ncol_sums &lt;- colSums(mat)\nprint(col_sums)\n\n# Create another matrix\nmat2 &lt;- matrix(rnorm(20), nrow = 5, ncol = 4)\nprint(mat2)\n\n# Perform matrix multiplication\nmat_mult &lt;- mat %*% mat2\nprint(mat_mult)\n\n# Transpose the matrix\nmat_transpose &lt;- t(mat)\nprint(mat_transpose)\n\n# Select the first two rows\nfirst_two_rows &lt;- mat[1:2, ]\nprint(first_two_rows)\n\n# Select the first three columns\nfirst_three_cols &lt;- mat[, 1:3]\nprint(first_three_cols)"
  },
  {
    "objectID": "course/2024/1_introR.html#logical",
    "href": "course/2024/1_introR.html#logical",
    "title": "Introduction to R",
    "section": "Logical",
    "text": "Logical\n# Logical variable\nlogical_var &lt;- TRUE\nprint(logical_var)  # Output: TRUE\n\n# Logical operations\nresult &lt;- logical_var & FALSE\nprint(result)  # Output: FALSE\n\n\n# Logical operations\na &lt;- TRUE\nb &lt;- FALSE\n\n# AND operation\nresult_and &lt;- a & b\nprint(result_and)  # Output: FALSE\n\n# OR operation\nresult_or &lt;- a | b\nprint(result_or)   # Output: TRUE\n\n# NOT operation\nresult_not &lt;- !a\nprint(result_not)  # Output: FALSE\n\n# Comparison operators\nx &lt;- 5\ny &lt;- 10\n\n# Greater than\nresult_gt &lt;- x &gt; y\nprint(result_gt)  # Output: FALSE\n\n# Less than\nresult_lt &lt;- x &lt; y\nprint(result_lt)  # Output: TRUE\n\n# Equal to\nresult_eq &lt;- x == y\nprint(result_eq)  # Output: FALSE\n\n# Not equal to\nresult_neq &lt;- x != y\nprint(result_neq)  # Output: TRUE"
  },
  {
    "objectID": "course/2024/1_introR.html#while",
    "href": "course/2024/1_introR.html#while",
    "title": "Introduction to R",
    "section": "while",
    "text": "while\nA while loop is a control flow statement that allows code to be executed repeatedly based on a given Boolean condition. The loop executes the loop body as long as the condition remains true. When the condition becomes false, the loop terminates.\n# Example of a while loop\nx &lt;- 1\nwhile (x &lt;= 5) {\n  print(x)\n  x &lt;- x + 1\n}\n\nConsiderations:\n\nEnsure that the loop has an exit condition that is guaranteed to be met to avoid infinite loops.\nAvoid complex conditions that can make the loop difficult to read and maintain.\nUse while loops when the number of iterations is not known before the loop starts, as opposed to for loops, which are better suited for a known number of iterations.\nManage loop variables carefully to ensure they are updated correctly and the loop condition changes as expected."
  },
  {
    "objectID": "course/2024/1_introR.html#for",
    "href": "course/2024/1_introR.html#for",
    "title": "Introduction to R",
    "section": "for",
    "text": "for\nA for loop is a control flow statement used in many programming languages to repeat a block of code multiple times. It is particularly useful for iterating over sequences (like lists, arrays, or strings) and executing a piece of code for each element in the sequence.\n# Example of a for loop\nfor (i in 1:5) {\n  print(i)\n}\n\nConsiderations\n\nEnsure the loop has a condition that eventually becomes false to prevent infinite loops.\nBe careful with the loop’s scope and variables to avoid unintended side effects."
  },
  {
    "objectID": "course/2024/1_introR.html#apply",
    "href": "course/2024/1_introR.html#apply",
    "title": "Introduction to R",
    "section": "apply",
    "text": "apply\nThe apply() function in R is a powerful tool for applying a function to the rows or columns of a matrix or data frame. It is particularly useful for performing operations across a dataset without needing to write explicit loops. The syntax for apply() is:\napply(X, margin, function, ...)\n\n# X: This is the array or matrix on which you want to apply the function.\n# margin: A value that specifies whether to apply the function over rows (1), columns (2), or both (c(1, 2)).\n# function: The function you want to apply to each row or column.\nTo calculate the sum of each row in a matrix:\n# Create a matrix\nmy_matrix &lt;- matrix(1:9, nrow=3)\n\n# Apply sum function across rows\nrow_sums &lt;- apply(my_matrix, 1, sum)\nprint(row_sums)\nTo find the mean of each column in a data frame:\n# Create a data frame\ndf &lt;- data.frame(a = c(1, 2, 3), b = c(4, 5, 6))\n\n# Apply mean function across columns\ncolumn_means &lt;- apply(df, 2, mean)\nprint(column_means)\n\nsapply and lappy\n\nlapply() returns a list, regardless of the output of each application of the function.\nsapply() attempts to simplify the result into a vector or matrix if possible. If simplification is not possible, it returns a list similar to lapply().\n\nSuppose you have a list of numerical vectors and you want to compute the sum of each vector. Here’s how you could use lapply():\n# Define a list of vectors\nnum_list &lt;- list(c(1, 2, 3), c(4, 5), c(6, 7, 8, 9))\n\n# Use lapply to apply the sum function\nlist_sums &lt;- lapply(num_list, sum)\nprint(list_sums)\nUsing the same list of numerical vectors, if you use sapply() to compute the sum, the function will try to simplify the output into a vector:\n# Use sapply to apply the sum function\nvector_sums &lt;- sapply(num_list, sum)\nprint(vector_sums)\nWhen to Use Each\n\nlapply(): When you need the robustness of a list output, especially when dealing with heterogeneous data or when the function can return variable lengths or types.\nsapply(): When you are working with homogeneous data and prefer a simplified output such as a vector or matrix, assuming the lengths and types are consistent across elements."
  },
  {
    "objectID": "course/2024/1_introR.html#finding-patterns",
    "href": "course/2024/1_introR.html#finding-patterns",
    "title": "Introduction to R",
    "section": "Finding Patterns",
    "text": "Finding Patterns\nFinding specific sequences or motifs within biological sequences is a common task.\nlibrary(stringr)\nsequence &lt;- \"ATGCGTACGTTGACA\"\nmotif &lt;- \"CGT\"\nstr_locate(sequence, motif)"
  },
  {
    "objectID": "course/2024/1_introR.html#replacing-substrings",
    "href": "course/2024/1_introR.html#replacing-substrings",
    "title": "Introduction to R",
    "section": "Replacing Substrings",
    "text": "Replacing Substrings\nModifying sequences by replacing specific nucleotides or amino acids.\ndna_sequence &lt;- \"ATGCGTACGTTGACT\"\nrna_sequence &lt;- str_replace_all(dna_sequence, \"T\", \"U\")\nprint(rna_sequence)"
  },
  {
    "objectID": "course/2024/1_introR.html#substring-extraction",
    "href": "course/2024/1_introR.html#substring-extraction",
    "title": "Introduction to R",
    "section": "Substring Extraction",
    "text": "Substring Extraction\nExtracting parts of sequences, such as cutting out genes or regions of interest.\nextracted_sequence &lt;- str_sub(sequence, 3, 8)\nprint(extracted_sequence)"
  },
  {
    "objectID": "course/2024/1_introR.html#length-calculation",
    "href": "course/2024/1_introR.html#length-calculation",
    "title": "Introduction to R",
    "section": "Length Calculation",
    "text": "Length Calculation\nDetermining the length of sequences.\nsequence_length &lt;- str_length(sequence)\nprint(sequence_length)"
  },
  {
    "objectID": "course/2024/1_introR.html#case-conversion",
    "href": "course/2024/1_introR.html#case-conversion",
    "title": "Introduction to R",
    "section": "Case Conversion",
    "text": "Case Conversion\nConverting uppercase to lowercase, or vice versa.\nsequence_upper &lt;- str_to_upper(sequence)\nprint(sequence_upper)"
  },
  {
    "objectID": "course/2024/1_introR.html#splitting-strings",
    "href": "course/2024/1_introR.html#splitting-strings",
    "title": "Introduction to R",
    "section": "Splitting Strings",
    "text": "Splitting Strings\nSplitting sequences into arrays, useful for reading fasta files or analyzing codons.\ncodons &lt;- str_sub(sequence, seq(1, str_length(sequence), by = 3), seq(3, str_length(sequence), by = 3))\nprint(codons)\nBonus Challenge: What if our sequence length wasn’t a multiple of three? For an example of how to approach this situation please check out the Bonus challenge."
  },
  {
    "objectID": "course/2024/1_introR.html#counting-specific-characters",
    "href": "course/2024/1_introR.html#counting-specific-characters",
    "title": "Introduction to R",
    "section": "Counting Specific Characters",
    "text": "Counting Specific Characters\nCounting occurrences of specific nucleotides or amino acids.\nguanine_count &lt;- str_count(sequence, \"G\")\nprint(guanine_count)"
  },
  {
    "objectID": "course/2024/1_introR.html#tidyverse",
    "href": "course/2024/1_introR.html#tidyverse",
    "title": "Introduction to R",
    "section": "Tidyverse",
    "text": "Tidyverse\nTidyverse is a collection of R packages designed for data science. It includes packages like ggplot2 for data visualization and dplyr for data manipulation.\n\nTidyverse Data Frames\nTidyverse is a collection of R packages designed for data science, developed with a focus on simplicity, consistency, and ease of use. One of the key components of Tidyverse is the concept of tidy data frames.\nA tidyverse data frame is a rectangular table-like structure where:\n\nEach row represents an observation.\nEach column represents a variable.\nEach cell holds a single value.\n\n\n\nPrinciples of Tidy Data:\n\nEach variable forms a column: In a tidy data frame, each variable is placed in its own column. This makes it easy to work with the dataset as each variable is explicitly represented.\nEach observation forms a row: Each row corresponds to a distinct observation, entity, or case. This makes it straightforward to perform operations on individual observations.\nEach type of observational unit forms a table: Different types of data should be stored in separate tables, with relationships between tables represented using unique identifiers.\n\n\n\nWhy Tidy Data Frames are Important:\n\nEase of Analysis: Tidy data frames make it easier to perform data manipulation, visualization, and analysis using Tidyverse packages such as dplyr, ggplot2, and tidyr.\nReadability and Interpretability: Tidy data frames have a consistent structure, making it easier for others to understand and interpret your data.\nEfficiency: Tidy data frames facilitate efficient and concise code, reducing the complexity of data manipulation tasks.\n\n\n\nTidyverse Packages for Data Manipulation:\n\ndplyr: Provides a grammar of data manipulation for data frames, enabling easy filtering, selecting, mutating, summarizing, and arranging of data.\ntidyr: Offers tools for reshaping and tidying messy datasets, such as gather() and spread() functions for converting between wide and long formats.\nggplot2: Allows for the creation of sophisticated data visualizations using a layered grammar of graphics.\n\nlibrary(ggplot2)\nlibrary(tidyr)\nlibrary(dplyr)\n\n# Example dataset\ndata &lt;- data.frame(\n  ID = 1:3,\n  Name = c(\"Alice\", \"Bob\", \"Charlie\"),\n  Math = c(90, 85, 95),\n  Science = c(88, 92, 89)\n)\n\n# Original dataset\nprint(\"Original dataset:\")\nprint(data)\n\n# Tidy the dataset using gather() function from tidyr\ntidy_data &lt;- gather(data, Subject, Score, -ID, -Name)\n\n# Tidied dataset\nprint(\"Tidied dataset:\")\nprint(tidy_data)"
  },
  {
    "objectID": "course/2024/1_introR.html#selecting-columns",
    "href": "course/2024/1_introR.html#selecting-columns",
    "title": "Introduction to R",
    "section": "Selecting Columns",
    "text": "Selecting Columns\nSelecting columns allows you to choose specific columns from your dataset. It helps you focus on the variables of interest and ignore the rest.\nselected_data &lt;- select(data, ID, Math)\nprint(selected_data)"
  },
  {
    "objectID": "course/2024/1_introR.html#filtering-rows",
    "href": "course/2024/1_introR.html#filtering-rows",
    "title": "Introduction to R",
    "section": "Filtering Rows",
    "text": "Filtering Rows\nFiltering rows allows you to subset your dataset based on specific conditions. It helps you extract only the rows that meet certain criteria.\n# Filtering rows based on conditions\nfiltered_data &lt;- filter(data, Math &gt; 90)\nprint(filtered_data)"
  },
  {
    "objectID": "course/2024/1_introR.html#summarizing-data",
    "href": "course/2024/1_introR.html#summarizing-data",
    "title": "Introduction to R",
    "section": "Summarizing Data",
    "text": "Summarizing Data\nSummarizing data involves calculating summary statistics or aggregating data to get a concise overview of your dataset. It helps you understand the overall characteristics of your data.\nsummary_data &lt;- summarize(data, \n                          Mean_Math = mean(Math), \n                          Mean_Science = mean(Science))\nprint(summary_data)"
  },
  {
    "objectID": "course/2024/1_introR.html#sorting-arranging",
    "href": "course/2024/1_introR.html#sorting-arranging",
    "title": "Introduction to R",
    "section": "Sorting (Arranging)",
    "text": "Sorting (Arranging)\nArranging rows involves sorting your dataset based on the values of one or more columns. It helps you reorder your data for better visualization or analysis.\narranged_data &lt;- arrange(data, desc(Math))\nprint(arranged_data)"
  },
  {
    "objectID": "course/2024/1_introR.html#mutate",
    "href": "course/2024/1_introR.html#mutate",
    "title": "Introduction to R",
    "section": "Mutate",
    "text": "Mutate\nThe mutate() function in the dplyr package is essential for transforming data frames in R. It allows you to add new columns to a data frame or modify existing ones, using existing data. mutate() is part of the tidyverse, a collection of R packages designed for data science that makes data manipulation, exploration, and visualization easy and intuitive.\n\nAdding columns\nCalculating the GC content of DNA sequences.\nlibrary(dplyr)\nlibrary(stringr)\n\n# Sample data\nsequences &lt;- tibble(\n  id = c(\"seq1\", \"seq2\", \"seq3\"),\n  dna = c(\"ATGCGC\", \"GCGTACGT\", \"ATATATAT\")\n)\n\n# Calculate GC content\nsequences &lt;- sequences %&gt;%\n  mutate(gc_content = (str_count(dna, \"[GC]\") / str_length(dna)) * 100)\n\nprint(sequences)\n\n\nReplacing existing columns\nTranscription of DNA sequences into RNA sequences involves replacing thymine (T) with uracil (U).\n# Convert DNA to RNA\nsequences &lt;- sequences %&gt;%\n  mutate(rna = str_replace_all(dna, \"T\", \"U\"))\n\nprint(sequences)\n\n\nMultiple Transformations\nIdentifying potential neoantigens by finding motifs associated with high mutation frequencies or specific mutation patterns.\n# Sample DNA sequences\nsequences &lt;- tibble(\n  id = c(\"seq1\", \"seq2\", \"seq3\"),\n  dna = c(\"ATGCGCATC\", \"GCGTACGTAGT\", \"ATATATATAT\")\n)\n\n# Assume a simple motif that might indicate a neoantigen\nmotif = \"ATG\"\n\n# Annotate sequences with potential neoantigen presence\nsequences &lt;- sequences %&gt;%\n  mutate(\n    start_position = str_locate(dna, motif)[,1],\n    is_neoantigen_candidate = ifelse(start_position &gt; 0 & str_count(dna, \"[GC]\") / str_length(dna) &gt; 0.5, \"Yes\", \"No\")\n  )\n\nprint(sequences)"
  },
  {
    "objectID": "course/2024/1_introR.html#definition",
    "href": "course/2024/1_introR.html#definition",
    "title": "Introduction to R",
    "section": "Definition",
    "text": "Definition\nFunctions (function) in R perform specific tasks. They take input (arguments), perform operations, and return output.\nfunction_name &lt;- function(argument1, argument2, ...) {\n  # Function body\n  # Perform operations\n  # Return a value (optional)\n}\n\nfunction_name: Name of the function.\nargument1, argument2, …: Arguments passed to the function (optional).\nFunction body: Code block where you define what the function should do.\nReturn a value (optional): Use the return() statement to specify what the function should return (optional).\n\nHere, we define a function and call it!\n# Define a function to calculate the square of a number\nsquare &lt;- function(x) {\n  return(x^2)\n}\n\n# Call the function\nresult &lt;- square(5)\nprint(result)  # Output: 25"
  },
  {
    "objectID": "course/2024/1_introR.html#example",
    "href": "course/2024/1_introR.html#example",
    "title": "Introduction to R",
    "section": "Example",
    "text": "Example\nLet’s bring together concepts by writing the function analyze_tcr_data.\nThe function will:\n\nFilter T cell sequences based on a specified length threshold.\nSort the remaining data by clonality in descending order to identify the most prevalent TCRs.\nCreate a new column that indicates the presence of a specific motif within the TCR sequence, a common task in sequence analysis.\n\nlibrary(dplyr)\nlibrary(stringr)\n\n# Define the function\nanalyze_tcr_data &lt;- function(tcr_tibble, length_threshold) {\n  # Validate input\n  if (!inherits(tcr_tibble, \"tbl_df\")) {\n    stop(\"Input must be a tibble.\")\n  }\n  \n  # Filter TCR sequences longer than the threshold and sort by clonality\n  filtered_and_sorted &lt;- tcr_tibble %&gt;%\n    filter(str_length(tcr_sequence) &gt; length_threshold) %&gt;%\n    arrange(desc(clonality))\n  \n  # Add a column to indicate the presence of a specific motif (e.g., 'CASS')\n  enhanced_tcr_tibble &lt;- filtered_and_sorted %&gt;%\n    mutate(has_motif = if_else(str_detect(tcr_sequence, \"CASS\"), \"Yes\", \"No\"))\n  \n  # Return the transformed tibble\n  return(enhanced_tcr_tibble)\n}\n\n# Example usage\n# Assuming a tibble with TCR sequences and clonality metrics\ntcr_data &lt;- tibble(\n  tcr_sequence = c(\"CASSLGGTDTQYF\", \"CASSLGDETQYF\", \"CASSLG\", \"CASSEGTDTQYF\"),\n  clonality = c(0.25, 0.15, 0.05, 0.55)\n)\n\n# Apply the function with a length threshold of 10\nresult_data &lt;- analyze_tcr_data(tcr_data, 10)\nprint(result_data)\nExplanation\n\nValidation: The function starts by checking if the provided data is a tibble to ensure type safety.\nFiltering: Uses filter() to retain only TCR sequences longer than the specified length_threshold.\nSorting: Uses arrange() to sort the data by clonality in descending order.\nString Manipulation: Adding has_motif Column: Uses mutate() combined with str_detect() from the stringr package to add a column that indicates whether each TCR sequence contains the motif “CASS”.\nReturn Value: Outputs a tibble that’s been filtered, sorted, and enhanced with additional string-based analysis."
  },
  {
    "objectID": "course/2024/1_introR.html#bonus-challenge",
    "href": "course/2024/1_introR.html#bonus-challenge",
    "title": "Introduction to R",
    "section": "Bonus challenge",
    "text": "Bonus challenge\nIf our sequence isn’t length three, this code will throw an error because the function expects a sequence that is a multiple of 3:\nlibrary(stringr)\nsequence &lt;- \"ATGCGTACGTTGAC\" # Length 14\n# sequence &lt;- \"ATGCGTACGTTGACA\" # Length 15\n\ncodons &lt;- str_sub(sequence, seq(1, str_length(sequence), by = 3), seq(3, str_length(sequence), by = 3))\n\n#&gt; Error in `str_sub()`:\n#&gt; ! Can't recycle `string` (size 5) to match `end` (size 4).\n#&gt; Run `rlang::last_trace()` to see where the error occurred.\nCan you try to code up a solution to this problem?\n\n\nHINT\n# We can use the modulus operator \"%%\". It yields the remainder when the first operand is divided by the second.\n\n15 %% 3 \n#&gt; [1] 0\n\n14 %% 3\n#&gt; [1] 2\n\n\nHere are a few options for how to solve this (only peek after trying yourself).\n\n\nShow me the solution\nsequence &lt;- \"ATGCGTACGTTGAC\" # Length 14\n\n# sequence &lt;- \"ATGCGTACGTTGACA\" # Length 15\n\n# Option 1: Check for length 3 and make sure before running the code, otherwise print a warning.\n\nsequence_length &lt;- nchar(sequence) # Get sequence length\n\nif (sequence_length %% 3 != 0) {\n  print(paste(\"Error! The sequence is length:\", sequence_length))\n  codons &lt;- \"ERROR\"\n} else {\n  codons &lt;- str_sub(string = sequence, \n  start = seq(1, str_length(sequence), by = 3), \n  end = seq(3, str_length(sequence), by = 3))\n}\n\nprint(codons)\n\n# Option 2: Add N to the sequence to make in a multiple of 3, and then any seqeuence will run properly.\n\nif (sequence_length %% 3 != 0) {\n  leftover &lt;- 3 - nchar(sequence) %% 3\n  \n  leftover_n &lt;- paste(rep(\"N\", leftover), collapse = \"\")\n  \n  sequence &lt;- paste0(sequence, leftover_n)\n}\n\ncodons &lt;- str_sub(sequence, seq(1, str_length(sequence), by = 3), seq(3, str_length(sequence), by = 3))\n\nprint(codons)"
  },
  {
    "objectID": "course/2024/python_tutorial.html",
    "href": "course/2024/python_tutorial.html",
    "title": "Intro to Python",
    "section": "",
    "text": "You can import packages using “import” (analogous to ‘library’ in R).\n\nimport os\n\nYou can also assign “nicknames” to packages when you’re importing them.\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\nIf you only need certain functions in a package, you can also only import the module within the package that you need.\n\nimport matplotlib.pyplot as plt\nimport skimage.io as io"
  },
  {
    "objectID": "course/2024/python_tutorial.html#importing-packages",
    "href": "course/2024/python_tutorial.html#importing-packages",
    "title": "Intro to Python",
    "section": "",
    "text": "You can import packages using “import” (analogous to ‘library’ in R).\n\nimport os\n\nYou can also assign “nicknames” to packages when you’re importing them.\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\n\nIf you only need certain functions in a package, you can also only import the module within the package that you need.\n\nimport matplotlib.pyplot as plt\nimport skimage.io as io"
  },
  {
    "objectID": "course/2024/python_tutorial.html#basic-data-structures",
    "href": "course/2024/python_tutorial.html#basic-data-structures",
    "title": "Intro to Python",
    "section": "Basic data structures",
    "text": "Basic data structures\nA python list is like an array:\n\nxs = [3, 1, 2]   # Create a list\nprint(xs)\n\nOne difference between R and Python is that Python uses 0-indexing, meaning that the first element of the list is accessed using 0.\n\nprint(xs[0])\nprint(xs[-1])     # Negative indices count from the end of the list; prints \"2\"\n\nIn addition to accessing list elements one at a time, Python provides concise syntax to access sublists; this is known as slicing.\n\nnums = list(range(5))    # range is a built-in function that creates a list of integers\nprint(nums)         # Prints \"[0, 1, 2, 3, 4]\"\nprint(nums[2:4])    # Get a slice from index 2 to 4 (exclusive); prints \"[2, 3]\"\nprint(nums[2:])     # Get a slice from index 2 to the end; prints \"[2, 3, 4]\"\nprint(nums[:2])     # Get a slice from the start to index 2 (exclusive); prints \"[0, 1]\"\nprint(nums[:])      # Get a slice of the whole list; prints [\"0, 1, 2, 3, 4]\"\nprint(nums[:-1])    # Slice indices can be negative; prints [\"0, 1, 2, 3]\"\nnums[2:4] = [8, 9] # Assign a new sublist to a slice\nprint(nums)         # Prints \"[0, 1, 8, 9, 4]\"\n\nYou can loop over the elements of a list like this:\n\nanimals = ['cat', 'dog', 'monkey']\nfor animal in animals:\n    print(animal)\n\nIf you want access to the index of each element within the body of a loop, use the built-in enumerate function.\n\nanimals = ['cat', 'dog', 'monkey']\nfor idx, animal in enumerate(animals):\n    print('#{}: {}'.format(idx + 1, animal)) #in 'print' syntax, the variables after 'format' are printed where the '{}' are\n\nWhen programming, frequently we want to perform operations on every element of a list. As a simple example, consider the following code that computes square numbers:\n\nnums = [0, 1, 2, 3, 4]\nsquares = []\nfor x in nums:\n    squares.append(x ** 2)\nprint(squares)\n\nYou can make this code a lot simpler using a list comprehension:\n\nnums = [0, 1, 2, 3, 4]\nsquares = [x ** 2 for x in nums]\nprint(squares)\n\nList comprehensions can also contain conditions (“%” is the mod function, which performs division between two numbers and returns the remainder, ex. 6 % 3 = 0, 7 % 3 = 1):\n\nnums = [0, 1, 2, 3, 4]\neven_squares = [x ** 2 for x in nums if x % 2 == 0]\nprint(even_squares)\n\nAnother useful data structure is a dictionary. A dictionary stores (key, value) pairs. You can use it like this:\n\nd = {'cat': 'cute', 'dog': 'furry'}  # Create a new dictionary with some data in 'key':'value' pairs\nprint(d['cat'])       # Get an entry from a dictionary; prints \"cute\"\nprint('cat' in d)     # Check if a dictionary has a given key; prints \"True\"\n\n\nd['fish'] = 'wet'    # Create an entry in a dictionary\nprint(d['fish'])      # Access an entry in a dictionary, prints \"wet\"\n\nYou can iterate over the entries in a dictionary:\n\nd = {'person': 2, 'cat': 4, 'spider': 8}\nfor animal, legs in d.items(): # Here, we are deconstructing each key-value pair into variables called 'animal' and 'legs'\n    print('A {} has {} legs'.format(animal, legs))"
  },
  {
    "objectID": "course/2024/python_tutorial.html#defining-functions",
    "href": "course/2024/python_tutorial.html#defining-functions",
    "title": "Intro to Python",
    "section": "Defining functions",
    "text": "Defining functions\nPython functions are defined using the def keyword. For example:\n\ndef sign(x):\n    if x &gt; 0:\n        return 'positive'\n    elif x &lt; 0:\n        return 'negative'\n    else:\n        return 'zero'\n\n\nfor x in [-1, 0, 1]:\n    print(sign(x))"
  },
  {
    "objectID": "course/2024/python_tutorial.html#numpy",
    "href": "course/2024/python_tutorial.html#numpy",
    "title": "Intro to Python",
    "section": "Numpy",
    "text": "Numpy\nNumpy is the core library for scientific computing in Python. It provides a high-performance multidimensional array object, and tools for working with these arrays.\n\nimport numpy as np\n\nWe can initialize numpy arrays from nested Python lists, and access elements using square brackets:\n\na = np.array([[1,2,3],[4,5,6]])\nprint(a)\nprint(a.shape)\nprint(a[0, 0], a[0, 1], a[1, 0])\n\nBoolean array indexing lets you pick out arbitrary elements of an array. Frequently this type of indexing is used to select the elements of an array that satisfy some condition. Here is an example:\n\na = np.array([[1,2], [3, 4], [5, 6]])\n\nbool_idx = (a &gt; 2)  # Find the elements of a that are bigger than 2;\n                    # this returns a numpy array of Booleans of the same\n                    # shape as a, where each slot of bool_idx tells\n                    # whether that element of a is &gt; 2.\n\nprint(bool_idx)\n\n\n# We use boolean array indexing to construct a rank 1 array\n# consisting of the elements of a corresponding to the True values\n# of bool_idx\nprint(a[bool_idx])\n\n# We can do all of the above in a single concise statement:\nprint(a[a &gt; 2])\n\nNumpy is a powerful library that underlies a lot of machine learning packages in Python. To learn more, check out the documentation: https://numpy.org/doc/stable/user/quickstart.html.\nScipy is a collection of mathematical algorithms built on top of Numpy. For more tutorials and information, see https://scipy.org/."
  },
  {
    "objectID": "course/2024/python_tutorial.html#pandas",
    "href": "course/2024/python_tutorial.html#pandas",
    "title": "Intro to Python",
    "section": "Pandas",
    "text": "Pandas\nPandas is a powerful library for working with tabular data (similar to data frames in R).\n\nimport pandas as pd\n\nHere is a simple example:\n\ndata = {\n  \"calories\": [420, 380, 390],\n  \"duration\": [50, 40, 45]\n}\n\n# Load data into a DataFrame object:\ndf = pd.DataFrame(data)\ndf\n\nTo locate a specific row:\n\nprint(df.loc[0])\n\nAdd column to data frame:\n\ndf['group'] = ['group1','group1','group2']\ndf\n\nSubset a data frame:\n\n# Only keep rows in group 1\ndf.loc[df['group'] == 'group1']\n\nCount the number of rows in each group:\n\ndf.groupby('group').size()\n\nYou can also perform operations on groups of rows. For example, here, we are finding the mean calories in each group.\n\ndf.groupby('group')['calories'].mean()\n\nPandas dataframes are indexed (the numbers on the left hand side). Even when you subset a table, the indices do not change. For example, we can see that when we subset for group2 only, the index for that row stays the same.\n\ndf.loc[df['group'] == 'group2']\n\nIf we want to reset the index, we can use the reset_index function:\n\ndf.loc[df['group'] == 'group2'].reset_index()\n\nWe can merge 2 data frames using hte merge function:\n\nnew_data = {\n  \"group\": ['group1', 'group2'],\n  \"new_col\": [100, 200]\n}\n\n# Load data into a DataFrame object:\nnew_df = pd.DataFrame(new_data)\nnew_df\n\n\ndf.merge(new_df, on='group') # merge the two tables using the 'group' column\n\nTo read data directly from a file and load it as a pandas DataFrame:\n\npd.read_csv(\"example_data/cell_table.csv\")\n\nPandas has a lot more functionality. For more tutorials and information, see https://pandas.pydata.org/docs/index.html."
  },
  {
    "objectID": "course/2024/python_tutorial.html#matplotlib-and-seaborn",
    "href": "course/2024/python_tutorial.html#matplotlib-and-seaborn",
    "title": "Intro to Python",
    "section": "Matplotlib and Seaborn",
    "text": "Matplotlib and Seaborn\nMatplotlib is a plotting library. In this section, we will give a brief introduction to the matplotlib.pyplot module.\n\nimport matplotlib.pyplot as plt\n\nHere is a simple example:\n\n# Compute the x and y coordinates for points on a sine curve\nx = np.arange(0, 3 * np.pi, 0.1)\ny = np.sin(x)\n\n# Plot the points using matplotlib\nplt.plot(x, y)\nplt.show()\n\nYou can plot different things in the same figure using the subplot function. Here is an example:\n\n# Compute the x and y coordinates for points on sine and cosine curves\nx = np.arange(0, 3 * np.pi, 0.1)\ny_sin = np.sin(x)\ny_cos = np.cos(x)\n\n# Set up a subplot grid that has height 2 and width 1,\n# and set the first such subplot as active.\nplt.subplot(2, 1, 1)\n\n# Make the first plot\nplt.plot(x, y_sin)\nplt.title('Sine')\n\n# Set the second subplot as active, and make the second plot.\nplt.subplot(2, 1, 2)\nplt.plot(x, y_cos)\nplt.title('Cosine')\n\n# Show the figure.\nplt.show()\n\nSeaborn is another popular plotting package that can generate “prettier” plots (maybe similar to ‘ggplot2’ in R).\n\nimport seaborn as sns\n\n\n# Load an example dataset\ntips = sns.load_dataset(\"tips\")\n\n# Create a visualization\nsns.relplot(\n    data=tips,\n    x=\"total_bill\", y=\"tip\", col=\"time\",\n    hue=\"smoker\", style=\"smoker\", size=\"size\",\n)\nplt.show()\n\nFor more documentation on these two packages, see https://matplotlib.org/3.5.3/api/_as_gen/matplotlib.pyplot.html and https://seaborn.pydata.org/tutorial/introduction.html."
  },
  {
    "objectID": "course/2024/python_tutorial.html#images",
    "href": "course/2024/python_tutorial.html#images",
    "title": "Intro to Python",
    "section": "Images",
    "text": "Images\nThere are a few different packages that you can use for opening images. scikit-image (shorted as skimage) is a popular one.\n\nimport skimage.io as io\n\nHere, we are loading in an image.\n\nexample_image = io.imread(\"example_data/fov1/image_data/CD45.tiff\")\n\nWe can see that the image is just an array of numbers.\n\nexample_image\n\nWe can inspect the shape of the array (the shape of the image).\n\nexample_image.shape\n\nWe can also display the image.\n\nfig = plt.figure(figsize=(8,8))\nplt.imshow(example_image, origin=\"lower\", cmap='gray', vmax=np.quantile(example_image,0.99))\nplt.axis('off')\nplt.tight_layout()\n\nFor more documentation on scikit-image, see https://scikit-image.org/."
  },
  {
    "objectID": "course/2024/python_tutorial.html#additional-exercises",
    "href": "course/2024/python_tutorial.html#additional-exercises",
    "title": "Intro to Python",
    "section": "Additional exercises",
    "text": "Additional exercises\n\nRead in the cell tabe at “example_data/cell_table.csv”. How many unique types of cells are there? How many unique FOVs?\nFilter the cell table for cells in FOV2. How many CD4 T cells are there?\nMake a dictionary mapping each cell ID to its cell type.\nMake a bargraph showing the number of cells of each cell type in FOV1.\nRead in some example images in the “example_data” folder and play with the “vmax” parameter to see how the image changes."
  },
  {
    "objectID": "course/4_qctrouble.html",
    "href": "course/4_qctrouble.html",
    "title": "Reading, navigating, and plotting data",
    "section": "",
    "text": "The key to accomplishing any analysis is to start by understand what your data looks like and how it’s organized. This might include:\n\nWhat type of files are you working with?\nHow do they get loaded into R?\nWhat is the size of the dataset?\nWhat types of questions can I ask of the data?\n\n\nlibrary(dplyr)\n\nvariant_file &lt;- \"/cloud/project/data/single_cell_rna/cancer_cell_id/mcb6c-exome-somatic.variants.annotated.clean.filtered.tsv\"\nprint(variant_file)\n\n[1] \"/cloud/project/data/single_cell_rna/cancer_cell_id/mcb6c-exome-somatic.variants.annotated.clean.filtered.tsv\"\n\n\n# File is a \"tsv\" file -&gt; Tab-delimited file\nread_tsv &lt;- read.csv(variant_file, sep = '\\t')\nNow we can explore the file we just loaded:\n\n# Look at the file: View(), head(), or click on it to the right\nView(read_tsv)\nhead(read_tsv)\n\n  CHROM     POS REF ALT                    set NORMAL.GT NORMAL.AD NORMAL.AF\n1  chr1 4347274   T   A mutect-varscan-strelka       T/T     122,0         0\n2  chr1 4347386   C   A mutect-varscan-strelka       C/C      96,0         0\n3  chr1 4830263   C   T mutect-varscan-strelka       C/C     195,0         0\n4  chr1 5084567   C   T mutect-varscan-strelka       C/C      61,0         0\n5  chr1 6247967   T   G mutect-varscan-strelka       T/T      79,0         0\n6  chr1 6249819   T   A mutect-varscan-strelka       T/T     189,0         0\n  NORMAL.DP TUMOR.GT TUMOR.AD TUMOR.AF TUMOR.DP\n1       122      T/A    44,48  0.52174       92\n2        96      C/A    42,42  0.50000       84\n3       195      C/T    79,64  0.44755      143\n4        62      C/T    26,19  0.42222       45\n5        79      T/G    24,18  0.42857       42\n6       189      T/A    58,57  0.49565      115\n                           Consequence  SYMBOL Feature_type\n1                     missense_variant     Rp1   Transcript\n2                     missense_variant     Rp1   Transcript\n3 splice_region_variant&intron_variant  Lypla1   Transcript\n4 splice_region_variant&intron_variant Atp6v1h   Transcript\n5                       intron_variant  Rb1cc1   Transcript\n6                     missense_variant  Rb1cc1   Transcript\n                Feature cDNA_position CDS_position Protein_position Amino_acids\n1  ENSMUST00000027032.5          3741         3614             1205         N/I\n2  ENSMUST00000027032.5          3629         3502             1168         V/F\n3 ENSMUST00000027036.10                                                        \n4 ENSMUST00000044369.12                                                        \n5 ENSMUST00000027040.12                                                        \n6 ENSMUST00000027040.12          3928         3461             1154         V/D\n   Codons NORMAL.REF.AD NORMAL.ALT.AD TUMOR.REF.AD TUMOR.ALT.AD\n1 aAt/aTt           122             0           44           48\n2 Gtt/Ttt            96             0           42           42\n3                   195             0           79           64\n4                    61             0           26           19\n5                    79             0           24           18\n6 gTt/gAt           189             0           58           57\n\n# Understand the variables and data structure: typeof(), str(), colnames()\ntypeof(read_tsv)\n\n[1] \"list\"\n\nstr(read_tsv)\n\n'data.frame':   10427 obs. of  26 variables:\n $ CHROM           : chr  \"chr1\" \"chr1\" \"chr1\" \"chr1\" ...\n $ POS             : int  4347274 4347386 4830263 5084567 6247967 6249819 6264623 6842242 6842967 6844056 ...\n $ REF             : chr  \"T\" \"C\" \"C\" \"C\" ...\n $ ALT             : chr  \"A\" \"A\" \"T\" \"T\" ...\n $ set             : chr  \"mutect-varscan-strelka\" \"mutect-varscan-strelka\" \"mutect-varscan-strelka\" \"mutect-varscan-strelka\" ...\n $ NORMAL.GT       : chr  \"T/T\" \"C/C\" \"C/C\" \"C/C\" ...\n $ NORMAL.AD       : chr  \"122,0\" \"96,0\" \"195,0\" \"61,0\" ...\n $ NORMAL.AF       : num  0 0 0 0 0 0 0 0 0 0 ...\n $ NORMAL.DP       : int  122 96 195 62 79 189 83 60 86 77 ...\n $ TUMOR.GT        : chr  \"T/A\" \"C/A\" \"C/T\" \"C/T\" ...\n $ TUMOR.AD        : chr  \"44,48\" \"42,42\" \"79,64\" \"26,19\" ...\n $ TUMOR.AF        : num  0.522 0.5 0.448 0.422 0.429 ...\n $ TUMOR.DP        : int  92 84 143 45 42 115 63 33 55 43 ...\n $ Consequence     : chr  \"missense_variant\" \"missense_variant\" \"splice_region_variant&intron_variant\" \"splice_region_variant&intron_variant\" ...\n $ SYMBOL          : chr  \"Rp1\" \"Rp1\" \"Lypla1\" \"Atp6v1h\" ...\n $ Feature_type    : chr  \"Transcript\" \"Transcript\" \"Transcript\" \"Transcript\" ...\n $ Feature         : chr  \"ENSMUST00000027032.5\" \"ENSMUST00000027032.5\" \"ENSMUST00000027036.10\" \"ENSMUST00000044369.12\" ...\n $ cDNA_position   : chr  \"3741\" \"3629\" \"\" \"\" ...\n $ CDS_position    : chr  \"3614\" \"3502\" \"\" \"\" ...\n $ Protein_position: chr  \"1205\" \"1168\" \"\" \"\" ...\n $ Amino_acids     : chr  \"N/I\" \"V/F\" \"\" \"\" ...\n $ Codons          : chr  \"aAt/aTt\" \"Gtt/Ttt\" \"\" \"\" ...\n $ NORMAL.REF.AD   : int  122 96 195 61 79 189 83 60 86 77 ...\n $ NORMAL.ALT.AD   : int  0 0 0 0 0 0 0 0 0 0 ...\n $ TUMOR.REF.AD    : int  44 42 79 26 24 58 35 11 24 21 ...\n $ TUMOR.ALT.AD    : int  48 42 64 19 18 57 28 22 31 22 ...\n\ncolnames(read_tsv)\n\n [1] \"CHROM\"            \"POS\"              \"REF\"              \"ALT\"             \n [5] \"set\"              \"NORMAL.GT\"        \"NORMAL.AD\"        \"NORMAL.AF\"       \n [9] \"NORMAL.DP\"        \"TUMOR.GT\"         \"TUMOR.AD\"         \"TUMOR.AF\"        \n[13] \"TUMOR.DP\"         \"Consequence\"      \"SYMBOL\"           \"Feature_type\"    \n[17] \"Feature\"          \"cDNA_position\"    \"CDS_position\"     \"Protein_position\"\n[21] \"Amino_acids\"      \"Codons\"           \"NORMAL.REF.AD\"    \"NORMAL.ALT.AD\"   \n[25] \"TUMOR.REF.AD\"     \"TUMOR.ALT.AD\"",
    "crumbs": [
      "Intro to R",
      "Reading, navigating, and plotting data"
    ]
  },
  {
    "objectID": "course/4_qctrouble.html#nan-and-missing-data",
    "href": "course/4_qctrouble.html#nan-and-missing-data",
    "title": "Reading, navigating, and plotting data",
    "section": "\nNaN and missing data",
    "text": "NaN and missing data\nDealing with missing data and NaN (Not a Number) values is a common challenge in R programming. These values can affect the results of your analyses and visualizations. It’s essential to handle missing data appropriately, either by imputing them or excluding them from the analysis, depending on the context.\nExample:\n\n# Creating a dataframe with missing values\ndf &lt;- data.frame(\n  x = c(1, 2, NA, 4),\n  y = c(5, NA, 7, 8)\n)\n\n# Check for missing values\nsum(is.na(df))\n\n[1] 2\n\n\n\n# Understanding missing or sparse information\nsummary(read_tsv)\n\n    CHROM                POS                REF                ALT           \n Length:10427       Min.   :  2107407   Length:10427       Length:10427      \n Class :character   1st Qu.: 38008521   Class :character   Class :character  \n Mode  :character   Median : 73553346   Mode  :character   Mode  :character  \n                    Mean   : 75250958                                        \n                    3rd Qu.:107567817                                        \n                    Max.   :195169782                                        \n                    NA's   :1                                                \n     set             NORMAL.GT          NORMAL.AD           NORMAL.AF        \n Length:10427       Length:10427       Length:10427       Min.   :0.0000000  \n Class :character   Class :character   Class :character   1st Qu.:0.0000000  \n Mode  :character   Mode  :character   Mode  :character   Median :0.0000000  \n                                                          Mean   :0.0001637  \n                                                          3rd Qu.:0.0000000  \n                                                          Max.   :0.0408200  \n                                                          NA's   :1          \n   NORMAL.DP        TUMOR.GT           TUMOR.AD            TUMOR.AF      \n Min.   :  31.0   Length:10427       Length:10427       Min.   :0.05063  \n 1st Qu.:  67.0   Class :character   Class :character   1st Qu.:0.41860  \n Median :  97.0   Mode  :character   Mode  :character   Median :0.48247  \n Mean   : 118.3                                         Mean   :0.48403  \n 3rd Qu.: 148.0                                         3rd Qu.:0.54217  \n Max.   :1351.0                                         Max.   :1.00000  \n NA's   :2                                              NA's   :1        \n    TUMOR.DP      Consequence           SYMBOL          Feature_type      \n Min.   : 31.00   Length:10427       Length:10427       Length:10427      \n 1st Qu.: 45.00   Class :character   Class :character   Class :character  \n Median : 65.00   Mode  :character   Mode  :character   Mode  :character  \n Mean   : 79.68                                                           \n 3rd Qu.: 98.00                                                           \n Max.   :813.00                                                           \n NA's   :2                                                                \n   Feature          cDNA_position      CDS_position       Protein_position  \n Length:10427       Length:10427       Length:10427       Length:10427      \n Class :character   Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n                                                                            \n                                                                            \n                                                                            \n                                                                            \n Amino_acids           Codons          NORMAL.REF.AD    NORMAL.ALT.AD    \n Length:10427       Length:10427       Min.   :  30.0   Min.   :0.00000  \n Class :character   Class :character   1st Qu.:  67.0   1st Qu.:0.00000  \n Mode  :character   Mode  :character   Median :  97.0   Median :0.00000  \n                                       Mean   : 118.2   Mean   :0.01928  \n                                       3rd Qu.: 148.0   3rd Qu.:0.00000  \n                                       Max.   :1350.0   Max.   :2.00000  \n                                       NA's   :2        NA's   :2        \n  TUMOR.REF.AD     TUMOR.ALT.AD   \n Min.   :  0.00   Min.   :  4.00  \n 1st Qu.: 23.00   1st Qu.: 21.00  \n Median : 34.00   Median : 31.00  \n Mean   : 41.68   Mean   : 37.96  \n 3rd Qu.: 52.00   3rd Qu.: 47.00  \n Max.   :691.00   Max.   :229.00  \n NA's   :2        NA's   :2       \n\nread_tsv[!complete.cases(read_tsv),]\n\n      CHROM      POS   REF  ALT            set   NORMAL.GT NORMAL.AD NORMAL.AF\n9862   chr9 53503887 AACAC  AAC mutect-varscan AACAC/AACAC      &lt;NA&gt;     0.014\n10272  &lt;NA&gt;       NA  &lt;NA&gt; &lt;NA&gt;           &lt;NA&gt;        &lt;NA&gt;      &lt;NA&gt;        NA\n      NORMAL.DP TUMOR.GT TUMOR.AD TUMOR.AF TUMOR.DP    Consequence SYMBOL\n9862         NA  AACAC/.     &lt;NA&gt;    0.184       NA intron_variant    Atm\n10272        NA     &lt;NA&gt;     &lt;NA&gt;       NA       NA           &lt;NA&gt;   &lt;NA&gt;\n      Feature_type              Feature cDNA_position CDS_position\n9862    Transcript ENSMUST00000232179.1                           \n10272         &lt;NA&gt;                 &lt;NA&gt;          &lt;NA&gt;         &lt;NA&gt;\n      Protein_position Amino_acids Codons NORMAL.REF.AD NORMAL.ALT.AD\n9862                                                 NA            NA\n10272             &lt;NA&gt;        &lt;NA&gt;   &lt;NA&gt;            NA            NA\n      TUMOR.REF.AD TUMOR.ALT.AD\n9862            NA           NA\n10272           NA           NA",
    "crumbs": [
      "Intro to R",
      "Reading, navigating, and plotting data"
    ]
  },
  {
    "objectID": "course/4_qctrouble.html#data-organization-and-structure-factors",
    "href": "course/4_qctrouble.html#data-organization-and-structure-factors",
    "title": "Reading, navigating, and plotting data",
    "section": "Data organization and structure: Factors",
    "text": "Data organization and structure: Factors\nColumns that contain strings are automatically read in as character vectors and are arranged alphanumericall. Factors assign a logical order to a series of samples.\n\nlibrary(ggplot2)\n# Organize the chromosomes to the correct order\n# If CHROM is a character vector, the chromosomes are not ordered properly\nstr(read_tsv) # Shows that CHROM variable is a character vector\n\n'data.frame':   10427 obs. of  26 variables:\n $ CHROM           : chr  \"chr1\" \"chr1\" \"chr1\" \"chr1\" ...\n $ POS             : int  4347274 4347386 4830263 5084567 6247967 6249819 6264623 6842242 6842967 6844056 ...\n $ REF             : chr  \"T\" \"C\" \"C\" \"C\" ...\n $ ALT             : chr  \"A\" \"A\" \"T\" \"T\" ...\n $ set             : chr  \"mutect-varscan-strelka\" \"mutect-varscan-strelka\" \"mutect-varscan-strelka\" \"mutect-varscan-strelka\" ...\n $ NORMAL.GT       : chr  \"T/T\" \"C/C\" \"C/C\" \"C/C\" ...\n $ NORMAL.AD       : chr  \"122,0\" \"96,0\" \"195,0\" \"61,0\" ...\n $ NORMAL.AF       : num  0 0 0 0 0 0 0 0 0 0 ...\n $ NORMAL.DP       : int  122 96 195 62 79 189 83 60 86 77 ...\n $ TUMOR.GT        : chr  \"T/A\" \"C/A\" \"C/T\" \"C/T\" ...\n $ TUMOR.AD        : chr  \"44,48\" \"42,42\" \"79,64\" \"26,19\" ...\n $ TUMOR.AF        : num  0.522 0.5 0.448 0.422 0.429 ...\n $ TUMOR.DP        : int  92 84 143 45 42 115 63 33 55 43 ...\n $ Consequence     : chr  \"missense_variant\" \"missense_variant\" \"splice_region_variant&intron_variant\" \"splice_region_variant&intron_variant\" ...\n $ SYMBOL          : chr  \"Rp1\" \"Rp1\" \"Lypla1\" \"Atp6v1h\" ...\n $ Feature_type    : chr  \"Transcript\" \"Transcript\" \"Transcript\" \"Transcript\" ...\n $ Feature         : chr  \"ENSMUST00000027032.5\" \"ENSMUST00000027032.5\" \"ENSMUST00000027036.10\" \"ENSMUST00000044369.12\" ...\n $ cDNA_position   : chr  \"3741\" \"3629\" \"\" \"\" ...\n $ CDS_position    : chr  \"3614\" \"3502\" \"\" \"\" ...\n $ Protein_position: chr  \"1205\" \"1168\" \"\" \"\" ...\n $ Amino_acids     : chr  \"N/I\" \"V/F\" \"\" \"\" ...\n $ Codons          : chr  \"aAt/aTt\" \"Gtt/Ttt\" \"\" \"\" ...\n $ NORMAL.REF.AD   : int  122 96 195 61 79 189 83 60 86 77 ...\n $ NORMAL.ALT.AD   : int  0 0 0 0 0 0 0 0 0 0 ...\n $ TUMOR.REF.AD    : int  44 42 79 26 24 58 35 11 24 21 ...\n $ TUMOR.ALT.AD    : int  48 42 64 19 18 57 28 22 31 22 ...\n\nggplot(read_tsv, aes(x = CHROM)) +\n  geom_bar() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n# make CHROM a factor\nread_tsv$CHROM &lt;- factor(read_tsv$CHROM, levels = paste0(\"chr\", c(1:22, 'X','Y')))\nstr(read_tsv) # shows that CHROM variable is a factor with a set order of character strings\n\n'data.frame':   10427 obs. of  26 variables:\n $ CHROM           : Factor w/ 24 levels \"chr1\",\"chr2\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ POS             : int  4347274 4347386 4830263 5084567 6247967 6249819 6264623 6842242 6842967 6844056 ...\n $ REF             : chr  \"T\" \"C\" \"C\" \"C\" ...\n $ ALT             : chr  \"A\" \"A\" \"T\" \"T\" ...\n $ set             : chr  \"mutect-varscan-strelka\" \"mutect-varscan-strelka\" \"mutect-varscan-strelka\" \"mutect-varscan-strelka\" ...\n $ NORMAL.GT       : chr  \"T/T\" \"C/C\" \"C/C\" \"C/C\" ...\n $ NORMAL.AD       : chr  \"122,0\" \"96,0\" \"195,0\" \"61,0\" ...\n $ NORMAL.AF       : num  0 0 0 0 0 0 0 0 0 0 ...\n $ NORMAL.DP       : int  122 96 195 62 79 189 83 60 86 77 ...\n $ TUMOR.GT        : chr  \"T/A\" \"C/A\" \"C/T\" \"C/T\" ...\n $ TUMOR.AD        : chr  \"44,48\" \"42,42\" \"79,64\" \"26,19\" ...\n $ TUMOR.AF        : num  0.522 0.5 0.448 0.422 0.429 ...\n $ TUMOR.DP        : int  92 84 143 45 42 115 63 33 55 43 ...\n $ Consequence     : chr  \"missense_variant\" \"missense_variant\" \"splice_region_variant&intron_variant\" \"splice_region_variant&intron_variant\" ...\n $ SYMBOL          : chr  \"Rp1\" \"Rp1\" \"Lypla1\" \"Atp6v1h\" ...\n $ Feature_type    : chr  \"Transcript\" \"Transcript\" \"Transcript\" \"Transcript\" ...\n $ Feature         : chr  \"ENSMUST00000027032.5\" \"ENSMUST00000027032.5\" \"ENSMUST00000027036.10\" \"ENSMUST00000044369.12\" ...\n $ cDNA_position   : chr  \"3741\" \"3629\" \"\" \"\" ...\n $ CDS_position    : chr  \"3614\" \"3502\" \"\" \"\" ...\n $ Protein_position: chr  \"1205\" \"1168\" \"\" \"\" ...\n $ Amino_acids     : chr  \"N/I\" \"V/F\" \"\" \"\" ...\n $ Codons          : chr  \"aAt/aTt\" \"Gtt/Ttt\" \"\" \"\" ...\n $ NORMAL.REF.AD   : int  122 96 195 61 79 189 83 60 86 77 ...\n $ NORMAL.ALT.AD   : int  0 0 0 0 0 0 0 0 0 0 ...\n $ TUMOR.REF.AD    : int  44 42 79 26 24 58 35 11 24 21 ...\n $ TUMOR.ALT.AD    : int  48 42 64 19 18 57 28 22 31 22 ...\n\nggplot(read_tsv, aes(x = CHROM)) +\n  geom_bar() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))",
    "crumbs": [
      "Intro to R",
      "Reading, navigating, and plotting data"
    ]
  },
  {
    "objectID": "course/4_qctrouble.html#missing-packages-or-dependencies",
    "href": "course/4_qctrouble.html#missing-packages-or-dependencies",
    "title": "Reading, navigating, and plotting data",
    "section": "Missing packages or dependencies",
    "text": "Missing packages or dependencies\nWhen running R code, you may encounter errors related to missing packages or dependencies. This occurs when you try to use functions or libraries that are not installed on your system. This will commonly look like a red error stating “There is no package…” Installing the required packages using install.packages(“package_name”) can resolve this issue.\n# Trying to use a function from an uninstalled package\nlibrary(ggplot2)\nggplot(df, aes(x, y)) + geom_point()\n# Error: there is no package called 'ggplot2'",
    "crumbs": [
      "Intro to R",
      "Reading, navigating, and plotting data"
    ]
  },
  {
    "objectID": "course/4_qctrouble.html#stack-exchange",
    "href": "course/4_qctrouble.html#stack-exchange",
    "title": "Reading, navigating, and plotting data",
    "section": "Stack Exchange",
    "text": "Stack Exchange\nOnline communities like Stack Exchange, particularly the Stack Overflow platform, are excellent resources for getting help with R programming. You can search for solutions to specific problems or ask questions if you’re facing challenges. If you attempt to Google what you’re trying to accomplish with your dataset, StackExchange often includes responses from others who may have gone through this effort before. With the open crowdsourcing of troubleshooting, these responses that work for others are “upvoted” so that you can try to adapt the code to work for your dataset. Importantly, take care to change the names of variables and file paths in any code that you try to implement from others.",
    "crumbs": [
      "Intro to R",
      "Reading, navigating, and plotting data"
    ]
  },
  {
    "objectID": "course/4_qctrouble.html#chatgpt",
    "href": "course/4_qctrouble.html#chatgpt",
    "title": "Reading, navigating, and plotting data",
    "section": "ChatGPT",
    "text": "ChatGPT\nChatGPT is an AI assistant that can provide guidance and answer questions related to R programming. You can ask for clarification on concepts, debugging assistance, or advice on best practices. If you copy/paste an error into ChatGPT, it often tries to debug without any other preface. However, if you try to explain exactly what your code is trying to accomplish, it can be a helpful way to debug your help.",
    "crumbs": [
      "Intro to R",
      "Reading, navigating, and plotting data"
    ]
  },
  {
    "objectID": "course/4_qctrouble.html#good-practices",
    "href": "course/4_qctrouble.html#good-practices",
    "title": "Reading, navigating, and plotting data",
    "section": "Good practices",
    "text": "Good practices\n##Commenting your code Adding comments to your code is crucial for making it more understandable to yourself and others. Comments provide context and explanations for the code’s functionality, making it easier to troubleshoot and maintain.",
    "crumbs": [
      "Intro to R",
      "Reading, navigating, and plotting data"
    ]
  },
  {
    "objectID": "course/4_qctrouble.html#publicly-accessible-resources",
    "href": "course/4_qctrouble.html#publicly-accessible-resources",
    "title": "Reading, navigating, and plotting data",
    "section": "Publicly accessible resources",
    "text": "Publicly accessible resources\nGithub\nGithub hosts numerous repositories containing R scripts, packages, and projects. Browsing through repositories and contributing to open-source projects can help you learn from others’ code and collaborate with the R community. This can be a direct way to make your code available upon publication, according to journal practices.",
    "crumbs": [
      "Intro to R",
      "Reading, navigating, and plotting data"
    ]
  },
  {
    "objectID": "course/4_qctrouble.html#recreate-a-paper-figure-from-scratch",
    "href": "course/4_qctrouble.html#recreate-a-paper-figure-from-scratch",
    "title": "Reading, navigating, and plotting data",
    "section": "Recreate a Paper Figure from Scratch",
    "text": "Recreate a Paper Figure from Scratch\nHere we will recreate a figure from Schmidt et al (Science 2022):\n\n\n\n\nFirst we will load the data from the supplemental table (download here):\n\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(ggrepel)\n\nlog2_df &lt;- read_excel(\"data/science.abj4008_table_s2.xlsx\")\n\n# Always good practice to check the data to ensure it was loaded correctly:\n# head(log2_df)\n\n# The plot only contains the IL2 results from CRISPRa screens,\n# so we will filter down\nlog2_df_filter &lt;- log2_df %&gt;%\n  filter(Cytokine == \"IL2\", CRISPRa_or_i == \"CRISPRa\")\n\n# Select genes that we want to label based on top LFC\nngenes_label &lt;- 16\nlabel_genes &lt;- log2_df_filter %&gt;%\n  filter(Screen_Version == \"Primary\") %&gt;% # For ease only use primary donor\n  group_by(sign(LFC)) %&gt;% # Group by the sign (pos or neg) lfc\n  arrange(desc(abs(LFC))) %&gt;% # Sort by descending absolute lfc values\n  slice_head(n = ngenes_label) %&gt;% # Take the top n genes\n  pull(Gene) # Pull these out from the data frame\n\n# We need to pivot the Screen_Version variable (which is equivalent to Donor1 and Donor 2) into separate columns\nlog2_df_filter_wide &lt;- log2_df_filter %&gt;%\n  pivot_wider(id_cols = c(\"Gene\", \"Hit_Type\"), names_from = \"Screen_Version\", values_from = \"LFC\") %&gt;%\n  filter(!is.na(Primary), !is.na(CD4_Supplement))\n\n\n# Reorder the factor so that we can plot the hits on top\nlog2_df_filter_wide$Hit_Type &lt;- factor(log2_df_filter_wide$Hit_Type,\n  levels = c(\"Positive Hit\", \"Negative Hit\", \"NA\"),\n  labels = c(\"Positive Hit\", \"Negative Hit\", \"Not a Hit\")\n)\n\n\n# Now we can plot:\nfinal_plot &lt;- log2_df_filter_wide %&gt;%\n  # This will order by the factor so the hits are plotted on top\n  arrange(desc(Hit_Type)) %&gt;%\n  ggplot(aes(x = Primary, y = CD4_Supplement)) +\n  geom_hline(yintercept = 0, linetype = \"dashed\") +\n  geom_vline(xintercept = 0, linetype = \"dashed\") +\n  geom_point(aes(color = Hit_Type)) +\n  stat_density_2d(color = \"black\") +\n  # For this geom text, we filter the original data to only include\n  # genes that we want labeled from the above filtering\n  geom_text_repel(\n    data = filter(log2_df_filter_wide, Gene %in% label_genes),\n    aes(label = Gene, colour = Hit_Type),\n    size = 0.36 * 7, show.legend = F, max.overlaps = Inf\n  ) +\n  scale_colour_manual(values = c(\"Positive Hit\" = \"red\", \"Not a Hit\" = \"grey80\", \"Negative Hit\" = \"blue\")) +\n  theme_bw() +\n  theme(\n    panel.grid = element_blank(),\n    # Place legend inside the plot\n    legend.position = c(0.85, 0.15),\n    # Change vertical spacing between legend items\n    legend.key.height = unit(3, \"mm\"),\n    # Place a box around the legend\n    legend.background = element_rect(fill = NA, color = \"black\")\n  ) +\n  labs(\n    x = \"Donor 1, log2FoldChange\",\n    y = \"Donor 2, log2FoldChange\",\n    title = \"IL-2 CRISPRa Screen\",\n    color = \"Screen Hit\"\n  )\n# We can save our plot if desired\n# ggsave(final_plot, file = \"my_pretty_plot.pdf\", width = 5, height = 4)\n\n# And now view it\nfinal_plot",
    "crumbs": [
      "Intro to R",
      "Reading, navigating, and plotting data"
    ]
  },
  {
    "objectID": "course/4_qctrouble.html#create-a-custom-heatmap",
    "href": "course/4_qctrouble.html#create-a-custom-heatmap",
    "title": "Reading, navigating, and plotting data",
    "section": "Create a Custom Heatmap",
    "text": "Create a Custom Heatmap\nHere we will use the excellent R package patchwork to compose a complex heatmap, that is both clustered and plotted next to relevant sample metadata and dendrograms.\n\nSetup Toy Data\nNOTE: This is not important for understanding how to plot, but kept here in case you want to adjust some of these parameters to see what happens!\n\n# Load necessary libraries\nlibrary(tidyverse)\nlibrary(patchwork)\n\n# Set seed for reproducibility\nset.seed(3)\n\n# Generate gene expression data\ngenes &lt;- paste0(\"Gene\", 1:50)\nsamples &lt;- paste0(\"Sample\", 1:20)\nexpression_data &lt;- matrix(rnorm(50 * 20), nrow = 50, dimnames = list(genes, samples))\n\n# Introduce differential expression for case vs control\nidx_case_genes &lt;- 1:20\nidx_conrol_genes &lt;- 21:40\nidx_case_samples &lt;- 1:10\nidx_control_samples &lt;- 11:20\nexpression_data[idx_case_genes, idx_case_samples] &lt;-\n  expression_data[idx_case_genes, idx_case_samples] + 3\nexpression_data[idx_conrol_genes, idx_control_samples] &lt;-\n  expression_data[idx_conrol_genes, idx_control_samples] - 3\n# Some drug specific effects\nexpression_data[1:5, 1:5] &lt;- expression_data[1:5, 1:5] - 3\nexpression_data[6:10, 6:10] &lt;- expression_data[6:10, 6:10] - 3\nexpression_data[21:25, 11:15] &lt;- expression_data[21:25, 11:15] + 3\nexpression_data[26:30, 16:20] &lt;- expression_data[26:30, 16:20] + 3\n\n# Create some metadata\nmetadata &lt;- data.frame(\n  Sample = samples,\n  Condition = rep(c(\"Case\", \"Control\"), each = 10),\n  Gender = rep(c(\"Male\", \"Female\"), times = 10),\n  Drug = c(rep(\"DrugA\", 5), rep(\"DrugB\", 5), rep(\"DrugC\", 5), rep(\"DrugD\", 5))\n)\n\nNow we have a matrix of gene expression values:\n\n\n\n\n\n\nAnd associated metadata:\n\n\n\n\n\n\nLet’s reshape our data into long format to make it possible to plot with ggplot2:\n\n# Reshape the data into long format for ggplot2. Here we use the tidyverse\n# family of packages but there are many ways to approach this reshaping of data\nexpression_long &lt;- expression_data %&gt;%\n  # Convert the matrix to a data frame\n  as.data.frame() %&gt;%\n  # Move the rownames into a column\n  rownames_to_column(\"Gene\") %&gt;%\n  # Here we pivot the table, all columns except for the gene column\n  pivot_longer(cols = !Gene, names_to = \"Sample\", values_to = \"Expression\")\n\nNow we have a data frame with each row representing the expression of a single gene from a single sample:\n\n\n\n\n\n\nWe plot using the tile geom from ggplot2.\n\nheatmap_plot &lt;- ggplot(expression_long, aes(x = Sample, y = Gene, fill = Expression)) +\n  geom_tile() +\n  scale_fill_gradient2(low = \"blue\", high = \"red\", mid = \"white\", midpoint = 0) +\n  guides(x = guide_axis(angle = 90)) +\n  labs(x = \"Samples\", y = \"Genes\") +\n  theme(panel.background = element_blank())\nheatmap_plot\n\n\n\n\n\n\n\nPerform clustering\nIt seems there are some patterns in the data, but at this point the samples and genes are not yet clustered.\nTo view the structure in the data we will first cluster on samples:\n\n# Perform hierarchical clustering:\n## 1. Compute a sample-wise distance matrix\ndist_matrix_samples &lt;- dist(t(scale(expression_data)), method = \"euclidean\")\n## 2. Perform hierarchical clustering\nhclust_samples &lt;- hclust(dist_matrix_samples, method = \"ward.D2\")\n## 3. Pull out the order of the samples from the hclust object\nordered_samples &lt;- hclust_samples$labels[hclust_samples$order]\n\nAnd then cluster on genes. Note that here we don’t t() to transpose the matrix, and this cluster on genes instead of samples. These functions expect the variable being clustered to be in the rows of the corresponding matrix.\n\n# Perform hierarchical clustering:\n## 1. Compute a sample-wise distance matrix\ndist_matrix_genes &lt;- dist(scale(expression_data), method = \"euclidean\")\n## 2. Perform hierarchical clustering\nhclust_genes &lt;- hclust(dist_matrix_genes, method = \"ward.D2\")\n## 3. Pull out the order of the genes from the hclust object\nordered_genes &lt;- hclust_genes$labels[hclust_genes$order]\n\nNow we can reorder the input data as we plot using fct_relevel()\n\n# Plot heatmap using ggplot2\nheatmap_plot &lt;- ggplot(expression_long, aes(x = fct_relevel(Sample, ordered_samples), y = fct_relevel(Gene, ordered_genes), fill = Expression)) +\n  geom_tile() +\n  scale_fill_gradient2(low = \"blue\", high = \"red\", mid = \"white\", midpoint = 0) +\n  labs(title = NULL, x = \"Samples\", y = \"Genes\") +\n  guides(x = guide_axis(angle = 90)) +\n  theme(\n    axis.line = element_blank(),\n    panel.background = element_blank(),\n    plot.margin = margin(t = 0, r = 3, b = 3, l = 3, unit = \"mm\")\n  )\nheatmap_plot\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhat does plot.margin do?\n\n\n\n\n\nTo improve aesthetics, we carefully control the spacing around plots using the theme element plot.margin. This option is specified by using a function called margin() and tells ggplot how much space to leave around your plots:\nplot +\n  theme(\n    plot.margin = margin(t = 0, r = 3, b = 3, l = 3, unit = \"mm\")\n  )\n\n\n\nMetadata plot\nCreate the metadata plot in ggplot.\n\nlibrary(ggnewscale)\nlibrary(ggsci)\n\n# Options for legend width and heights\nguide_kwd &lt;- 0.5\nguide_kht &lt;- 0.5\n# Create metadata plot\nmetadata_plot &lt;- ggplot(metadata, aes(x = fct_relevel(Sample, ordered_samples))) +\n  geom_tile(aes(y = \"Condition\", fill = Condition), color = \"black\") +\n  # Here we tell the order of the legends\n  scale_fill_nejm(guide = guide_legend(\n    order = 1,\n    keywidth = guide_kwd,\n    keyheight = guide_kht\n  )) +\n  ggnewscale::new_scale_fill() +\n  geom_tile(aes(y = \"Drug\", fill = Drug), color = \"black\") +\n  scale_fill_aaas(guide = guide_legend(\n    order = 2,\n    keywidth = guide_kwd,\n    keyheight = guide_kht\n  )) +\n  ggnewscale::new_scale_fill() +\n  geom_tile(aes(y = \"Gender\", fill = Gender), color = \"black\") +\n  scale_fill_npg(guide = guide_legend(\n    order = 2,\n    keywidth = guide_kwd,\n    keyheight = guide_kht\n  )) +\n  # Limits set the order of the main panel\n  scale_y_discrete(\n    limits = c(\"Condition\", \"Drug\", \"Gender\"),\n    expand = c(0, 0)\n  ) +\n  # Adjust theme elements to remove redundant text etc\n  theme(\n    axis.line = element_blank(),\n    panel.background = element_blank(),\n    legend.position = \"right\"\n  ) +\n  labs(x = NULL, y = NULL) +\n  guides(x = guide_axis(angle = 90))\n\nmetadata_plot + coord_fixed() # We add coord fixed just to fit with legend\n\n\n\n\n\n\n\n\n\n\n\n\n\nWhy do we use ggnewscale?\n\n\n\n\n\nWe are using a package called ggnewscale to specify three distinct color scales for the seperate geom_tile() metadata rows. We then set guide=... within the geom_tile() function to specify the order of the legends. If we didn’t do this, the legends would be merged across all metadata categories, which we don’t want.\n\n\n\nMerge the metadata and heatmap plots together\n\n# Compute relative heights of main and metadata plot\nrel_height &lt;- nrow(expression_data) / 3 # Since we have 3 metadata categories\n\n# Adjust the formatting to adjust the plot margin, remove the axis text and ticks\nmetadata_plot &lt;- metadata_plot +\n  theme(\n    axis.ticks.x = element_blank(), axis.text.x = element_blank(),\n    plot.margin = margin(t = 0, r = 3, b = 0, l = 3, unit = \"mm\")\n  )\n# Combine plots with patchwork\ncombined_plot &lt;- metadata_plot + heatmap_plot +\n  plot_layout(heights = c(1, rel_height), ncol = 1, guides = \"collect\")\n\n# Print the combined plot\ncombined_plot\n\n\n\n\n\n\n\nAdd Dendrograms\nNow we will use ggdendro to add dendrograms in the margins.\n\n\n\n\n\n\nTip\n\n\n\n\n\nAnother good package for plotting ‘tree’ (aka hierarchical trees) data this is ggtree.\n\n\n\n\nlibrary(ggdendro)\ndendro_top &lt;- ggdendrogram(hclust_samples, rotate = FALSE, theme_dendro = TRUE) +\n  # This expands the y axis to fill the full panel below, and 5% extra space above\n  scale_y_continuous(expand = expansion(mult = c(0, 0.05))) +\n  # We need to null the x to remove the space\n  # between the dendrogram and the plot\n  labs(x = NULL, y = NULL) +\n  theme(\n    axis.text.y = element_blank(),\n    axis.text.x = element_blank()\n  )\n# Layout\ndendro_top + metadata_plot + plot_layout(ncol = 1, heights = c(0.5, 1)) &\n  scale_x_discrete(expand = expansion(mult = c(0.05, 0.05)))\n\n\n\n\n\n\n\nLikewise for genes:\n\ndendro_side &lt;- ggdendrogram(hclust_genes, rotate = TRUE, theme_dendro = TRUE) +\n  # This expands the x axis to fill the full panel below, and 5% extra space above\n  scale_y_continuous(expand = expansion(mult = c(0, 0.05))) +\n  # We need to null the x to remove the space\n  # between the dendrogram and the plot\n  labs(x = NULL, y = NULL) +\n  scale_x_discrete(expand = expansion(mult = c(0.01, 0.01))) +\n  theme(\n    axis.text.x = element_blank(), axis.text.y.left = element_blank(),\n    plot.margin = margin(t = 0, r = 3, b = 0, l = 0, unit = \"mm\")\n  )\n# Layout\n(heatmap_plot + scale_y_discrete(expand = expansion(mult = c(0, 0)))) + dendro_side + plot_layout(nrow = 1, widths = c(1, 0.25), guides = \"collect\", axes = \"collect\")\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlot alignment tip using expansion():\n\n\n\n\n\nWhat is scale_y_continuous(expand = expansion(mult = c(0, 0.05))) doing?\nWe use this to adjust the total space the given axis takes up in the panel – either stretching the axis to fill the plot, or leave a slight gap either below/above (for the y-axis) or left/right (for the x-axis). See the function documentation for more explanation and examples.\nSo why do we need this? This is needed because we want to ensure the dendrogram is aligned with the main heatmap plot. In this case it required a bit of tweaking to get the values correct. Feel free to adjust the values or remove these adjustments entirely to see what happens!\n\n\n\nPut it all together\nNow we need to combine all of these plots together. We think of each plot as a patch in a grid, and also use the plot_spacer() function from patchwork to fill in the empty gap in this figure.\n\n# To help we are going to customize each plot margin now to make sure things fit nicely\n# Note the default for margin is 0 space on all sides\ndendro_top &lt;- dendro_top + theme(plot.margin = margin())\nmetadata_plot &lt;- metadata_plot + theme(plot.margin = margin())\nheatmap_plot &lt;- heatmap_plot + theme(plot.margin = margin())\ndendro_side &lt;- dendro_side + theme(plot.margin = margin())\n\n# The default plot spacer has too much space for our purposes so we set\n# plot.margin to be 0\nps &lt;- plot_spacer() + theme(plot.margin = margin())\n\n# Plot the plot!\ndendro_top + ps +\n  metadata_plot + ps +\n  heatmap_plot + dendro_side +\n  plot_layout(ncol = 2, widths = c(1, 0.25), heights = c(1, 1, rel_height), guides = \"collect\")\n\n\n\n\n\n\n\nBonus Challenge\nCan you add a boxplot summarizing the mean expression of each gene to the right of this plot (in between the dendrogram and the main heatmap)? Think carefully about which data should be input to create this plot. Give it a try before opening up the solution below:\n\n\n\n\n\n\nBonus Challenge Solution\n\n\n\n\n\nFirst create the boxplot using the long form expression data. Note that because the boxplot is going to be horizontal, the x-axis represents the numeric expression values, while the y-axis represents the individual genes. Also, don’t forget to reorder the genes\n\ngenexp_boxplot &lt;- ggplot(expression_long, aes(x = Expression, y = fct_relevel(Gene, ordered_genes))) +\n  geom_boxplot() +\n  geom_vline(xintercept = 0, linetype = \"dotted\") +\n  theme(\n    panel.background = element_blank(),\n    panel.border = element_rect(fill = NA)\n  ) +\n  labs(x = \"Expression\", y = NULL)\ngenexp_boxplot\n\n\n\n\n\n\n\nNow we add this into our big patchwork. Note: we also have to add extra spacers since our patchwork has an extra column. Finally, we use the free() function from patchwork to make sure our expression boxplot label is ‘free’ to move next to the plot. Remove this piece of code to see what happens to the label.\n\n# To help we are going to customize each plot margin now to make sure things fit nicely\n# Note the default for margin is 0 space on all sides\ndendro_top &lt;- dendro_top + theme(plot.margin = margin())\nmetadata_plot &lt;- metadata_plot + theme(plot.margin = margin())\nheatmap_plot &lt;- heatmap_plot + theme(plot.margin = margin())\ndendro_side &lt;- dendro_side + theme(plot.margin = margin())\n# Also remove the axis text. We kept to verify the gene ordering\ngenexp_boxplot &lt;- genexp_boxplot + theme(\n  axis.text.y = element_blank(),\n  axis.ticks.y = element_blank(),\n  plot.margin = margin()\n)\n# The default plot spacer has too much space for our purposes so we set\n# plot.margin to be 0\nps &lt;- plot_spacer() + theme(plot.margin = margin())\n\n# Plot the plot!\ndendro_top + ps + ps +\n  metadata_plot + ps + ps +\n  heatmap_plot + free(genexp_boxplot, type = \"label\", side = \"b\") + dendro_side +\n  plot_layout(ncol = 3, widths = c(1, 0.25, 0.25), heights = c(1, 1, rel_height), guides = \"collect\")\n\n\n\n\n\n\n\n\n\n\nOther options\nAnother great package for composing complex heatmaps is the aptly named ComplexHeatmap. While some like the customization and syntax of ggplot2, ComplexHeatmap can accomplish this figure with arguably far less effort. See the example below using the same code:\n\nlibrary(ComplexHeatmap)\n\n# Note that ComplexHeatmap uses the matrix directly\nHeatmap(\n  matrix = expression_data,\n  name = \"Expression\",\n  clustering_method_columns = \"ward.D2\",\n  clustering_method_rows = \"ward.D2\"\n)\n\n\n\n\n\n\n\nAdd the column annotation following the instructions provided by the ComplexHeatmap authors here.\n\n# Expects sample names in rows\nmeta_ht &lt;- metadata %&gt;%\n  column_to_rownames(\"Sample\")\n\n# Set up the colors\nmeta_cols &lt;- list(\n  \"Condition\" = c(\n    \"Case\" = \"#BC3C29FF\",\n    \"Control\" = \"#0072B5FF\"\n  ),\n  \"Drug\" = c(\n    \"DrugA\" = \"#3B4992FF\",\n    \"DrugB\" = \"#EE0000FF\",\n    \"DrugC\" = \"#008B45FF\",\n    \"DrugD\" = \"#631879FF\"\n  ),\n  \"Sex\" = c(\n    \"Male\" = \"#E64B35FF\",\n    \"Female\" = \"#4DBBD5FF\"\n  )\n)\n\ntop_anno &lt;- HeatmapAnnotation(\n  df = meta_ht,\n  col = meta_cols\n)\n\nHeatmap(\n  matrix = expression_data,\n  name = \"Expression\",\n  clustering_method_columns = \"ward.D2\",\n  clustering_method_rows = \"ward.D2\",\n  top_annotation = top_anno\n)\n\n\n\n\n\n\n\nAdd the boxplot\n\nright_anno &lt;- HeatmapAnnotation(Expression = anno_boxplot(expression_data), which = \"row\")\n\nHeatmap(\n  matrix = expression_data,\n  name = \"Expression\",\n  clustering_method_columns = \"ward.D2\",\n  clustering_method_rows = \"ward.D2\",\n  top_annotation = top_anno,\n  right_annotation = right_anno\n)\n\n\n\n\n\n\n\nThere are many additional parameters and ways to customize ComplexHeatmap plots. Refer to the extensive documentation. In many cases it may be easier to see if you can achieve the type of plot you want with ComplexHeatmap before attempting one with ggplot2. It may help with more rapid data exploration as well since it tends to take far less code to get to a quality figure.",
    "crumbs": [
      "Intro to R",
      "Reading, navigating, and plotting data"
    ]
  },
  {
    "objectID": "course/4_qctrouble.html#codefights",
    "href": "course/4_qctrouble.html#codefights",
    "title": "Reading, navigating, and plotting data",
    "section": "CodeFights",
    "text": "CodeFights\nCodeFights (now CodeSignal) offers coding challenges and exercises in R and other programming languages. Practicing coding problems can help reinforce your skills and improve your problem-solving abilities.",
    "crumbs": [
      "Intro to R",
      "Reading, navigating, and plotting data"
    ]
  },
  {
    "objectID": "course/3_clustering.html",
    "href": "course/3_clustering.html",
    "title": "Clustering concepts and correlation",
    "section": "",
    "text": "Understanding distance metrics is critical for clustering and dimensionality reduction because these techniques heavily rely on measuring the similarity or dissimilarity between data points. Distance metrics define how “close” or “far” two points are in the feature space, and they play a crucial role in determining the grouping of similar points in clustering algorithms and the preservation of local and global structures in dimensionality reduction techniques.\n\nManhattan distance, or L1 distance, measures the distance between two points by summing the absolute differences of their Cartesian coordinates. It is equivalent to the total number of moves required to go from one point to another if only axis-aligned moves (up, down, left, right) are allowed, mimicking a city grid.\n\n\nLess influenced by outliers compared to Euclidean distance.\nUseful in high-dimensional spaces.\n\nEuclidean distance, also known as the L2 distance, is the most common metric used to measure the straight-line distance between two points in Euclidean space. It is the default distance measure in many analytical applications.\n\n\nMeasures the shortest path between points.\nSensitive to outliers.\nUsed in default settings for many algorithms like K-means clustering.\n\n\n# Load the iris dataset\ndata(iris)\niris_numeric &lt;- iris[, 1:4] # Exclude the species column\n\n# Euclidean distance\neuclidean_distances &lt;- dist(iris_numeric, method = \"euclidean\")\n\n# Manhattan distance\nmanhattan_distances &lt;- dist(iris_numeric, method = \"manhattan\")",
    "crumbs": [
      "Intro to R",
      "Clustering concepts and correlation"
    ]
  },
  {
    "objectID": "course/3_clustering.html#manhattan-distance",
    "href": "course/3_clustering.html#manhattan-distance",
    "title": "Clustering concepts and correlation",
    "section": "",
    "text": "Manhattan distance, or L1 distance, measures the distance between two points by summing the absolute differences of their Cartesian coordinates. It is equivalent to the total number of moves required to go from one point to another if only axis-aligned moves (up, down, left, right) are allowed, mimicking a city grid.\n\n\nLess influenced by outliers compared to Euclidean distance.\nUseful in high-dimensional spaces.",
    "crumbs": [
      "Intro to R",
      "Clustering concepts and correlation"
    ]
  },
  {
    "objectID": "course/3_clustering.html#euclidean-distance",
    "href": "course/3_clustering.html#euclidean-distance",
    "title": "Clustering concepts and correlation",
    "section": "",
    "text": "Euclidean distance, also known as the L2 distance, is the most common metric used to measure the straight-line distance between two points in Euclidean space. It is the default distance measure in many analytical applications.\n\n\nMeasures the shortest path between points.\nSensitive to outliers.\nUsed in default settings for many algorithms like K-means clustering.",
    "crumbs": [
      "Intro to R",
      "Clustering concepts and correlation"
    ]
  },
  {
    "objectID": "course/3_clustering.html#dist-in-r",
    "href": "course/3_clustering.html#dist-in-r",
    "title": "Clustering concepts and correlation",
    "section": "",
    "text": "# Load the iris dataset\ndata(iris)\niris_numeric &lt;- iris[, 1:4] # Exclude the species column\n\n# Euclidean distance\neuclidean_distances &lt;- dist(iris_numeric, method = \"euclidean\")\n\n# Manhattan distance\nmanhattan_distances &lt;- dist(iris_numeric, method = \"manhattan\")",
    "crumbs": [
      "Intro to R",
      "Clustering concepts and correlation"
    ]
  },
  {
    "objectID": "course/3_clustering.html#k-means-clustering",
    "href": "course/3_clustering.html#k-means-clustering",
    "title": "Clustering concepts and correlation",
    "section": "K-means clustering",
    "text": "K-means clustering\nK-means clustering is a popular unsupervised machine learning algorithm used for partitioning a dataset into a predetermined number of clusters. The goal of k-means clustering is to group data points into clusters such that data points within the same cluster are more similar to each other than to those in other clusters. The algorithm works iteratively to assign each data point to the nearest cluster centroid (center point of a cluster) and then update the centroid based on the mean of all data points assigned to that cluster. This process continues until the centroids no longer change significantly, or a specified number of iterations is reached.\nK-means has some limitations, such as sensitivity to the initial random selection of centroids and the need to specify the number of clusters beforehand. Additionally, k-means may not perform well on datasets with non-spherical or irregularly shaped clusters.\nRunning on K-means on the iris data set:\n\n# Load data\ndata(iris)\niris_data &lt;- iris[, -5]\n\n# Run k-means\nset.seed(123)\nkmeans_result &lt;- kmeans(iris_data, centers = 3, nstart = 20)\n\n# Results\nprint(kmeans_result)\n\nK-means clustering with 3 clusters of sizes 62, 38, 50\n\nCluster means:\n  Sepal.Length Sepal.Width Petal.Length Petal.Width\n1     5.901613    2.748387     4.393548    1.433871\n2     6.850000    3.073684     5.742105    2.071053\n3     5.006000    3.428000     1.462000    0.246000\n\nClustering vector:\n  [1] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n [38] 3 3 3 3 3 3 3 3 3 3 3 3 3 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [75] 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 2 2 2 2 1 2 2 2 2\n[112] 2 2 1 1 2 2 2 2 1 2 1 2 1 2 2 1 1 2 2 2 2 2 1 2 2 2 2 1 2 2 2 1 2 2 2 1 2\n[149] 2 1\n\nWithin cluster sum of squares by cluster:\n[1] 39.82097 23.87947 15.15100\n (between_SS / total_SS =  88.4 %)\n\nAvailable components:\n\n[1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n[6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"      \n\ntable(Cluster = kmeans_result$cluster, Species = iris$Species)\n\n       Species\nCluster setosa versicolor virginica\n      1      0         48        14\n      2      0          2        36\n      3     50          0         0\n\n# Visualization\nlibrary(ggplot2)\nggplot(iris, aes(Sepal.Length, Sepal.Width, color = factor(kmeans_result$cluster))) +\n  geom_point(alpha = 0.5) +\n  labs(color = \"Cluster\") +\n  ggtitle(\"K-means Clustering of the Iris Dataset\")",
    "crumbs": [
      "Intro to R",
      "Clustering concepts and correlation"
    ]
  },
  {
    "objectID": "course/3_clustering.html#heirarchical-clustering",
    "href": "course/3_clustering.html#heirarchical-clustering",
    "title": "Clustering concepts and correlation",
    "section": "Heirarchical Clustering",
    "text": "Heirarchical Clustering\nHierarchical clustering is a method used in unsupervised learning to group similar data points into clusters based on their pairwise distances or similarities. The main idea behind hierarchical clustering is to build a hierarchy of clusters, where each data point starts in its own cluster and pairs of clusters are progressively merged until all points belong to a single cluster.\nThe result of hierarchical clustering is often visualized using a dendrogram, which is a tree-like diagram that illustrates the hierarchical structure of the clusters.\nLets use hclust on a set of TCR sequences, where the distance between each sequence is defined as the edit distance. We can plot a dendrogram highlighting sequence similarity.\n\n# Install and load necessary packages\nif (!requireNamespace(\"stringdist\", quietly = TRUE)) {\n  install.packages(\"stringdist\")\n}\nlibrary(stringdist)\n\n# Define TCR sequences\ntcr_sequences &lt;- c(\"CASSLGTQYEQYF\", \"CASSLGTEAFF\", \"CASSQETQYEQYF\", \"CASSLRTDTQYF\")\nnames(tcr_sequences) &lt;- tcr_sequences # Use sequences as labels\n\n# Calculate pairwise string distances using the Levenshtein method\ndist_matrix &lt;- stringdistmatrix(tcr_sequences, tcr_sequences, method = \"lv\")\n\n# Perform hierarchical clustering using the complete linkage method\nhc &lt;- hclust(as.dist(dist_matrix), method = \"complete\")\n\n# Plot the dendrogram\nplot(hc,\n  main = \"Hierarchical Clustering of TCR Sequences\", sub = \"\", xlab = \"\", ylab = \"Distance\",\n  labels = names(tcr_sequences), hang = -1\n) # Ensure labels hang below the plot\n\n\n\n\n\n\n\nWe can create a more complex simulated dataset of simulated single cell gene expression data. In this case, we have two cell types, and expect that the resulting dendrogram produced from the clustering should show clear differences between these cell types. Finally, we can plot the expression values in heatmap to visualize the difference between the genes across cells. The ordering of the rows is dictated by the dendrogram, drawing more similar cells closer together, allowing us to see the expression patterns that define each cell type.\n\n# Install and load pheatmap if not already installed\nif (!requireNamespace(\"pheatmap\", quietly = TRUE)) {\n  install.packages(\"pheatmap\")\n}\nlibrary(pheatmap)\n\n# Set seed for reproducibility\nset.seed(42)\n\n# Define parameters\nnum_genes &lt;- 100\nnum_samples &lt;- 50 # 25 T cells + 25 Cancer cells\n\n# Simulate base gene expression\ngene_expression &lt;- matrix(rnorm(num_genes * num_samples, mean = 10, sd = 2),\n  nrow = num_genes, ncol = num_samples\n)\n\n# Introduce differences in expression between the two groups\ngene_expression[81:100, 1:25] &lt;- gene_expression[81:100, 1:25] + 2 # T cells\ngene_expression[81:100, 26:50] &lt;- gene_expression[81:100, 26:50] - 2 # Cancer cells\n\n# Label rows and columns\nrownames(gene_expression) &lt;- paste(\"Gene\", 1:num_genes, sep = \"\")\ncolnames(gene_expression) &lt;- c(paste(\"T_Cell\", 1:25, sep = \"\"), paste(\"Cancer_Cell\", 1:25, sep = \"\"))\n\n# Transpose the gene expression matrix\ntransposed_gene_expression &lt;- t(gene_expression)\n\n# Creating a heatmap with clustering and annotation\npheatmap(transposed_gene_expression,\n  show_rownames = FALSE,\n  show_colnames = FALSE,\n  clustering_distance_rows = \"euclidean\",\n  cluster_rows = TRUE,\n  cluster_cols = FALSE,\n  main = \"Heatmap of Gene Expression with Clustering\"\n)",
    "crumbs": [
      "Intro to R",
      "Clustering concepts and correlation"
    ]
  },
  {
    "objectID": "course/3_clustering.html#principal-component-analysis-pca",
    "href": "course/3_clustering.html#principal-component-analysis-pca",
    "title": "Clustering concepts and correlation",
    "section": "Principal Component Analysis (PCA)",
    "text": "Principal Component Analysis (PCA)\nPCA is a widely used dimension reduction technique that transforms high-dimensional data into a lower-dimensional representation by identifying the principal components that capture the maximum variance in the data. These principal components are orthogonal to each other and can be used to visualize the data in lower dimensions.\n\n# Load necessary packages\nlibrary(ggplot2)\n\n# Load data\ndata(iris)\niris_data &lt;- iris[, 1:4]\n\n# PCA\npca_results &lt;- prcomp(iris_data, center = TRUE, scale. = TRUE)\nprint(summary(pca_results))\n\nImportance of components:\n                          PC1    PC2     PC3     PC4\nStandard deviation     1.7084 0.9560 0.38309 0.14393\nProportion of Variance 0.7296 0.2285 0.03669 0.00518\nCumulative Proportion  0.7296 0.9581 0.99482 1.00000\n\n\n\n# Scatter plot of the first two PCs\npc_df &lt;- data.frame(PC1 = pca_results$x[, 1], PC2 = pca_results$x[, 2], Species = iris$Species)\nggplot(pc_df, aes(x = PC1, y = PC2, color = Species)) +\n  geom_point() +\n  labs(\n    title = \"PCA of Iris Dataset\",\n    x = \"Principal Component 1\",\n    y = \"Principal Component 2\"\n  ) +\n  theme_minimal()",
    "crumbs": [
      "Intro to R",
      "Clustering concepts and correlation"
    ]
  },
  {
    "objectID": "course/3_clustering.html#t-sne",
    "href": "course/3_clustering.html#t-sne",
    "title": "Clustering concepts and correlation",
    "section": "t-SNE",
    "text": "t-SNE\nt-SNE (t-Distributed Stochastic Neighbor Embedding) is a dimensionality reduction technique commonly used in bioinformatics to visualize high-dimensional data, such as gene expression profiles or single-cell RNA sequencing (scRNA-seq) data, in a lower-dimensional space. t-SNE aims to preserve local structure and clusterings in the data by modeling similarities between data points in the high-dimensional space and embedding them into a lower-dimensional space. In t-SNE, similarities between data points are represented by conditional probabilities that two points are similar given their high-dimensional representations. t-SNE iteratively adjusts the positions of data points in the lower-dimensional space to minimize the difference between the conditional probabilities of pairwise similarities in the high-dimensional and low-dimensional spaces.\n\n# Load the Rtsne package\nlibrary(Rtsne)\n\nunique_iris &lt;- unique(iris[, 1:4])\n\n# Run t-SNE on the deduplicated data\nset.seed(42) # for reproducibility\ntsne_results &lt;- Rtsne(unique_iris, dims = 2, perplexity = 30, verbose = TRUE)\n\nPerforming PCA\nRead the 149 x 4 data matrix successfully!\nUsing no_dims = 2, perplexity = 30.000000, and theta = 0.500000\nComputing input similarities...\nBuilding tree...\nDone in 0.01 seconds (sparsity = 0.709067)!\nLearning embedding...\nIteration 50: error is 43.497353 (50 iterations in 0.01 seconds)\nIteration 100: error is 43.761732 (50 iterations in 0.02 seconds)\nIteration 150: error is 45.596069 (50 iterations in 0.02 seconds)\nIteration 200: error is 44.171561 (50 iterations in 0.02 seconds)\nIteration 250: error is 45.601410 (50 iterations in 0.02 seconds)\nIteration 300: error is 0.257111 (50 iterations in 0.02 seconds)\nIteration 350: error is 0.127160 (50 iterations in 0.01 seconds)\nIteration 400: error is 0.124022 (50 iterations in 0.01 seconds)\nIteration 450: error is 0.123273 (50 iterations in 0.01 seconds)\nIteration 500: error is 0.123686 (50 iterations in 0.01 seconds)\nIteration 550: error is 0.122445 (50 iterations in 0.01 seconds)\nIteration 600: error is 0.121704 (50 iterations in 0.01 seconds)\nIteration 650: error is 0.120104 (50 iterations in 0.01 seconds)\nIteration 700: error is 0.118276 (50 iterations in 0.01 seconds)\nIteration 750: error is 0.117021 (50 iterations in 0.01 seconds)\nIteration 800: error is 0.116558 (50 iterations in 0.01 seconds)\nIteration 850: error is 0.114659 (50 iterations in 0.01 seconds)\nIteration 900: error is 0.113433 (50 iterations in 0.02 seconds)\nIteration 950: error is 0.114671 (50 iterations in 0.02 seconds)\nIteration 1000: error is 0.115401 (50 iterations in 0.02 seconds)\nFitting performed in 0.30 seconds.\n\n# Create a data frame for plotting (assuming you want to include species labels)\ntsne_data &lt;- data.frame(tsne_results$Y)\n\n# Assuming you want to add back the species information\n# This assumes that species information was not a factor in duplicates\n# If species data was part of the duplication, handle accordingly\nspecies_data &lt;- iris[!duplicated(iris[, 1:4]), \"Species\"]\ntsne_data$Species &lt;- species_data\n\n# Plot the results using ggplot2\nlibrary(ggplot2)\nggplot(tsne_data, aes(x = X1, y = X2, color = Species)) +\n  geom_point(alpha = 0.8) +\n  labs(\n    title = \"Iris dataset t-SNE plot\",\n    x = \"t-SNE 1\", y = \"t-SNE 2\"\n  ) +\n  theme_minimal()",
    "crumbs": [
      "Intro to R",
      "Clustering concepts and correlation"
    ]
  },
  {
    "objectID": "course/3_clustering.html#umap",
    "href": "course/3_clustering.html#umap",
    "title": "Clustering concepts and correlation",
    "section": "UMAP",
    "text": "UMAP\nUMAP (Uniform Manifold Approximation and Projection) is a modern technique for dimensionality reduction that is particularly useful for visualizing clusters or groups in high-dimensional data. Similar to t-SNE, UMAP focuses on preserving the local structure of the data but also tries to retain more of the global structure compared to t-SNE. UMAP is based on manifold learning and operates under the assumption that the data is uniformly distributed on a locally connected Riemannian manifold.\n\n# Install and load umap if not already installed\nif (!requireNamespace(\"umap\", quietly = TRUE)) {\n  install.packages(\"umap\")\n}\n\nlibrary(umap)\nlibrary(ggplot2)\n\n# Load data\ndata(iris)\n\n# Run UMAP\nset.seed(42) # for reproducibility\numap_results &lt;- umap(iris[, 1:4])\n\n# Create a data frame for plotting\niris_umap &lt;- data.frame(umap_results$layout)\niris_umap$Species &lt;- iris$Species\n\nggplot(iris_umap, aes(x = X1, y = X2, color = Species)) +\n  geom_point(alpha = 0.8) +\n  labs(\n    title = \"Iris Dataset UMAP Plot\",\n    x = \"UMAP 1\", y = \"UMAP 2\"\n  ) +\n  theme_minimal()",
    "crumbs": [
      "Intro to R",
      "Clustering concepts and correlation"
    ]
  },
  {
    "objectID": "course/3_clustering.html#spearman-vs.-pearson-correlation",
    "href": "course/3_clustering.html#spearman-vs.-pearson-correlation",
    "title": "Clustering concepts and correlation",
    "section": "Spearman vs. Pearson Correlation",
    "text": "Spearman vs. Pearson Correlation\n\n\nPearson Correlation: Measures the linear relationship between two variables. It assumes that the variables are normally distributed and have a linear relationship.\n\n\n# Generate example data\nset.seed(42)\nx &lt;- rnorm(100) # Generate 100 random numbers from a standard normal distribution\ny &lt;- x + rnorm(100, mean = 0, sd = 0.5) # Create y as a noisy version of x\n\n# Calculate Pearson correlation coefficient\npearson_correlation &lt;- cor(x, y, method = \"pearson\")\nprint(paste(\"Pearson correlation coefficient:\", round(pearson_correlation, 2)))\n\n[1] \"Pearson correlation coefficient: 0.92\"\n\n\n\n\nSpearman Correlation: Measures the monotonic relationship between two variables. It does not assume linearity and is more robust to outliers and non-normal distributions.\n\n\n# Generate example data\nset.seed(42)\nx &lt;- rnorm(100) # Generate 100 random numbers from a standard normal distribution\ny &lt;- x + rnorm(100, mean = 0, sd = 0.5) # Create y as a noisy version of x\n\n# Calculate Spearman correlation coefficient\nspearman_correlation &lt;- cor(x, y, method = \"spearman\")\nprint(paste(\"Spearman correlation coefficient:\", round(spearman_correlation, 2)))\n\n[1] \"Spearman correlation coefficient: 0.9\"",
    "crumbs": [
      "Intro to R",
      "Clustering concepts and correlation"
    ]
  },
  {
    "objectID": "course/3_clustering.html#geom_smooth",
    "href": "course/3_clustering.html#geom_smooth",
    "title": "Clustering concepts and correlation",
    "section": "geom_smooth()",
    "text": "geom_smooth()\nSimple example using iris.\n\nlibrary(ggplot2)\n\n# Create a scatter plot with a smoothed line\nggplot(data = iris, aes(x = Sepal.Length, y = Sepal.Width, color = Species)) +\n  geom_point() + # Add scatter points\n  geom_smooth(method = \"lm\", se = FALSE) + # Add linear regression line\n  labs(title = \"Scatter Plot with Smoothed Line\", x = \"Sepal Length\", y = \"Sepal Width\")\n\n\n\n\n\n\n\nSimulate data points from two groups and use geom_smooth with a different model for each group.\n\n# Load required libraries\nlibrary(ggplot2)\n\n# Generate example data\nset.seed(42)\nn &lt;- 100 # Number of data points per group\nx &lt;- 1:n # x values\ngroup &lt;- rep(c(\"linear\", \"sinusoidal\"), each = n) # Group labels\ny &lt;- c(\n  0.5 * x + rnorm(n, mean = 0, sd = 5), # Group 'linear' with a linear trend\n  2 * sin(seq(0, 2 * pi, length.out = n)) + rnorm(n, mean = 0, sd = 0.5)\n) # Group 'sinusoidal' with a sinusoidal trend\n\n# Standardize y-values within each group\ny &lt;- ave(y, group, FUN = scale)\n\n# Create a data frame\ndf &lt;- data.frame(x = rep(x, 2), y = y, group = rep(group, 2))\n\n# Plot the data with smoothed lines and confidence intervals for each group\nggplot(data = df, aes(x = x, y = y, color = group)) +\n  geom_point() + # Add scatter points\n  geom_smooth(data = subset(df, group == \"linear\"), method = \"lm\", se = TRUE) + # Add linear smoothed line with confidence intervals\n  geom_smooth(data = subset(df, group == \"sinusoidal\"), method = \"loess\", se = TRUE) + # Add sinusoidal smoothed line with confidence intervals\n  labs(title = \"Scatter Plot with Smoothed Lines and Confidence Intervals by Group\", x = \"X\", y = \"Y (Standardized)\") +\n  theme_minimal()",
    "crumbs": [
      "Intro to R",
      "Clustering concepts and correlation"
    ]
  },
  {
    "objectID": "course/3_clustering.html#linear-regression",
    "href": "course/3_clustering.html#linear-regression",
    "title": "Clustering concepts and correlation",
    "section": "Linear Regression",
    "text": "Linear Regression\nStatistical model to estimate the linear relationship between a dependent variable and a set of independent variables. The goal is find the best fit line by fitting the observed data to a linear equation.\n\n# Generate example data with more noise\nset.seed(42)\ndf &lt;- data.frame(\n  x = 1:100, # Independent variable\n  y = 2 * df$x + rnorm(100, mean = 0, sd = 30)\n) # Dependent variable with more noise\n\nmodel &lt;- lm(y ~ x, data = df)\n\n# Visualize the data and fitted line\nggplot(df, aes(x = x, y = y)) +\n  geom_point() + # Add scatter points\n  geom_smooth(method = \"lm\", se = FALSE) + # Add fitted line\n  labs(title = \"Linear Regression Example\", x = \"x\", y = \"y\")",
    "crumbs": [
      "Intro to R",
      "Clustering concepts and correlation"
    ]
  },
  {
    "objectID": "course/3_clustering.html#example-code",
    "href": "course/3_clustering.html#example-code",
    "title": "Clustering concepts and correlation",
    "section": "Example Code",
    "text": "Example Code\n\n# Load required libraries\nlibrary(ggpubr)\n\n# Example dataframe\ndf &lt;- data.frame(\n  Gene1 = c(1, 2, 3, 4, 5),\n  Gene2 = c(5, 4, 3, 2, 1),\n  Gene3 = c(2, 3, 4, 5, 6)\n)\n\n# Perform PCA\npca_result &lt;- prcomp(df)\n\n# Plot PCA\nplot(pca_result$x[,1], pca_result$x[,2], \n     xlab = \"PC1\", ylab = \"PC2\", \n     main = \"PCA Plot\")\n\n\n\n\n\n\n# Perform clustering\n# Example clustering algorithm: k-means\nkmeans_result &lt;- kmeans(df, centers = 2)\n\n# Plot clustering\nplot(df, col = kmeans_result$cluster, \n     main = \"Clustering Plot\")\n\n\n\n\n\n\n# Add correlation statistics to a plot\n# Example plot\nggscatter(df, x = \"Gene1\", y = \"Gene2\", \n          add = \"reg.line\", \n          cor.coef = TRUE, \n          cor.method = \"spearman\", \n          cor.coeff.args = list(method = \"spearman\"))",
    "crumbs": [
      "Intro to R",
      "Clustering concepts and correlation"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CRI Bioinformatics Workshop 2025",
    "section": "",
    "text": "Welcome to the 2025 Cancer Research Institute Bioinformatics Bootcamp!"
  },
  {
    "objectID": "index.html#dates-may-17-22-2025",
    "href": "index.html#dates-may-17-22-2025",
    "title": "CRI Bioinformatics Workshop 2025",
    "section": "Dates: May 17-22, 2025",
    "text": "Dates: May 17-22, 2025"
  },
  {
    "objectID": "index.html#location-wyndham-grand-orlando-resort-bonnet-creek-orlando-florida",
    "href": "index.html#location-wyndham-grand-orlando-resort-bonnet-creek-orlando-florida",
    "title": "CRI Bioinformatics Workshop 2025",
    "section": "Location: Wyndham Grand Orlando Resort Bonnet Creek, Orlando Florida",
    "text": "Location: Wyndham Grand Orlando Resort Bonnet Creek, Orlando Florida\nBonnet Creek Boardroom A/B"
  },
  {
    "objectID": "index.html#course-resources",
    "href": "index.html#course-resources",
    "title": "CRI Bioinformatics Workshop 2025",
    "section": "Course Resources",
    "text": "Course Resources\n\nCourse Overview\nSchedule\nRNAbio companion site\nAdditional Resources"
  },
  {
    "objectID": "course_2024.html",
    "href": "course_2024.html",
    "title": "CRI Bioinformatics Bootcamp 2024",
    "section": "",
    "text": "R Workshop (Day 1, Sat. - Day 2, Sun.)\n\nIntroduction to R\nBasic plotting and statistics\nClustering concepts and correlation\nCommon challenges and additional resources\n\n\n\nBulk RNA sequence analysis (Day 3, Mon.)\n\nIntroduction to DNA and RNA sequencing lecture slides (Obi Griffith and Malachi Griffith)\n\n\nIntroduction to rnabio.org and bulk RNAseq dataset (Lead: Obi/Malachi)\nIntroduction to IGV (Lead: Malachi)\nDifferential Expression Analysis (DESeq2) (Lead: Zach) Code blocks for DE\nDifferential Expression Visualization (DESeq2) (Lead: Zach)\nDifferential Expression Visualization (Advanced R) (Lead: Obi/Charles)\nPathway Analysis (Lead: Obi)\nBatch Correction (Lead: Malachi)\n\n\n\nSingle cell RNA sequencing (Day 4, Tue. - Day 5, Wed.)\n\nIntroduction to scRNA sequencing technology lecture slides (Obi Griffith)\nIntroduction to scRNA sequencing analysis lecture slides (Malachi Griffith)\n\n\nIntroduction to scRNAseq dataset (Lead: Malachi)\nQA/QC and Clustering (Lead: Evelyn)\nCell Type Annotation (Lead: Kelsy)\nDifferential Expression (Lead: Kartik)\nGene Set Enrichment (Lead: Kartik)\nCancer Cell Identification (Lead: Kartik)\nTrajectory Analysis (Lead: Evelyn/Malachi)\nTCR/BCR Repertoire Analysis (Lead: Obi)\n\n\n\nImaging (Day 6, Thu.)\n\nIntro to Python\nSpatial Analysis"
  }
]