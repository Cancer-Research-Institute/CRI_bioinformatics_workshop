{
  "hash": "3d872d5a0d4cdaea4ea754733083a4bf",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Clustering concepts and correlation\"\nformat:\n  html:\n    code-link: true\nengine: knitr\nexecute:\n  warning: false\n---\n\n\n# Distances\n\nUnderstanding distance metrics is critical for clustering and dimensionality reduction because these techniques heavily rely on measuring the similarity or dissimilarity between data points. Distance metrics define how \"close\" or \"far\" two points are in the feature space, and they play a crucial role in determining the grouping of similar points in clustering algorithms and the preservation of local and global structures in dimensionality reduction techniques.\n\n## Manhattan Distance\n\nManhattan distance, or L1 distance, measures the distance between two points by summing the absolute differences of their Cartesian coordinates. It is equivalent to the total number of moves required to go from one point to another if only axis-aligned moves (up, down, left, right) are allowed, mimicking a city grid.\n\n### Considerations\n\n - Less influenced by outliers compared to Euclidean distance.\n - Useful in high-dimensional spaces.\n \n## Euclidean Distance\n\nEuclidean distance, also known as the L2 distance, is the most common metric used to measure the straight-line distance between two points in Euclidean space. It is the default distance measure in many analytical applications.\n\n### Considerations\n\n - Measures the shortest path between points.\n - Sensitive to outliers.\n - Used in default settings for many algorithms like K-means clustering.\n\n## `dist` in R\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load the iris dataset\ndata(iris)\niris_numeric <- iris[, 1:4] # Exclude the species column\n\n# Euclidean distance\neuclidean_distances <- dist(iris_numeric, method = \"euclidean\")\n\n# Manhattan distance\nmanhattan_distances <- dist(iris_numeric, method = \"manhattan\")\n```\n:::\n\n\n\n# Clustering\n\n*Clustering* is a technique used to group similar objects or data points together based on their characteristics or features. For example, clustering can be applied to gene expression data to identify groups of genes that behave similarly under different experimental conditions. These clusters might ultimately inform cell types or subtypes. There are several different clustering techniques, including hierarchical or K-means clustering. It is important to understand the limitations or assumptions of a clustering algorithm when applying it to your data.\n\n## K-means clustering\n\nK-means clustering is a popular unsupervised machine learning algorithm used for partitioning a dataset into a predetermined number of clusters. The goal of k-means clustering is to group data points into clusters such that data points within the same cluster are more similar to each other than to those in other clusters. The algorithm works iteratively to assign each data point to the nearest cluster centroid (center point of a cluster) and then update the centroid based on the mean of all data points assigned to that cluster. This process continues until the centroids no longer change significantly, or a specified number of iterations is reached.\n\nK-means has some limitations, such as sensitivity to the initial random selection of centroids and the need to specify the number of clusters beforehand. Additionally, k-means may not perform well on datasets with non-spherical or irregularly shaped clusters.\n\nRunning on K-means on the iris data set:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load data\ndata(iris)\niris_data <- iris[, -5]\n\n# Run k-means\nset.seed(123)\nkmeans_result <- kmeans(iris_data, centers = 3, nstart = 20)\n\n# Results\nprint(kmeans_result)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nK-means clustering with 3 clusters of sizes 62, 38, 50\n\nCluster means:\n  Sepal.Length Sepal.Width Petal.Length Petal.Width\n1     5.901613    2.748387     4.393548    1.433871\n2     6.850000    3.073684     5.742105    2.071053\n3     5.006000    3.428000     1.462000    0.246000\n\nClustering vector:\n  [1] 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3\n [38] 3 3 3 3 3 3 3 3 3 3 3 3 3 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n [75] 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 2 2 2 2 1 2 2 2 2\n[112] 2 2 1 1 2 2 2 2 1 2 1 2 1 2 2 1 1 2 2 2 2 2 1 2 2 2 2 1 2 2 2 1 2 2 2 1 2\n[149] 2 1\n\nWithin cluster sum of squares by cluster:\n[1] 39.82097 23.87947 15.15100\n (between_SS / total_SS =  88.4 %)\n\nAvailable components:\n\n[1] \"cluster\"      \"centers\"      \"totss\"        \"withinss\"     \"tot.withinss\"\n[6] \"betweenss\"    \"size\"         \"iter\"         \"ifault\"      \n```\n\n\n:::\n\n```{.r .cell-code}\ntable(Cluster = kmeans_result$cluster, Species = iris$Species)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n       Species\nCluster setosa versicolor virginica\n      1      0         48        14\n      2      0          2        36\n      3     50          0         0\n```\n\n\n:::\n\n```{.r .cell-code}\n# Visualization\nlibrary(ggplot2)\nggplot(iris, aes(Sepal.Length, Sepal.Width, color = factor(kmeans_result$cluster))) +\n  geom_point(alpha = 0.5) +\n  labs(color = \"Cluster\") +\n  ggtitle(\"K-means Clustering of the Iris Dataset\")\n```\n\n::: {.cell-output-display}\n![](3_clustering_files/figure-html/unnamed-chunk-2-1.png){width=672}\n:::\n:::\n\n\n\n## Heirarchical Clustering\n\nHierarchical clustering is a method used in unsupervised learning to group similar data points into clusters based on their pairwise distances or similarities. The main idea behind hierarchical clustering is to build a hierarchy of clusters, where each data point starts in its own cluster and pairs of clusters are progressively merged until all points belong to a single cluster.\n\nThe result of hierarchical clustering is often visualized using a dendrogram, which is a tree-like diagram that illustrates the hierarchical structure of the clusters.\n\nLets use hclust on a set of TCR sequences, where the distance between each sequence is defined as the edit distance. We can plot a dendrogram highlighting sequence similarity.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Install and load necessary packages\nif (!requireNamespace(\"stringdist\", quietly = TRUE)) {\n  install.packages(\"stringdist\")\n}\nlibrary(stringdist)\n\n# Define TCR sequences\ntcr_sequences <- c(\"CASSLGTQYEQYF\", \"CASSLGTEAFF\", \"CASSQETQYEQYF\", \"CASSLRTDTQYF\")\nnames(tcr_sequences) <- tcr_sequences # Use sequences as labels\n\n# Calculate pairwise string distances using the Levenshtein method\ndist_matrix <- stringdistmatrix(tcr_sequences, tcr_sequences, method = \"lv\")\n\n# Perform hierarchical clustering using the complete linkage method\nhc <- hclust(as.dist(dist_matrix), method = \"complete\")\n\n# Plot the dendrogram\nplot(hc,\n  main = \"Hierarchical Clustering of TCR Sequences\", sub = \"\", xlab = \"\", ylab = \"Distance\",\n  labels = names(tcr_sequences), hang = -1\n) # Ensure labels hang below the plot\n```\n\n::: {.cell-output-display}\n![](3_clustering_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\nWe can create a more complex simulated dataset of simulated single cell gene expression data. In this case, we have two cell types, and expect that the resulting dendrogram produced from the clustering should show clear differences between these cell types. Finally, we can plot the expression values in heatmap to visualize the difference between the genes across cells. The ordering of the rows is dictated by the dendrogram, drawing more similar cells closer together, allowing us to see the expression patterns that define each cell type.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Install and load pheatmap if not already installed\nif (!requireNamespace(\"pheatmap\", quietly = TRUE)) {\n  install.packages(\"pheatmap\")\n}\nlibrary(pheatmap)\n\n# Set seed for reproducibility\nset.seed(42)\n\n# Define parameters\nnum_genes <- 100\nnum_samples <- 50 # 25 T cells + 25 Cancer cells\n\n# Simulate base gene expression\ngene_expression <- matrix(rnorm(num_genes * num_samples, mean = 10, sd = 2),\n  nrow = num_genes, ncol = num_samples\n)\n\n# Introduce differences in expression between the two groups\ngene_expression[81:100, 1:25] <- gene_expression[81:100, 1:25] + 2 # T cells\ngene_expression[81:100, 26:50] <- gene_expression[81:100, 26:50] - 2 # Cancer cells\n\n# Label rows and columns\nrownames(gene_expression) <- paste(\"Gene\", 1:num_genes, sep = \"\")\ncolnames(gene_expression) <- c(paste(\"T_Cell\", 1:25, sep = \"\"), paste(\"Cancer_Cell\", 1:25, sep = \"\"))\n\n# Transpose the gene expression matrix\ntransposed_gene_expression <- t(gene_expression)\n\n# Creating a heatmap with clustering and annotation\npheatmap(transposed_gene_expression,\n  show_rownames = FALSE,\n  show_colnames = FALSE,\n  clustering_distance_rows = \"euclidean\",\n  cluster_rows = TRUE,\n  cluster_cols = FALSE,\n  main = \"Heatmap of Gene Expression with Clustering\"\n)\n```\n\n::: {.cell-output-display}\n![](3_clustering_files/figure-html/unnamed-chunk-4-1.png){width=576}\n:::\n:::\n\n\n# Dimensionality Reduction\n\nDimension reduction is a crucial step in analyzing high-dimensional data, such as gene expression data, where the number of features (genes) is large compared to the number of samples (experimental conditions). Dimension reduction techniques aim to simplify the data while preserving its essential structure, simplifying hundreds or thousands of features into a two-dimensional space. For clustering cell types or samples, this approach enables the identification of which genes are the most different across groups or clusters.\n\n## Principal Component Analysis (PCA)\n\nPCA is a widely used dimension reduction technique that transforms high-dimensional data into a lower-dimensional representation by identifying the principal components that capture the maximum variance in the data. These principal components are orthogonal to each other and can be used to visualize the data in lower dimensions.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load necessary packages\nlibrary(ggplot2)\n\n# Load data\ndata(iris)\niris_data <- iris[, 1:4]\n\n# PCA\npca_results <- prcomp(iris_data, center = TRUE, scale. = TRUE)\nprint(summary(pca_results))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nImportance of components:\n                          PC1    PC2     PC3     PC4\nStandard deviation     1.7084 0.9560 0.38309 0.14393\nProportion of Variance 0.7296 0.2285 0.03669 0.00518\nCumulative Proportion  0.7296 0.9581 0.99482 1.00000\n```\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n# Scatter plot of the first two PCs\npc_df <- data.frame(PC1 = pca_results$x[, 1], PC2 = pca_results$x[, 2], Species = iris$Species)\nggplot(pc_df, aes(x = PC1, y = PC2, color = Species)) +\n  geom_point() +\n  labs(\n    title = \"PCA of Iris Dataset\",\n    x = \"Principal Component 1\",\n    y = \"Principal Component 2\"\n  ) +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](3_clustering_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\n\n## t-SNE\n\n\nt-SNE (t-Distributed Stochastic Neighbor Embedding) is a dimensionality reduction technique commonly used in bioinformatics to visualize high-dimensional data, such as gene expression profiles or single-cell RNA sequencing (scRNA-seq) data, in a lower-dimensional space. t-SNE aims to preserve local structure and clusterings in the data by modeling similarities between data points in the high-dimensional space and embedding them into a lower-dimensional space. In t-SNE, similarities between data points are represented by conditional probabilities that two points are similar given their high-dimensional representations. t-SNE iteratively adjusts the positions of data points in the lower-dimensional space to minimize the difference between the conditional probabilities of pairwise similarities in the high-dimensional and low-dimensional spaces.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load the Rtsne package\nlibrary(Rtsne)\n\nunique_iris <- unique(iris[, 1:4])\n\n# Run t-SNE on the deduplicated data\nset.seed(42) # for reproducibility\ntsne_results <- Rtsne(unique_iris, dims = 2, perplexity = 30, verbose = TRUE)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nPerforming PCA\nRead the 149 x 4 data matrix successfully!\nUsing no_dims = 2, perplexity = 30.000000, and theta = 0.500000\nComputing input similarities...\nBuilding tree...\nDone in 0.01 seconds (sparsity = 0.709067)!\nLearning embedding...\nIteration 50: error is 43.497353 (50 iterations in 0.01 seconds)\nIteration 100: error is 43.761732 (50 iterations in 0.02 seconds)\nIteration 150: error is 45.596069 (50 iterations in 0.02 seconds)\nIteration 200: error is 44.171561 (50 iterations in 0.02 seconds)\nIteration 250: error is 45.601410 (50 iterations in 0.02 seconds)\nIteration 300: error is 0.257111 (50 iterations in 0.02 seconds)\nIteration 350: error is 0.127160 (50 iterations in 0.01 seconds)\nIteration 400: error is 0.124022 (50 iterations in 0.01 seconds)\nIteration 450: error is 0.123273 (50 iterations in 0.01 seconds)\nIteration 500: error is 0.123686 (50 iterations in 0.01 seconds)\nIteration 550: error is 0.122445 (50 iterations in 0.01 seconds)\nIteration 600: error is 0.121704 (50 iterations in 0.01 seconds)\nIteration 650: error is 0.120104 (50 iterations in 0.01 seconds)\nIteration 700: error is 0.118276 (50 iterations in 0.01 seconds)\nIteration 750: error is 0.117021 (50 iterations in 0.01 seconds)\nIteration 800: error is 0.116558 (50 iterations in 0.01 seconds)\nIteration 850: error is 0.114659 (50 iterations in 0.01 seconds)\nIteration 900: error is 0.113433 (50 iterations in 0.02 seconds)\nIteration 950: error is 0.114671 (50 iterations in 0.02 seconds)\nIteration 1000: error is 0.115401 (50 iterations in 0.02 seconds)\nFitting performed in 0.30 seconds.\n```\n\n\n:::\n\n```{.r .cell-code}\n# Create a data frame for plotting (assuming you want to include species labels)\ntsne_data <- data.frame(tsne_results$Y)\n\n# Assuming you want to add back the species information\n# This assumes that species information was not a factor in duplicates\n# If species data was part of the duplication, handle accordingly\nspecies_data <- iris[!duplicated(iris[, 1:4]), \"Species\"]\ntsne_data$Species <- species_data\n\n# Plot the results using ggplot2\nlibrary(ggplot2)\nggplot(tsne_data, aes(x = X1, y = X2, color = Species)) +\n  geom_point(alpha = 0.8) +\n  labs(\n    title = \"Iris dataset t-SNE plot\",\n    x = \"t-SNE 1\", y = \"t-SNE 2\"\n  ) +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](3_clustering_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\n## UMAP\n\nUMAP (Uniform Manifold Approximation and Projection) is a modern technique for dimensionality reduction that is particularly useful for visualizing clusters or groups in high-dimensional data. Similar to t-SNE, UMAP focuses on preserving the local structure of the data but also tries to retain more of the global structure compared to t-SNE. UMAP is based on manifold learning and operates under the assumption that the data is uniformly distributed on a locally connected Riemannian manifold.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Install and load umap if not already installed\nif (!requireNamespace(\"umap\", quietly = TRUE)) {\n  install.packages(\"umap\")\n}\n\nlibrary(umap)\nlibrary(ggplot2)\n\n# Load data\ndata(iris)\n\n# Run UMAP\nset.seed(42) # for reproducibility\numap_results <- umap(iris[, 1:4])\n\n# Create a data frame for plotting\niris_umap <- data.frame(umap_results$layout)\niris_umap$Species <- iris$Species\n\nggplot(iris_umap, aes(x = X1, y = X2, color = Species)) +\n  geom_point(alpha = 0.8) +\n  labs(\n    title = \"Iris Dataset UMAP Plot\",\n    x = \"UMAP 1\", y = \"UMAP 2\"\n  ) +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](3_clustering_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\n# Correlation\n\nCorrelation measures the strength and direction of the relationship between two variables. In bioinformatics, correlation analysis is often used to explore relationships between gene expression levels across different samples or experimental conditions.\n\n## Spearman vs. Pearson Correlation\n\n- **Pearson Correlation**: Measures the linear relationship between two variables. It assumes that the variables are normally distributed and have a linear relationship.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Generate example data\nset.seed(42)\nx <- rnorm(100) # Generate 100 random numbers from a standard normal distribution\ny <- x + rnorm(100, mean = 0, sd = 0.5) # Create y as a noisy version of x\n\n# Calculate Pearson correlation coefficient\npearson_correlation <- cor(x, y, method = \"pearson\")\nprint(paste(\"Pearson correlation coefficient:\", round(pearson_correlation, 2)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Pearson correlation coefficient: 0.92\"\n```\n\n\n:::\n:::\n\n\n\n- **Spearman Correlation**: Measures the monotonic relationship between two variables. It does not assume linearity and is more robust to outliers and non-normal distributions.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Generate example data\nset.seed(42)\nx <- rnorm(100) # Generate 100 random numbers from a standard normal distribution\ny <- x + rnorm(100, mean = 0, sd = 0.5) # Create y as a noisy version of x\n\n# Calculate Spearman correlation coefficient\nspearman_correlation <- cor(x, y, method = \"spearman\")\nprint(paste(\"Spearman correlation coefficient:\", round(spearman_correlation, 2)))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"Spearman correlation coefficient: 0.9\"\n```\n\n\n:::\n:::\n\n\n## `geom_smooth()`\n\nSimple example using iris.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggplot2)\n\n# Create a scatter plot with a smoothed line\nggplot(data = iris, aes(x = Sepal.Length, y = Sepal.Width, color = Species)) +\n  geom_point() + # Add scatter points\n  geom_smooth(method = \"lm\", se = FALSE) + # Add linear regression line\n  labs(title = \"Scatter Plot with Smoothed Line\", x = \"Sepal Length\", y = \"Sepal Width\")\n```\n\n::: {.cell-output-display}\n![](3_clustering_files/figure-html/unnamed-chunk-11-1.png){width=672}\n:::\n:::\n\n\nSimulate data points from two groups and use geom_smooth with a different model for each group.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load required libraries\nlibrary(ggplot2)\n\n# Generate example data\nset.seed(42)\nn <- 100 # Number of data points per group\nx <- 1:n # x values\ngroup <- rep(c(\"linear\", \"sinusoidal\"), each = n) # Group labels\ny <- c(\n  0.5 * x + rnorm(n, mean = 0, sd = 5), # Group 'linear' with a linear trend\n  2 * sin(seq(0, 2 * pi, length.out = n)) + rnorm(n, mean = 0, sd = 0.5)\n) # Group 'sinusoidal' with a sinusoidal trend\n\n# Standardize y-values within each group\ny <- ave(y, group, FUN = scale)\n\n# Create a data frame\ndf <- data.frame(x = rep(x, 2), y = y, group = rep(group, 2))\n\n# Plot the data with smoothed lines and confidence intervals for each group\nggplot(data = df, aes(x = x, y = y, color = group)) +\n  geom_point() + # Add scatter points\n  geom_smooth(data = subset(df, group == \"linear\"), method = \"lm\", se = TRUE) + # Add linear smoothed line with confidence intervals\n  geom_smooth(data = subset(df, group == \"sinusoidal\"), method = \"loess\", se = TRUE) + # Add sinusoidal smoothed line with confidence intervals\n  labs(title = \"Scatter Plot with Smoothed Lines and Confidence Intervals by Group\", x = \"X\", y = \"Y (Standardized)\") +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](3_clustering_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n\n## Linear Regression\n\nStatistical model to estimate the linear relationship between a dependent variable and a set of independent variables. The goal is find the best fit line by fitting the observed data to a linear equation.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Generate example data with more noise\nset.seed(42)\ndf <- data.frame(\n  x = 1:100, # Independent variable\n  y = 2 * df$x + rnorm(100, mean = 0, sd = 30)\n) # Dependent variable with more noise\n\nmodel <- lm(y ~ x, data = df)\n\n# Visualize the data and fitted line\nggplot(df, aes(x = x, y = y)) +\n  geom_point() + # Add scatter points\n  geom_smooth(method = \"lm\", se = FALSE) + # Add fitted line\n  labs(title = \"Linear Regression Example\", x = \"x\", y = \"y\")\n```\n\n::: {.cell-output-display}\n![](3_clustering_files/figure-html/unnamed-chunk-13-1.png){width=672}\n:::\n:::\n\n\n\n## Example Code\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load required libraries\nlibrary(ggpubr)\n\n# Example dataframe\ndf <- data.frame(\n  Gene1 = c(1, 2, 3, 4, 5),\n  Gene2 = c(5, 4, 3, 2, 1),\n  Gene3 = c(2, 3, 4, 5, 6)\n)\n\n# Perform PCA\npca_result <- prcomp(df)\n\n# Plot PCA\nplot(pca_result$x[,1], pca_result$x[,2], \n     xlab = \"PC1\", ylab = \"PC2\", \n     main = \"PCA Plot\")\n```\n\n::: {.cell-output-display}\n![](3_clustering_files/figure-html/unnamed-chunk-14-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# Perform clustering\n# Example clustering algorithm: k-means\nkmeans_result <- kmeans(df, centers = 2)\n\n# Plot clustering\nplot(df, col = kmeans_result$cluster, \n     main = \"Clustering Plot\")\n```\n\n::: {.cell-output-display}\n![](3_clustering_files/figure-html/unnamed-chunk-14-2.png){width=672}\n:::\n\n```{.r .cell-code}\n# Add correlation statistics to a plot\n# Example plot\nggscatter(df, x = \"Gene1\", y = \"Gene2\", \n          add = \"reg.line\", \n          cor.coef = TRUE, \n          cor.method = \"spearman\", \n          cor.coeff.args = list(method = \"spearman\"))\n```\n\n::: {.cell-output-display}\n![](3_clustering_files/figure-html/unnamed-chunk-14-3.png){width=672}\n:::\n:::\n\n\n# Additional Exercises\n\n```R\n# Run PCA, t-SNE, and UMAP on the simulated gene expression data.\n# CODE YOUR ANSWER HERE\n\n# Run pheatmap again on the expression dataset, apply k_means clustering using 2, 10, 50 k's and view how clusters change\n# Hint! type ?pheatmap to view function documentation in R\n# CODE YOUR ANSWER HERE\n\n# Save the pheatmap object to a variable, can you find which genes correspond to which clusters (use \"$\" to access object data)\n# what cluster does Gene 5 correspond to\n# CODE YOUR ANSWER HERE\n```\n\n# Bonus Exercises: Heatmaps using ggplot2\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(ggdendro)\nlibrary(ggplot2)\nlibrary(tidyr)\nlibrary(tibble)\n\n# Manually perform hierarchical clustering to generate heatmap using ggplot2\n# Step 1. Perform distance functions\ngene_distances <- dist(gene_expression, method = 'euclidean')\ncell_distances <- dist(transposed_gene_expression, method = 'euclidean')\n\n# Step 2. Perform hierarchical clustering\ngene_clust <- hclust(gene_distances, method = 'complete')\ncell_clust <- hclust(cell_distances, method = 'complete')\n\n# Plot 1. Dendrograms\nggdendrogram(gene_clust, rotate = FALSE, size = 2)\n```\n\n::: {.cell-output-display}\n![](3_clustering_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n\n```{.r .cell-code}\nggdendrogram(cell_clust, rotate = FALSE, size = 2)\n```\n\n::: {.cell-output-display}\n![](3_clustering_files/figure-html/unnamed-chunk-15-2.png){width=672}\n:::\n\n```{.r .cell-code}\n# Plot 2. Heatmap in ggplot2\ngene_expression_df <- as.data.frame(gene_expression)\n# Add column with gene name to data frame (originally rownames)\ngene_expression_df <- rownames_to_column(gene_expression_df, \"Gene\")\n# Prepare for plotting\nfor_plotting <- gene_expression_df %>%\n  gather(Cell, expression_value, -Gene)\n\n# Code for plotting\nggplot(for_plotting, aes(x = Gene, y = Cell, fill = expression_value)) +\n  geom_tile() +\n  theme_bw() +\n  theme(axis.text = element_blank(),\n        axis.ticks = element_blank())\n```\n\n::: {.cell-output-display}\n![](3_clustering_files/figure-html/unnamed-chunk-15-3.png){width=672}\n:::\n\n```{.r .cell-code}\n# Reorder based upon clustering order\n# Set Cell and Gene to factors (giving them a sorted order, based upon the hierarchical clustering)\n# The order given by clustering is provided by the hclust_object$labels\nreorder_plot <- for_plotting\nreorder_plot$Gene <- factor(reorder_plot$Gene, levels = gene_clust$labels)\nreorder_plot$Cell <- factor(reorder_plot$Cell, levels = cell_clust$labels)\nggplot(reorder_plot, aes(x = Gene, y = Cell, fill = expression_value)) +\n  geom_tile() +\n  theme_bw() +\n  theme(axis.text = element_blank(),\n        axis.ticks = element_blank())\n```\n\n::: {.cell-output-display}\n![](3_clustering_files/figure-html/unnamed-chunk-15-4.png){width=672}\n:::\n\n```{.r .cell-code}\n# How did the plot change?\n\n# Make it pretty\nggplot(reorder_plot, aes(x = Gene, y = Cell, fill = expression_value)) +\n  geom_tile() +\n  scale_fill_viridis_c(option = 'rocket') +\n  coord_cartesian(expand = TRUE) + # removes blank space towards the edge\n  labs(fill = \"Expression\") +\n  theme_bw() +\n  theme(axis.text = element_blank(),\n        axis.ticks = element_blank())\n```\n\n::: {.cell-output-display}\n![](3_clustering_files/figure-html/unnamed-chunk-15-5.png){width=672}\n:::\n\n```{.r .cell-code}\n# How does the heatmap look different if you use other options for hierachical clustering?\n# Hint: Try \"ward.D2\" or \"centroid\" for the method parameter.\n# CODE YOUR ANSWER HERE\n\n\n# Do you have a gene expression matrix of your own? Try creating a heatmap from your data!\n# CODE YOUR ANSWER HERE\n```\n:::\n",
    "supporting": [
      "3_clustering_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}